{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Prompt Chains & Testing Workflows \u2014 SOLUTION KEY\n",
        "\n",
        "This document contains example solutions for all exercises. Your answers may differ \u2014 there are many valid approaches.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand and debug multi-step prompt chains\n",
        "- Design prompt workflows using sequential, fan-out, and iterative patterns\n",
        "- Critically evaluate biased A/B test designs\n",
        "- Architect complex multi-step pipelines for real business tasks\n",
        "\n",
        "**Duration:** 55\u201365 minutes | **Difficulty:** Intermediate\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Understanding Prompt Chains\n",
        "\n",
        "A **prompt chain** breaks a complex task into discrete steps where the output of one step becomes the input for the next.\n",
        "\n",
        "### Example: Article Writing Chain\n",
        "\n",
        "**Step 1 \u2014 Extract Topics:**\n",
        "> Extract the 5 most important topics from the following subject: AI in healthcare\n",
        "\n",
        "*Output:*\n",
        "> 1. Current adoption rates and growth trajectory\n",
        "> 2. Primary use cases in clinical settings\n",
        "> 3. Regulatory landscape and compliance\n",
        "> 4. Cost-benefit analysis for hospital systems\n",
        "> 5. Patient outcome improvements backed by studies\n",
        "\n",
        "**Step 2 \u2014 Create Outline** (uses Step 1 output):\n",
        "> Create a detailed article outline organized around these topics: [Step 1 output]\n",
        "\n",
        "**Step 3 \u2014 Write Introduction** (uses Step 2 output):\n",
        "> Write a compelling 150-word introduction for an article with this outline: [Step 2 output]\n",
        "\n",
        "### Why Chains Work\n",
        "- Each step has a focused, manageable scope\n",
        "- The AI produces better output with specific, narrow tasks\n",
        "- You can inspect and fix intermediate results before they cascade\n",
        "- You can reuse steps across different workflows\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow Patterns\n",
        "\n",
        "### 1. Sequential Pattern\n",
        "Steps run one after another. Each step\u2019s output feeds the next.\n",
        "```\n",
        "[Step 1: Research] \u2192 [Step 2: Outline] \u2192 [Step 3: Draft] \u2192 [Step 4: Polish]\n",
        "```\n",
        "**Best for:** Article writing, report generation, email drafting\n",
        "\n",
        "### 2. Fan-Out Pattern\n",
        "Same prompt applied to multiple inputs independently.\n",
        "```\n",
        "[Input A] \u2192 [Same Prompt] \u2192 [Result A]\n",
        "[Input B] \u2192 [Same Prompt] \u2192 [Result B]\n",
        "```\n",
        "**Best for:** Batch analysis, processing multiple documents, scoring candidates\n",
        "\n",
        "### 3. Iterative Pattern\n",
        "Same prompt re-applied to progressively refine output.\n",
        "```\n",
        "[Draft v1] \u2192 [Refine] \u2192 [Draft v2] \u2192 [Refine] \u2192 [Draft v3]\n",
        "```\n",
        "**Best for:** Editing, polishing, improving quality\n",
        "\n",
        "### Combining Patterns\n",
        "Real workflows often combine patterns. Example hiring pipeline:\n",
        "1. **Fan-out:** Score each candidate on 3 dimensions\n",
        "2. **Sequential:** Rank \u2192 Select finalists \u2192 Generate interview questions \u2192 Write memo\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Debug the Broken Chain (10 minutes)\n",
        "\n",
        "A 4-step prompt chain for generating a product launch email for **CloudSync Pro** has 3 bugs that cascade through the chain.\n",
        "\n",
        "**Product:** CloudSync Pro \u2014 AI-powered cloud storage with smart file organization, 2TB capacity, real-time collaboration, end-to-end encryption, and intelligent search.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Broken Chain \u2014 Bugs Identified\n",
        "\n",
        "**Step 1: Identify Features**\n",
        "\n",
        "*Buggy Output:*\n",
        "> Key Features of SmartFit Pro Fitness Tracker:\n",
        "> 1. Heart rate monitoring...\n",
        "\n",
        "**Bug:** The output describes the WRONG PRODUCT (a fitness tracker called SmartFit Pro instead of CloudSync Pro cloud storage). The step completely ignored the actual product in the input and hallucinated features for an unrelated product.\n",
        "\n",
        "**Corrected prompt for Step 1:**\n",
        "> List the 5 most compelling features of CloudSync Pro, an AI-powered cloud storage solution with smart file organization, 2TB capacity, real-time collaboration, end-to-end encryption, and intelligent search. Focus on what makes each feature valuable to business users.\n",
        "\n",
        "**Corrected Step 1 output (from AI tool):**\n",
        "> Key Features of CloudSync Pro:\n",
        "> 1. AI-powered smart file organization \u2014 automatically categorizes and tags files, saving hours of manual sorting\n",
        "> 2. 2TB storage capacity \u2014 enough for entire team libraries with room to grow\n",
        "> 3. Real-time collaboration \u2014 multiple users can edit documents simultaneously with live sync\n",
        "> 4. End-to-end encryption \u2014 enterprise-grade security ensures data stays protected at rest and in transit\n",
        "> 5. Intelligent search \u2014 natural language queries find files instantly across all folders and file types\n",
        "\n",
        "---\n",
        "\n",
        "**Step 2: Prioritize Features**\n",
        "\n",
        "*Buggy Output:*\n",
        "> 1. [LOW] Water resistance \u2014 nice-to-have\n",
        "> 2. [LOW] GPS mapping \u2014 niche appeal\n",
        "> ...\n",
        "> 5. [HIGH] AI-powered insights \u2014 unique selling point\n",
        "\n",
        "**Bug:** The priority order is REVERSED \u2014 the lowest-importance features are listed first (positions 1-2) and the highest-importance features are listed last (positions 4-5). Since the email draft step will \"lead with the top features,\" it will lead with the weakest ones.\n",
        "\n",
        "**Corrected prompt for Step 2:**\n",
        "> Rank the following features of CloudSync Pro by customer impact, with the HIGHEST importance first. For each feature, explain why it ranks where it does:\n",
        "> [paste Step 1 output]\n",
        "\n",
        "**Corrected Step 2 output:**\n",
        "> Prioritized Features (highest impact first):\n",
        "> 1. [HIGH] End-to-end encryption \u2014 #1 concern for enterprise buyers; non-negotiable for regulated industries\n",
        "> 2. [HIGH] AI-powered smart file organization \u2014 unique differentiator; saves measurable time daily\n",
        "> 3. [HIGH] Real-time collaboration \u2014 essential for remote/hybrid teams; directly competes with Google Drive\n",
        "> 4. [MED] Intelligent search \u2014 strong value-add; reduces file-finding time from minutes to seconds\n",
        "> 5. [MED] 2TB storage capacity \u2014 competitive baseline; expected but not a differentiator\n",
        "\n",
        "---\n",
        "\n",
        "**Step 3: Draft Email**\n",
        "\n",
        "*Buggy Output:*\n",
        "> Subject: Exciting News!\n",
        "> Hi there,\n",
        "> We have something new for you. Our product is great and you should buy it...\n",
        "\n",
        "**Bug:** The draft completely ignores the prioritized features from Step 2. It produces generic marketing copy with zero specifics \u2014 no product name, no features, no differentiators. It could be about any product.\n",
        "\n",
        "**Corrected prompt for Step 3:**\n",
        "> Write a product launch announcement email for CloudSync Pro. Lead with the top 2 features from this prioritized list, mention all 5, and include specific benefits for each:\n",
        "> [paste Step 2 output]\n",
        ">\n",
        "> Format: Compelling subject line, personal greeting, 3 paragraphs (lead feature, supporting features, invitation to try), professional sign-off.\n",
        "\n",
        "**Corrected Step 3 output:**\n",
        "> Subject: Introducing CloudSync Pro \u2014 Cloud Storage That Thinks for You\n",
        ">\n",
        "> Hi [Name],\n",
        ">\n",
        "> What if your cloud storage could organize itself? CloudSync Pro combines enterprise-grade end-to-end encryption with AI-powered file organization that automatically categorizes, tags, and sorts your documents \u2014 so you spend time working, not searching.\n",
        ">\n",
        "> But that\u2019s just the start. CloudSync Pro also features real-time collaboration for seamless teamwork, intelligent natural-language search that finds any file in seconds, and a generous 2TB of storage for your entire team. Whether you\u2019re in a regulated industry that demands bulletproof security or a fast-moving startup that needs instant access to everything, CloudSync Pro delivers.\n",
        ">\n",
        "> We\u2019d love for you to experience the difference firsthand.\n",
        ">\n",
        "> Best regards,\n",
        "> The CloudSync Team\n",
        "\n",
        "**Step 4 (working correctly) then adds the CTA:**\n",
        "> ---\n",
        "> LIMITED TIME: Pre-order now at 25% off. Use code LAUNCH25 at checkout. Offer expires March 15th.\n",
        "> >> Pre-order Now: https://example.com/launch <<"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Prompt Testing & A/B Comparison\n",
        "\n",
        "Professional prompt engineers test prompts systematically:\n",
        "1. Define test cases with expected outputs\n",
        "2. Run both prompts against the same inputs\n",
        "3. Score outputs on consistent dimensions\n",
        "4. Compare results in a structured table\n",
        "\n",
        "### Evaluation Dimensions\n",
        "\n",
        "| Dimension | What to Look For |\n",
        "|-----------|------------------|\n",
        "| **Relevance** | Does the output directly address the input? |\n",
        "| **Completeness** | Does it cover all requested points? |\n",
        "| **Format Compliance** | Does it follow the requested structure? |\n",
        "| **Consistency** | Is the quality consistent across different inputs? |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Design a Feedback Pipeline (15 minutes)\n",
        "\n",
        "Process 5 customer feedback items through a multi-step prompt workflow.\n",
        "\n",
        "### The Feedback Items\n",
        "1. \"The export feature crashes every time I try to save as PDF. This is blocking my entire team's workflow!!!\"\n",
        "2. \"Love the new dashboard redesign! The charts are so much clearer and the dark mode option is fantastic.\"\n",
        "3. \"It would be great if you could add integration with Slack so we get notifications when reports are ready.\"\n",
        "4. \"Your billing system charged me twice this month. I need an immediate refund. This is unacceptable and I'm considering switching to a competitor.\"\n",
        "5. \"The search function is slow when filtering by date range. Takes about 10 seconds to load results for large datasets.\"\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Categorization Prompt\n",
        "\n",
        "**Prompt used:**\n",
        "> Categorize the following customer feedback item into exactly one category: Bug, Feature Request, Praise, or Complaint. Respond with only the category name and a one-sentence justification.\n",
        ">\n",
        "> Feedback: \"{feedback text}\"\n",
        "\n",
        "**Results:**\n",
        "\n",
        "| # | Feedback (first 50 chars) | Category |\n",
        "|---|--------------------------|----------|\n",
        "| 1 | The export feature crashes every time... | **Bug** \u2014 Reports a crash (broken functionality) |\n",
        "| 2 | Love the new dashboard redesign... | **Praise** \u2014 Positive feedback on existing features |\n",
        "| 3 | It would be great if you could add... | **Feature Request** \u2014 Suggests new Slack integration |\n",
        "| 4 | Your billing system charged me twice... | **Complaint** \u2014 Reports billing error with demand for action |\n",
        "| 5 | The search function is slow when... | **Bug** \u2014 Reports performance issue (slow loading) |\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Urgency Scoring Prompt\n",
        "\n",
        "**Prompt used:**\n",
        "> Score the urgency of the following customer feedback on a scale of 1-5:\n",
        "> - 1 = Low (nice to know)\n",
        "> - 2 = Minor (address when convenient)\n",
        "> - 3 = Moderate (address this sprint)\n",
        "> - 4 = High (address this week)\n",
        "> - 5 = Critical (address immediately \u2014 revenue or customer retention at risk)\n",
        ">\n",
        "> Consider: Is functionality blocked? Is money involved? Is the customer threatening to leave? How many people are affected?\n",
        ">\n",
        "> Feedback: \"{feedback text}\"\n",
        "\n",
        "**Results:**\n",
        "\n",
        "| # | Feedback (first 50 chars) | Category | Urgency |\n",
        "|---|--------------------------|----------|--------|\n",
        "| 1 | The export feature crashes every time... | Bug | **5/5** \u2014 Blocking entire team |\n",
        "| 2 | Love the new dashboard redesign... | Praise | **1/5** \u2014 Positive, no action needed |\n",
        "| 3 | It would be great if you could add... | Feature Request | **2/5** \u2014 Nice to have, not blocking |\n",
        "| 4 | Your billing system charged me twice... | Complaint | **5/5** \u2014 Money involved, churn risk |\n",
        "| 5 | The search function is slow when... | Bug | **3/5** \u2014 Degraded but not blocked |\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Response Planning Prompt\n",
        "\n",
        "**Prompt used:**\n",
        "> Based on the following categorized and urgency-scored customer feedback, create an action plan. Prioritize by urgency (highest first). For each item, specify: the responsible team, the action to take, and a target resolution time.\n",
        ">\n",
        "> 1. Bug (Urgency 5/5): Export crashes when saving as PDF \u2014 blocking team workflow\n",
        "> 2. Praise (Urgency 1/5): Loves dashboard redesign and dark mode\n",
        "> 3. Feature Request (Urgency 2/5): Wants Slack integration for report notifications\n",
        "> 4. Complaint (Urgency 5/5): Double-charged on billing, demands refund, threatening to switch\n",
        "> 5. Bug (Urgency 3/5): Search slow with date filters on large datasets (10s load time)\n",
        "\n",
        "**AI-generated action plan:**\n",
        "\n",
        "> ### Customer Feedback Action Plan (Priority Order)\n",
        ">\n",
        "> **CRITICAL \u2014 Resolve Within 24 Hours:**\n",
        ">\n",
        "> | # | Issue | Team | Action | Target |\n",
        "> |---|-------|------|--------|--------|\n",
        "> | 4 | Double billing charge | Billing + Support | Issue immediate refund, send apology email with goodwill credit, investigate root cause | Today |\n",
        "> | 1 | PDF export crash | Engineering | Hotfix for export module, notify affected users when resolved | 24 hours |\n",
        ">\n",
        "> **MODERATE \u2014 Resolve This Sprint:**\n",
        ">\n",
        "> | # | Issue | Team | Action | Target |\n",
        "> |---|-------|------|--------|--------|\n",
        "> | 5 | Slow date-range search | Engineering (Backend) | Profile query performance, add database index for date filters | This sprint |\n",
        ">\n",
        "> **LOW \u2014 Backlog:**\n",
        ">\n",
        "> | # | Issue | Team | Action | Target |\n",
        "> |---|-------|------|--------|--------|\n",
        "> | 3 | Slack integration request | Product | Add to feature backlog, survey other users for demand validation | Q2 planning |\n",
        "> | 2 | Dashboard praise | Marketing + Product | Share with team for morale, use as testimonial (with permission) | Ongoing |\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "**What I learned about multi-step workflows:** Breaking the feedback processing into categorize \u2192 score \u2192 plan made each step clearer and more consistent. If I had tried to do all three in one prompt, the AI would have mixed up categories and urgency scores.\n",
        "\n",
        "**Hardest step:** The urgency scoring prompt required the most iteration. My first version produced inconsistent scores because I didn't define what each number meant. Adding the 1-5 rubric directly in the prompt fixed this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Expose the Rigged A/B Test (15 minutes)\n",
        "\n",
        "An A/B test below compares two prompts for product descriptions and concludes Prompt B wins dramatically. But the test is rigged.\n",
        "\n",
        "**Prompt A:** \"Describe this product: {product}\"\n",
        "**Prompt B:** (full CRAFT version with context, role, etc.)\n",
        "\n",
        "**Rigged Results:**\n",
        "\n",
        "| Product | Prompt A | Prompt B |\n",
        "|---------|---------|--------|\n",
        "| NovaBuds Pro earbuds | 6/20 | 19/20 |\n",
        "| ErgoRise laptop stand | 5/20 | 18/20 |\n",
        "| HydroTrack water bottle | 7/20 | 20/20 |\n",
        "| **Total** | **18/60** | **57/60** |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 1: Biases Identified\n",
        "\n",
        "**Bias 1: The expected keywords were copied directly from Prompt B's outputs.** The test cases use words like \"immerse,\" \"crystal-clear,\" \"premium,\" \"game-changer\" as expected keywords \u2014 these are the exact words used in Prompt B's simulated outputs. Prompt A was doomed to score low on completeness because the \"correct\" answers were written to match Prompt B.\n",
        "\n",
        "**Bias 2: Prompt A's simulated output is deliberately terrible.** The simulated output for Prompt A is the same generic 4 sentences for every product: \"This is a good product. It works well and looks nice.\" A real AI given Prompt A would produce a much more useful description than this. The simulation makes Prompt A look worse than it actually is.\n",
        "\n",
        "**Bias 3: Prompt B's simulated output is keyword-stuffed to maximize scoring.** Prompt B's outputs were specifically written to include every expected keyword from the test cases. This isn't how a real AI would respond \u2014 it's a simulation designed to get the highest possible completeness score.\n",
        "\n",
        "**Bias 4: The expected format patterns favor Prompt B's structure.** The format regex patterns (like `(hook|bullet|feature).*CTA`) are designed around Prompt B's requested format. Prompt A never asked for hooks and CTAs, so it's penalized for not matching a format it never requested.\n",
        "\n",
        "---\n",
        "\n",
        "### Part 2: Fair Test Design\n",
        "\n",
        "**Test products:**\n",
        "1. Sony WH-1000XM5 headphones\n",
        "2. Anker 737 portable charger\n",
        "3. Kindle Paperwhite e-reader\n",
        "\n",
        "**Fair evaluation criteria:**\n",
        "\n",
        "| Dimension | What to Look For |\n",
        "|-----------|------------------|\n",
        "| Accuracy | Are the product features described correctly? |\n",
        "| Persuasiveness | Would a customer want to buy after reading this? |\n",
        "| Completeness | Does it cover key features, benefits, and use cases? |\n",
        "| Readability | Is it well-organized and easy to scan? |\n",
        "\n",
        "---\n",
        "\n",
        "### Part 3: Fair Test Results\n",
        "\n",
        "**Product 1: Sony WH-1000XM5 headphones**\n",
        "\n",
        "*Prompt A output:*\n",
        "> The Sony WH-1000XM5 headphones deliver exceptional noise cancellation and audio quality. With 30-hour battery life, comfortable lightweight design, and multipoint Bluetooth connectivity, they're ideal for commuters, travelers, and work-from-home professionals. The adaptive sound control automatically adjusts to your environment.\n",
        "\n",
        "*Prompt B output:*\n",
        "> Looking for headphones that silence the world? The Sony WH-1000XM5 are the gold standard in noise cancellation:\n",
        "> - Industry-leading ANC with 8 microphones and Auto NC Optimizer\n",
        "> - 30-hour battery with quick charging (3 min = 3 hours)\n",
        "> - Speak-to-Chat pauses music when you talk\n",
        ">\n",
        "> Whether you're on a cross-country flight or in a noisy open office, these headphones create your personal sound sanctuary. Experience audio the way the artist intended.\n",
        "\n",
        "| Dimension | Prompt A | Prompt B |\n",
        "|-----------|---------|--------|\n",
        "| Accuracy | 4/5 | 5/5 |\n",
        "| Persuasiveness | 3/5 | 4/5 |\n",
        "| Completeness | 3/5 | 4/5 |\n",
        "| Readability | 3/5 | 5/5 |\n",
        "| **Total** | **13/20** | **18/20** |\n",
        "\n",
        "**Product 2: Anker 737 \u2014 Prompt A: 14/20, Prompt B: 17/20**\n",
        "\n",
        "**Product 3: Kindle Paperwhite \u2014 Prompt A: 13/20, Prompt B: 16/20**\n",
        "\n",
        "---\n",
        "\n",
        "### Fair Test Final Results\n",
        "\n",
        "| Product | Prompt A | Prompt B |\n",
        "|---------|---------|--------|\n",
        "| Sony headphones | 13/20 | 18/20 |\n",
        "| Anker charger | 14/20 | 17/20 |\n",
        "| Kindle Paperwhite | 13/20 | 16/20 |\n",
        "| **Total** | **40/60** | **51/60** |\n",
        "\n",
        "**Does Prompt B still win?** Yes \u2014 Prompt B still wins in a fair test, which makes sense because CRAFT prompts genuinely produce better structured output. But the margin is 40 vs 51 (85% vs 68%), not 18 vs 57 (95% vs 30%). The rigged test made the gap look 3x larger than reality.\n",
        "\n",
        "**What the biases hid:** Prompt A is actually decent \u2014 it produces usable descriptions. The rigged test made it look incompetent by using artificially bad simulated outputs and scoring criteria designed to match only Prompt B's style."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Workflow Architect \u2014 Hiring Pipeline (15 minutes)\n",
        "\n",
        "Design a multi-step prompt workflow to evaluate 4 candidates:\n",
        "\n",
        "1. **Alex Chen** \u2014 8 years Python/ML, built recommendation systems. Quiet in interviews.\n",
        "2. **Jordan Rivera** \u2014 3 years, bootcamp grad. Articulate presenter. Built open-source tool.\n",
        "3. **Sam Patel** \u2014 12 years full-stack, led teams of 10+. Strong opinions, sometimes clashes.\n",
        "4. **Morgan Kim** \u2014 5 years, PhD in NLP. 4 published papers. Limited industry experience.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Scoring Prompt (Fan-Out)\n",
        "\n",
        "**Prompt used:**\n",
        "> Score the following job candidate on three dimensions, each 1-5:\n",
        "> - **Technical Skills** (1=entry level, 5=expert): Consider years of experience, complexity of projects, depth of expertise\n",
        "> - **Communication** (1=poor, 5=excellent): Consider presentation ability, written communication, interview performance\n",
        "> - **Culture Fit** (1=poor, 5=excellent): Consider collaboration style, mentorship orientation, team dynamics\n",
        ">\n",
        "> Provide a one-sentence justification for each score.\n",
        ">\n",
        "> Candidate: \"{profile}\"\n",
        "\n",
        "**Results:**\n",
        "\n",
        "| Candidate | Technical (1-5) | Communication (1-5) | Culture Fit (1-5) | Total (/15) |\n",
        "|-----------|----------------|--------------------|--------------------|-------------|\n",
        "| Alex Chen | 5 | 2 | 3 | **10** |\n",
        "| Jordan Rivera | 2 | 5 | 4 | **11** |\n",
        "| Sam Patel | 5 | 3 | 2 | **10** |\n",
        "| Morgan Kim | 4 | 4 | 4 | **12** |\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Ranking Prompt\n",
        "\n",
        "**Prompt used:**\n",
        "> Rank the following candidates from highest to lowest total score. If there's a tie, use Culture Fit as the tiebreaker (higher is better). Explain the ranking.\n",
        ">\n",
        "> [scores table from Step 1]\n",
        "\n",
        "**Ranking output:**\n",
        "> 1. **Morgan Kim** (12/15) \u2014 Most balanced candidate. Strong across all three dimensions with no score below 4.\n",
        "> 2. **Jordan Rivera** (11/15) \u2014 Excellent communicator with strong culture fit, but lower technical score reflects limited experience.\n",
        "> 3. **Alex Chen** (10/15, Culture Fit: 3) \u2014 Top technical skills but communication is a concern. Culture fit tiebreaker over Sam.\n",
        "> 4. **Sam Patel** (10/15, Culture Fit: 2) \u2014 Deep technical expertise but peer conflict history is a red flag.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Selection Prompt\n",
        "\n",
        "**Prompt used:**\n",
        "> Based on the ranking below, select the top 2 finalists. Explain why each was selected and what risks to monitor.\n",
        "> [ranking from Step 2]\n",
        "\n",
        "**Selection output:**\n",
        "> **Finalist 1: Morgan Kim** \u2014 Selected for the most balanced profile. Strong technical foundation (PhD + publications) combined with good communication and culture fit. Risk: Limited industry experience may mean slower ramp-up.\n",
        ">\n",
        "> **Finalist 2: Jordan Rivera** \u2014 Selected for exceptional communication and culture fit, which are harder to teach than technical skills. Risk: Technical gap may require 3-6 months of mentored ramp-up.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4: Interview Questions Prompt\n",
        "\n",
        "**Prompt used:**\n",
        "> For each finalist below, generate 2 interview questions that probe their weakest dimension. The questions should give them a chance to demonstrate hidden strength OR confirm the concern.\n",
        ">\n",
        "> Finalist 1: Morgan Kim \u2014 Weakest dimension: Technical Skills (4/5, limited industry experience)\n",
        "> Finalist 2: Jordan Rivera \u2014 Weakest dimension: Technical Skills (2/5, 3 years experience)\n",
        "\n",
        "**Interview questions:**\n",
        "\n",
        "> **Morgan Kim** (probing industry experience):\n",
        "> - Q1: \"Describe a time you had to adapt your academic research approach to meet a real-world deadline or business constraint. What trade-offs did you make?\"\n",
        "> - Q2: \"Walk me through how you would design a production ML pipeline for a recommendation system \u2014 not the model, but the infrastructure, monitoring, and deployment.\"\n",
        ">\n",
        "> **Jordan Rivera** (probing technical depth):\n",
        "> - Q1: \"Your open-source CLI tool is impressive. Walk me through the hardest technical decision you made during its development and what you'd change with more experience.\"\n",
        "> - Q2: \"Given a dataset of 1 million customer interactions, describe how you would build a basic predictive model. What tools would you use and what pitfalls would you watch for?\"\n",
        "\n",
        "---\n",
        "\n",
        "### Step 5: Recommendation Memo\n",
        "\n",
        "**Prompt used:**\n",
        "> Write a hiring recommendation memo based on the following evaluation. Include: recommended hire, backup candidate, reasoning, and onboarding suggestions.\n",
        "> [all previous outputs]\n",
        "\n",
        "**Final recommendation memo:**\n",
        "> ### Hiring Recommendation \u2014 Senior AI Engineer Position\n",
        ">\n",
        "> **Recommended Hire: Morgan Kim**\n",
        "> Morgan offers the strongest overall profile with a 12/15 score and no dimension below 4. Her PhD in NLP and 4 published papers on transformer architectures provide a deep theoretical foundation that will be immediately relevant to our AI roadmap. While her industry experience is limited, her strong communication skills and collaborative style suggest she will ramp quickly with structured onboarding.\n",
        ">\n",
        "> **Backup: Jordan Rivera**\n",
        "> Jordan's exceptional communication (5/5) and culture fit (4/5) make them an excellent long-term investment, particularly if we can provide the mentorship they've requested. Their technical gap (2/5) is the most significant concern, but their open-source contributions demonstrate self-directed learning ability.\n",
        ">\n",
        "> **Onboarding Recommendations:**\n",
        "> - Pair Morgan with a senior engineer for the first 60 days to bridge the academic-to-industry gap\n",
        "> - If hiring Jordan, establish a 90-day technical development plan with weekly check-ins\n",
        "> - Both candidates would benefit from a \"buddy system\" with current team members"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Prompt Engineering Best Practices Checklist\n",
        "\n",
        "- Use the CRAFT framework for every important prompt\n",
        "- Break complex tasks into chains \u2014 don't ask the AI to do everything at once\n",
        "- Inspect intermediate outputs before they cascade\n",
        "- Test prompts systematically with multiple inputs\n",
        "- Watch for A/B test biases\n",
        "- Adapt prompts for your audience\n",
        "- Build a template library\n",
        "- Score prompts with a rubric \u2014 but remember rubrics have blind spots\n",
        "\n",
        "### When to Use Each Pattern\n",
        "\n",
        "| Pattern | Best For | Example |\n",
        "|---------|---------|--------|\n",
        "| **Sequential** | Multi-step processes | Research \u2192 Outline \u2192 Draft \u2192 Edit |\n",
        "| **Fan-Out** | Same task, many items | Scoring candidates, analyzing documents |\n",
        "| **Iterative** | Progressive refinement | Draft \u2192 Improve \u2192 Polish |\n",
        "| **Combined** | Complex real-world tasks | Fan-out scoring + sequential ranking |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Reflection \u2014 Example Answers\n",
        "\n",
        "**1. Most surprising thing about prompt chains?**\n",
        "The biggest surprise is how much cascade failures matter. A small error in Step 1 (wrong product features) completely ruined the final email \u2014 it wasn't just slightly wrong, it was about the wrong product entirely. This shows why inspecting intermediate outputs is critical.\n",
        "\n",
        "**2. A daily work task that could benefit from a multi-step workflow:**\n",
        "- Step 1: Extract key points from meeting transcript (focus/narrow)\n",
        "- Step 2: Categorize points into Decisions, Action Items, and Open Questions (structure)\n",
        "- Step 3: Format as a team email with owners and deadlines (audience-tailor)\n",
        "- Step 4: Generate follow-up questions for unresolved items (extend)\n",
        "\n",
        "**3. One A/B test bias to watch for:**\n",
        "Using expected keywords that are copied from one prompt's output. This guarantees that prompt will score higher on \"completeness\" regardless of actual quality. Fair tests need neutral evaluation criteria that don't favor either prompt's style.\n",
        "\n",
        "**4. One piece of prompt engineering advice:**\n",
        "Break complex requests into chains of 2-4 focused prompts. A single prompt trying to do everything produces mediocre results across the board. Three focused prompts each doing one thing well will always beat one prompt trying to do three things at once.\n",
        "\n",
        "---\n",
        "\n",
        "*Solution key complete. Remember: these are example answers \u2014 many valid approaches exist for each exercise.*\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}