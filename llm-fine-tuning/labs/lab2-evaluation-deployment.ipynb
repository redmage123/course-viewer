{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Evaluation and Deployment\n",
    "\n",
    "In this lab, you'll learn how to evaluate your fine-tuned model, merge LoRA weights, and prepare for deployment.\n",
    "\n",
    "## Learning Objectives\n",
    "- Load and test a fine-tuned LoRA adapter\n",
    "- Evaluate model quality using various metrics\n",
    "- Merge LoRA weights into the base model\n",
    "- Export the model for deployment\n",
    "- Understand quantization options for inference\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Lab 1 (or have a trained LoRA adapter)\n",
    "- GPU with 16GB+ VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes\n",
    "!pip install -q rouge-score nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load the Fine-Tuned Model\n",
    "\n",
    "Load the base model with the LoRA adapter from Lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "base_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "adapter_path = \"./lora-adapter\"  # Path from Lab 1\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizer loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization config for loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load adapter\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "print(\"Model with adapter loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Evaluation Dataset\n",
    "\n",
    "Create a held-out test set for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation dataset - questions with expected answers\n",
    "eval_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is deep learning?\",\n",
    "        \"expected_keywords\": [\"neural network\", \"layers\", \"learn\", \"data\", \"patterns\"]\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept of backpropagation.\",\n",
    "        \"expected_keywords\": [\"gradient\", \"weights\", \"error\", \"chain rule\", \"update\"]\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is a transformer architecture?\",\n",
    "        \"expected_keywords\": [\"attention\", \"sequence\", \"parallel\", \"encoder\", \"decoder\"]\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain regularization in machine learning.\",\n",
    "        \"expected_keywords\": [\"overfitting\", \"penalty\", \"generalization\", \"complexity\"]\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the purpose of an activation function?\",\n",
    "        \"expected_keywords\": [\"non-linear\", \"neuron\", \"output\", \"learn\", \"complex\"]\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Evaluation samples: {len(eval_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(instruction, max_tokens=256):\n",
    "    \"\"\"Generate a response for a given instruction.\"\"\"\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "# Generate responses for all eval samples\n",
    "responses = []\n",
    "for sample in eval_data:\n",
    "    response = generate_response(sample[\"instruction\"])\n",
    "    responses.append({\n",
    "        \"instruction\": sample[\"instruction\"],\n",
    "        \"response\": response,\n",
    "        \"expected_keywords\": sample[\"expected_keywords\"]\n",
    "    })\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {sample['instruction']}\")\n",
    "    print(f\"A: {response[:200]}...\" if len(response) > 200 else f\"A: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Response Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_keywords(response, keywords):\n",
    "    \"\"\"Check how many expected keywords appear in the response.\"\"\"\n",
    "    response_lower = response.lower()\n",
    "    found = [kw for kw in keywords if kw.lower() in response_lower]\n",
    "    return len(found) / len(keywords), found\n",
    "\n",
    "def evaluate_length(response):\n",
    "    \"\"\"Check if response length is appropriate.\"\"\"\n",
    "    words = len(response.split())\n",
    "    if words < 20:\n",
    "        return 0.3, \"Too short\"\n",
    "    elif words < 50:\n",
    "        return 0.6, \"Brief\"\n",
    "    elif words < 150:\n",
    "        return 1.0, \"Good\"\n",
    "    elif words < 300:\n",
    "        return 0.8, \"Verbose\"\n",
    "    else:\n",
    "        return 0.5, \"Too long\"\n",
    "\n",
    "def evaluate_coherence(response):\n",
    "    \"\"\"Simple coherence check.\"\"\"\n",
    "    # Check for repetition (simplified)\n",
    "    sentences = response.split('.')\n",
    "    if len(sentences) > 1:\n",
    "        unique_sentences = set(s.strip().lower() for s in sentences if s.strip())\n",
    "        repetition_ratio = len(unique_sentences) / len([s for s in sentences if s.strip()])\n",
    "        return repetition_ratio, \"Low repetition\" if repetition_ratio > 0.8 else \"Some repetition\"\n",
    "    return 1.0, \"Single sentence\"\n",
    "\n",
    "# Evaluate all responses\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_keyword_score = 0\n",
    "total_length_score = 0\n",
    "total_coherence_score = 0\n",
    "\n",
    "for resp in responses:\n",
    "    print(f\"\\nInstruction: {resp['instruction'][:50]}...\")\n",
    "    \n",
    "    # Keyword evaluation\n",
    "    kw_score, found_kw = evaluate_keywords(resp['response'], resp['expected_keywords'])\n",
    "    print(f\"  Keywords: {kw_score:.0%} ({len(found_kw)}/{len(resp['expected_keywords'])}) - Found: {found_kw}\")\n",
    "    \n",
    "    # Length evaluation\n",
    "    len_score, len_desc = evaluate_length(resp['response'])\n",
    "    print(f\"  Length: {len_score:.0%} ({len_desc}) - {len(resp['response'].split())} words\")\n",
    "    \n",
    "    # Coherence evaluation\n",
    "    coh_score, coh_desc = evaluate_coherence(resp['response'])\n",
    "    print(f\"  Coherence: {coh_score:.0%} ({coh_desc})\")\n",
    "    \n",
    "    total_keyword_score += kw_score\n",
    "    total_length_score += len_score\n",
    "    total_coherence_score += coh_score\n",
    "\n",
    "n = len(responses)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"AVERAGE SCORES:\")\n",
    "print(f\"  Keywords: {total_keyword_score/n:.0%}\")\n",
    "print(f\"  Length: {total_length_score/n:.0%}\")\n",
    "print(f\"  Coherence: {total_coherence_score/n:.0%}\")\n",
    "print(f\"  Overall: {(total_keyword_score + total_length_score + total_coherence_score)/(3*n):.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare with Base Model (Optional)\n",
    "\n",
    "Compare responses from the fine-tuned model vs the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare, you would load the base model without the adapter\n",
    "# This is optional and requires additional memory\n",
    "\n",
    "# Example comparison structure:\n",
    "print(\"To compare with base model:\")\n",
    "print(\"1. Load base model without adapter\")\n",
    "print(\"2. Generate responses with same prompts\")\n",
    "print(\"3. Run same evaluation metrics\")\n",
    "print(\"4. Compare scores side by side\")\n",
    "\n",
    "# Uncomment below to actually compare (requires ~2x memory)\n",
    "# base_only = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Merge LoRA Weights\n",
    "\n",
    "Merge the LoRA adapter weights into the base model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load the base model in full precision for merging\n",
    "# Note: This requires more memory than 4-bit loading\n",
    "\n",
    "print(\"Loading base model in full precision for merging...\")\n",
    "\n",
    "# Clean up current model to free memory\n",
    "del model, base_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load base model in fp16\n",
    "base_model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load adapter\n",
    "model_with_adapter = PeftModel.from_pretrained(base_model_fp16, adapter_path)\n",
    "\n",
    "print(\"Model loaded for merging!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights into base model\n",
    "print(\"Merging LoRA weights...\")\n",
    "merged_model = model_with_adapter.merge_and_unload()\n",
    "\n",
    "print(f\"Merged model parameters: {merged_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged model\n",
    "merged_path = \"./merged-model\"\n",
    "os.makedirs(merged_path, exist_ok=True)\n",
    "\n",
    "print(f\"Saving merged model to {merged_path}...\")\n",
    "merged_model.save_pretrained(merged_path)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "\n",
    "print(\"Merged model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Verify Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test merged model\n",
    "print(\"Loading merged model for verification...\")\n",
    "\n",
    "# Clean up\n",
    "del merged_model, model_with_adapter, base_model_fp16\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load merged model with quantization for inference\n",
    "test_model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(merged_path)\n",
    "\n",
    "print(\"Merged model loaded for testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test merged model\n",
    "def test_merged_model(instruction):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    inputs = test_tokenizer(prompt, return_tensors=\"pt\").to(test_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = test_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=test_tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "# Verify with a test question\n",
    "test_q = \"What is the difference between L1 and L2 regularization?\"\n",
    "print(f\"Question: {test_q}\")\n",
    "print(f\"\\nResponse: {test_merged_model(test_q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Export Options\n",
    "\n",
    "Different export formats for various deployment scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deployment Options:\")\n",
    "print(\"\")\n",
    "print(\"1. vLLM (High throughput serving)\")\n",
    "print(\"   python -m vllm.entrypoints.api_server --model ./merged-model\")\n",
    "print(\"\")\n",
    "print(\"2. Text Generation Inference (TGI)\")\n",
    "print(\"   docker run --gpus all -p 8080:80 ghcr.io/huggingface/text-generation-inference --model-id ./merged-model\")\n",
    "print(\"\")\n",
    "print(\"3. Ollama (Local deployment)\")\n",
    "print(\"   # First convert to GGUF format\")\n",
    "print(\"   python llama.cpp/convert.py ./merged-model --outtype f16\")\n",
    "print(\"   ./llama.cpp/quantize ./model.gguf ./model-q4.gguf q4_k_m\")\n",
    "print(\"   # Then create Modelfile and run with ollama\")\n",
    "print(\"\")\n",
    "print(\"4. Hugging Face Hub\")\n",
    "print(\"   from huggingface_hub import HfApi\")\n",
    "print(\"   api = HfApi()\")\n",
    "print(\"   api.upload_folder(folder_path='./merged-model', repo_id='your-username/model-name')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Create Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model card for documentation\n",
    "model_card = \"\"\"---\n",
    "language: en\n",
    "license: apache-2.0\n",
    "base_model: mistralai/Mistral-7B-v0.1\n",
    "tags:\n",
    "  - fine-tuned\n",
    "  - lora\n",
    "  - qlora\n",
    "---\n",
    "\n",
    "# Fine-Tuned Mistral-7B\n",
    "\n",
    "## Model Description\n",
    "This model is a fine-tuned version of Mistral-7B-v0.1 using QLoRA.\n",
    "\n",
    "## Training Details\n",
    "- **Base Model:** mistralai/Mistral-7B-v0.1\n",
    "- **Method:** QLoRA (4-bit quantization + LoRA)\n",
    "- **LoRA Rank:** 16\n",
    "- **LoRA Alpha:** 32\n",
    "- **Training Epochs:** 3\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"path/to/model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"path/to/model\")\n",
    "\n",
    "prompt = \"### Instruction:\\\\nYour question here\\\\n\\\\n### Response:\\\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "\n",
    "## Limitations\n",
    "- Fine-tuned on a small demonstration dataset\n",
    "- May not generalize to all topics\n",
    "- Should be evaluated before production use\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{merged_path}/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\"Model card created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Add more evaluation metrics**: Implement ROUGE or BLEU scores\n",
    "2. **A/B Testing**: Compare outputs from adapter-only vs merged model\n",
    "3. **Quantization**: Export the model in different quantization levels and compare\n",
    "4. **Deployment**: Deploy the model using one of the serving options\n",
    "5. **Safety Evaluation**: Test the model with adversarial prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Add your own experiments!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
