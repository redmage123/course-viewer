{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Lab 1B: Pandas Fundamentals\n",
    "\n",
    "## Comprehensive Data Manipulation for Machine Learning\n",
    "\n",
    "**Duration:** 90 minutes | **Difficulty:** Beginner to Intermediate | **Prerequisites:** Lab 1A (NumPy)\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Create and explore** DataFrames effectively\n",
    "2. **Select and filter** data using various methods\n",
    "3. **Clean data** by handling missing values and duplicates\n",
    "4. **Transform data** with apply, map, and aggregation\n",
    "5. **Merge and join** datasets from multiple sources\n",
    "6. **Prepare data** for machine learning pipelines\n",
    "7. **Visualize** data distributions and relationships\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell first to import libraries and configure matplotlib for inline display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Run this cell first\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure matplotlib for inline display\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Pandas display options\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 15)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"âœ“ Setup complete! Matplotlib configured for inline display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Creating and Exploring DataFrames\n",
    "\n",
    "A DataFrame is a 2D labeled data structure - think of it as a spreadsheet or SQL table in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1-1",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Create DataFrames\n",
    "\n",
    "Create DataFrames from different sources:\n",
    "\n",
    "1. `df_dict` - From a dictionary of lists\n",
    "2. `df_arrays` - From NumPy arrays with custom column names\n",
    "3. `df_records` - From a list of dictionaries (records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.1: Create DataFrames\n",
    "\n",
    "# Method 1: From dictionary of lists\n",
    "data_dict = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'age': [25, 30, 35, 28],\n",
    "    'city': ['NYC', 'LA', 'Chicago', 'NYC'],\n",
    "    'salary': [70000, 80000, 90000, 75000]\n",
    "}\n",
    "# YOUR CODE HERE\n",
    "df_dict = None  # pd.DataFrame(data_dict)\n",
    "\n",
    "# Method 2: From NumPy arrays\n",
    "np_data = np.random.randn(5, 3)\n",
    "# YOUR CODE HERE\n",
    "df_arrays = None  # pd.DataFrame(np_data, columns=['A', 'B', 'C'])\n",
    "\n",
    "# Method 3: From list of records (dictionaries)\n",
    "records = [\n",
    "    {'product': 'Widget', 'price': 25.99, 'quantity': 100},\n",
    "    {'product': 'Gadget', 'price': 49.99, 'quantity': 50},\n",
    "    {'product': 'Gizmo', 'price': 19.99, 'quantity': 200}\n",
    "]\n",
    "# YOUR CODE HERE\n",
    "df_records = None  # pd.DataFrame(records)\n",
    "\n",
    "print(\"From dictionary:\")\n",
    "print(df_dict)\n",
    "print(\"\\nFrom NumPy arrays:\")\n",
    "print(df_arrays)\n",
    "print(\"\\nFrom records:\")\n",
    "print(df_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1-2",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Explore DataFrames\n",
    "\n",
    "Let's create a larger dataset and explore it. Complete the exploration functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-2-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger sample dataset\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'customer_id': range(1000, 1000 + n),\n",
    "    'age': np.random.randint(18, 70, n),\n",
    "    'income': np.random.normal(60000, 20000, n).astype(int),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n, p=[0.3, 0.4, 0.2, 0.1]),\n",
    "    'city': np.random.choice(['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix'], n),\n",
    "    'signup_year': np.random.choice([2020, 2021, 2022, 2023], n),\n",
    "    'purchases': np.random.randint(0, 50, n),\n",
    "    'satisfaction': np.random.choice([1, 2, 3, 4, 5], n, p=[0.05, 0.1, 0.3, 0.35, 0.2])\n",
    "})\n",
    "\n",
    "# Add some missing values\n",
    "df.loc[np.random.choice(n, 10, replace=False), 'income'] = np.nan\n",
    "df.loc[np.random.choice(n, 5, replace=False), 'satisfaction'] = np.nan\n",
    "\n",
    "print(f\"Dataset created with {len(df)} rows and {len(df.columns)} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.2: Explore the DataFrame\n",
    "\n",
    "# YOUR CODE HERE - Get basic info about the DataFrame\n",
    "shape = None          # df.shape\n",
    "columns = None        # df.columns.tolist()\n",
    "dtypes = None         # df.dtypes\n",
    "first_5 = None        # df.head()\n",
    "last_3 = None         # df.tail(3)\n",
    "random_sample = None  # df.sample(5)\n",
    "stats = None          # df.describe()\n",
    "info = None           # df.info() - prints directly\n",
    "\n",
    "print(f\"Shape: {shape}\")\n",
    "print(f\"\\nColumns: {columns}\")\n",
    "print(f\"\\nData types:\\n{dtypes}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(first_5)\n",
    "print(f\"\\nStatistical summary:\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1-3",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Value Counts and Unique Values\n",
    "\n",
    "Understand categorical distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.3: Value Counts and Unique Values\n",
    "\n",
    "# YOUR CODE HERE\n",
    "unique_cities = None      # df['city'].unique()\n",
    "n_unique_cities = None    # df['city'].nunique()\n",
    "city_counts = None        # df['city'].value_counts()\n",
    "city_percentages = None   # df['city'].value_counts(normalize=True) * 100\n",
    "education_counts = None   # df['education'].value_counts()\n",
    "\n",
    "print(f\"Unique cities: {unique_cities}\")\n",
    "print(f\"Number of unique cities: {n_unique_cities}\")\n",
    "print(f\"\\nCity counts:\\n{city_counts}\")\n",
    "print(f\"\\nCity percentages:\\n{city_percentages.round(1)}\")\n",
    "print(f\"\\nEducation distribution:\\n{education_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1-viz",
   "metadata": {},
   "source": [
    "### Visualization: Data Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-viz-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Age distribution\n",
    "axes[0, 0].hist(df['age'], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_title('Age Distribution')\n",
    "\n",
    "# Income distribution\n",
    "axes[0, 1].hist(df['income'].dropna(), bins=20, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[0, 1].set_xlabel('Income ($)')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Income Distribution')\n",
    "\n",
    "# City counts (bar chart)\n",
    "city_counts = df['city'].value_counts()\n",
    "axes[1, 0].bar(city_counts.index, city_counts.values, edgecolor='black', color='seagreen')\n",
    "axes[1, 0].set_xlabel('City')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Customers by City')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Satisfaction distribution\n",
    "sat_counts = df['satisfaction'].value_counts().sort_index()\n",
    "axes[1, 1].bar(sat_counts.index.astype(str), sat_counts.values, edgecolor='black', color='purple', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Satisfaction Score')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('Satisfaction Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Selecting and Filtering Data\n",
    "\n",
    "Pandas provides multiple ways to select data: `[]`, `.loc[]`, `.iloc[]`, and boolean indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2-1",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Column Selection\n",
    "\n",
    "Select columns in different ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.1: Column Selection\n",
    "\n",
    "# YOUR CODE HERE\n",
    "single_col = None          # df['age']  - returns Series\n",
    "multi_cols = None          # df[['age', 'income', 'city']]  - returns DataFrame\n",
    "cols_loc = None            # df.loc[:, 'age':'city']  - range of columns\n",
    "cols_by_dtype = None       # df.select_dtypes(include=['int64'])  - select by dtype\n",
    "\n",
    "print(\"Single column (Series):\")\n",
    "print(single_col.head())\n",
    "print(f\"\\nMultiple columns:\")\n",
    "print(multi_cols.head())\n",
    "print(f\"\\nColumn range with .loc:\")\n",
    "print(cols_loc.head())\n",
    "print(f\"\\nNumeric columns only:\")\n",
    "print(cols_by_dtype.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2-2",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Row Selection with .loc and .iloc\n",
    "\n",
    "- `.loc[]` - Label-based selection\n",
    "- `.iloc[]` - Integer position-based selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.2: Row Selection\n",
    "\n",
    "# YOUR CODE HERE\n",
    "first_row = None           # df.iloc[0]\n",
    "rows_0_to_4 = None         # df.iloc[0:5]\n",
    "rows_and_cols = None       # df.iloc[0:5, [1, 2, 3]]  - first 5 rows, columns at index 1,2,3\n",
    "specific_cell = None       # df.iloc[0, 1]  - row 0, column 1\n",
    "\n",
    "# With .loc (label-based)\n",
    "row_by_label = None        # df.loc[0]  - row with index 0\n",
    "cols_by_name = None        # df.loc[0:4, ['age', 'income']]  - specific columns by name\n",
    "\n",
    "print(\"First row (iloc[0]):\")\n",
    "print(first_row)\n",
    "print(f\"\\nRows 0-4, specific columns (iloc):\")\n",
    "print(rows_and_cols)\n",
    "print(f\"\\nRows 0-4, named columns (loc):\")\n",
    "print(cols_by_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2-3",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Boolean Filtering\n",
    "\n",
    "Filter rows based on conditions - this is the most common selection method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.3: Boolean Filtering\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Single condition\n",
    "high_income = None         # df[df['income'] > 80000]\n",
    "nyc_customers = None       # df[df['city'] == 'NYC']\n",
    "\n",
    "# Multiple conditions (use & for AND, | for OR)\n",
    "young_high_earners = None  # df[(df['age'] < 35) & (df['income'] > 70000)]\n",
    "nyc_or_la = None           # df[(df['city'] == 'NYC') | (df['city'] == 'LA')]\n",
    "\n",
    "# Using .isin() for multiple values\n",
    "select_cities = None       # df[df['city'].isin(['NYC', 'LA', 'Chicago'])]\n",
    "\n",
    "# Using .query() method (alternative syntax)\n",
    "query_result = None        # df.query('age > 30 and income > 60000')\n",
    "\n",
    "print(f\"High income (>80k): {len(high_income)} rows\")\n",
    "print(f\"NYC customers: {len(nyc_customers)} rows\")\n",
    "print(f\"Young high earners (<35, >70k): {len(young_high_earners)} rows\")\n",
    "print(f\"NYC or LA: {len(nyc_or_la)} rows\")\n",
    "print(f\"\\nYoung high earners sample:\")\n",
    "print(young_high_earners.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2-4",
   "metadata": {},
   "source": [
    "### Exercise 2.4: String Methods\n",
    "\n",
    "Pandas provides string methods via the `.str` accessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.4: String Methods\n",
    "\n",
    "# YOUR CODE HERE\n",
    "upper_cities = None        # df['city'].str.upper()\n",
    "lower_cities = None        # df['city'].str.lower()\n",
    "contains_c = None          # df[df['city'].str.contains('C', case=False)]\n",
    "starts_with_h = None       # df[df['city'].str.startswith('H')]\n",
    "city_lengths = None        # df['city'].str.len()\n",
    "\n",
    "print(\"Uppercase cities:\")\n",
    "print(upper_cities.head())\n",
    "print(f\"\\nCities containing 'C': {len(contains_c)} rows\")\n",
    "print(contains_c['city'].unique())\n",
    "print(f\"\\nCities starting with 'H': {len(starts_with_h)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Data Cleaning\n",
    "\n",
    "Real-world data is messy! Cleaning data is typically 80% of a data scientist's work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-1",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1: Handling Missing Values\n",
    "\n",
    "# YOUR CODE HERE - Check for missing values\n",
    "missing_count = None       # df.isnull().sum()\n",
    "missing_percent = None     # (df.isnull().sum() / len(df) * 100).round(2)\n",
    "rows_with_missing = None   # df[df.isnull().any(axis=1)]\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_count)\n",
    "print(f\"\\nMissing percentages:\")\n",
    "print(missing_percent)\n",
    "print(f\"\\nRows with any missing: {len(rows_with_missing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-1b-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1b: Fill or Drop Missing Values\n",
    "\n",
    "# Create a copy to experiment\n",
    "df_clean = df.copy()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Option 1: Fill with specific value\n",
    "# df_clean['income'].fillna(0, inplace=True)\n",
    "\n",
    "# Option 2: Fill with mean/median\n",
    "income_median = None       # df_clean['income'].median()\n",
    "# df_clean['income'].fillna(income_median, inplace=True)\n",
    "\n",
    "# Option 3: Fill with mode (most common value)\n",
    "satisfaction_mode = None   # df_clean['satisfaction'].mode()[0]\n",
    "# df_clean['satisfaction'].fillna(satisfaction_mode, inplace=True)\n",
    "\n",
    "# Option 4: Forward fill (use previous value)\n",
    "# df_clean['column'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Option 5: Drop rows with missing values\n",
    "# df_clean.dropna(inplace=True)  # All missing\n",
    "# df_clean.dropna(subset=['income'], inplace=True)  # Specific column\n",
    "\n",
    "print(f\"Income median: {income_median}\")\n",
    "print(f\"Satisfaction mode: {satisfaction_mode}\")\n",
    "\n",
    "# Fill missing values with median/mode\n",
    "df_clean['income'].fillna(income_median, inplace=True)\n",
    "df_clean['satisfaction'].fillna(satisfaction_mode, inplace=True)\n",
    "\n",
    "print(f\"\\nAfter cleaning:\")\n",
    "print(df_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-2",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Handling Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.2: Handling Duplicates\n",
    "\n",
    "# Add some duplicate rows for demonstration\n",
    "df_with_dups = pd.concat([df_clean, df_clean.iloc[:5]], ignore_index=True)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "n_duplicates = None        # df_with_dups.duplicated().sum()\n",
    "duplicate_rows = None      # df_with_dups[df_with_dups.duplicated(keep=False)]\n",
    "df_no_dups = None          # df_with_dups.drop_duplicates()\n",
    "\n",
    "# Check duplicates based on specific columns\n",
    "dups_by_cols = None        # df_with_dups.duplicated(subset=['customer_id'])\n",
    "\n",
    "print(f\"Total rows: {len(df_with_dups)}\")\n",
    "print(f\"Duplicate rows: {n_duplicates}\")\n",
    "print(f\"After removing duplicates: {len(df_no_dups)}\")\n",
    "print(f\"\\nDuplicates by customer_id: {dups_by_cols.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-3",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.3: Data Type Conversion\n",
    "\n",
    "# Create sample data with type issues\n",
    "df_types = pd.DataFrame({\n",
    "    'id': ['001', '002', '003'],\n",
    "    'value': ['100', '200', '300'],\n",
    "    'price': ['$25.99', '$49.99', '$19.99'],\n",
    "    'date': ['2023-01-15', '2023-02-20', '2023-03-25'],\n",
    "    'active': ['True', 'False', 'True']\n",
    "})\n",
    "\n",
    "print(\"Original dtypes:\")\n",
    "print(df_types.dtypes)\n",
    "\n",
    "# YOUR CODE HERE - Convert types\n",
    "# Convert 'value' to integer\n",
    "# df_types['value'] = df_types['value'].astype(int)\n",
    "\n",
    "# Convert 'price' to float (need to remove $)\n",
    "# df_types['price'] = df_types['price'].str.replace('$', '').astype(float)\n",
    "\n",
    "# Convert 'date' to datetime\n",
    "# df_types['date'] = pd.to_datetime(df_types['date'])\n",
    "\n",
    "# Convert 'active' to boolean\n",
    "# df_types['active'] = df_types['active'].map({'True': True, 'False': False})\n",
    "\n",
    "df_types['value'] = df_types['value'].astype(int)\n",
    "df_types['price'] = df_types['price'].str.replace('$', '').astype(float)\n",
    "df_types['date'] = pd.to_datetime(df_types['date'])\n",
    "df_types['active'] = df_types['active'].map({'True': True, 'False': False})\n",
    "\n",
    "print(\"\\nConverted dtypes:\")\n",
    "print(df_types.dtypes)\n",
    "print(\"\\nData:\")\n",
    "print(df_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Data Transformation\n",
    "\n",
    "Transform data using apply, map, groupby, and aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex4-1",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Creating New Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1: Creating New Columns\n",
    "\n",
    "df_transform = df_clean.copy()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Simple arithmetic\n",
    "df_transform['income_thousands'] = None  # df_transform['income'] / 1000\n",
    "\n",
    "# Conditional column with np.where\n",
    "df_transform['income_level'] = None      # np.where(df_transform['income'] > 70000, 'High', 'Medium/Low')\n",
    "\n",
    "# Multiple conditions with np.select\n",
    "conditions = [\n",
    "    df_transform['age'] < 30,\n",
    "    df_transform['age'] < 50,\n",
    "    df_transform['age'] >= 50\n",
    "]\n",
    "choices = ['Young', 'Middle', 'Senior']\n",
    "df_transform['age_group'] = None         # np.select(conditions, choices)\n",
    "\n",
    "# Using cut for binning\n",
    "df_transform['satisfaction_label'] = None  # pd.cut(df_transform['satisfaction'], bins=[0, 2, 3, 5], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Fill in the code\n",
    "df_transform['income_thousands'] = df_transform['income'] / 1000\n",
    "df_transform['income_level'] = np.where(df_transform['income'] > 70000, 'High', 'Medium/Low')\n",
    "df_transform['age_group'] = np.select(conditions, choices)\n",
    "df_transform['satisfaction_label'] = pd.cut(df_transform['satisfaction'], bins=[0, 2, 3, 5], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(df_transform[['age', 'age_group', 'income', 'income_level', 'satisfaction', 'satisfaction_label']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex4-2",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Apply and Map Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2: Apply and Map Functions\n",
    "\n",
    "# apply() - Apply a function to each element/row/column\n",
    "# map() - Map values using a dictionary or function (Series only)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Apply function to column\n",
    "def categorize_purchases(x):\n",
    "    if x < 10: return 'Low'\n",
    "    elif x < 30: return 'Medium'\n",
    "    else: return 'High'\n",
    "\n",
    "df_transform['purchase_level'] = None  # df_transform['purchases'].apply(categorize_purchases)\n",
    "\n",
    "# Apply lambda function\n",
    "df_transform['age_squared'] = None     # df_transform['age'].apply(lambda x: x**2)\n",
    "\n",
    "# Map using dictionary\n",
    "education_years = {\n",
    "    'High School': 12,\n",
    "    'Bachelor': 16,\n",
    "    'Master': 18,\n",
    "    'PhD': 22\n",
    "}\n",
    "df_transform['education_years'] = None # df_transform['education'].map(education_years)\n",
    "\n",
    "# Apply to entire DataFrame (row-wise)\n",
    "# df_transform.apply(lambda row: row['income'] / row['age'], axis=1)\n",
    "\n",
    "df_transform['purchase_level'] = df_transform['purchases'].apply(categorize_purchases)\n",
    "df_transform['age_squared'] = df_transform['age'].apply(lambda x: x**2)\n",
    "df_transform['education_years'] = df_transform['education'].map(education_years)\n",
    "\n",
    "print(df_transform[['purchases', 'purchase_level', 'education', 'education_years']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex4-3",
   "metadata": {},
   "source": [
    "### Exercise 4.3: GroupBy and Aggregation\n",
    "\n",
    "GroupBy is one of the most powerful Pandas features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.3: GroupBy and Aggregation\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Simple groupby with single aggregation\n",
    "income_by_city = None      # df_transform.groupby('city')['income'].mean()\n",
    "\n",
    "# Multiple aggregations\n",
    "city_stats = None          # df_transform.groupby('city')['income'].agg(['mean', 'median', 'std', 'count'])\n",
    "\n",
    "# Group by multiple columns\n",
    "city_edu_stats = None      # df_transform.groupby(['city', 'education'])['income'].mean()\n",
    "\n",
    "# Multiple columns with different aggregations\n",
    "multi_agg = None           # df_transform.groupby('city').agg({'income': 'mean', 'purchases': 'sum', 'age': ['min', 'max']})\n",
    "\n",
    "income_by_city = df_transform.groupby('city')['income'].mean()\n",
    "city_stats = df_transform.groupby('city')['income'].agg(['mean', 'median', 'std', 'count'])\n",
    "city_edu_stats = df_transform.groupby(['city', 'education'])['income'].mean().unstack()\n",
    "multi_agg = df_transform.groupby('city').agg({'income': 'mean', 'purchases': 'sum', 'age': ['min', 'max']})\n",
    "\n",
    "print(\"Average income by city:\")\n",
    "print(income_by_city.round(0))\n",
    "print(\"\\nCity income statistics:\")\n",
    "print(city_stats.round(0))\n",
    "print(\"\\nIncome by city and education:\")\n",
    "print(city_edu_stats.round(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex4-viz",
   "metadata": {},
   "source": [
    "### Visualization: GroupBy Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-viz-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize groupby results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Income by city (bar chart)\n",
    "income_by_city.plot(kind='bar', ax=axes[0], color='steelblue', edgecolor='black')\n",
    "axes[0].set_title('Average Income by City')\n",
    "axes[0].set_xlabel('City')\n",
    "axes[0].set_ylabel('Average Income ($)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Income by education (horizontal bar)\n",
    "income_by_edu = df_transform.groupby('education')['income'].mean().sort_values()\n",
    "income_by_edu.plot(kind='barh', ax=axes[1], color='coral', edgecolor='black')\n",
    "axes[1].set_title('Average Income by Education')\n",
    "axes[1].set_xlabel('Average Income ($)')\n",
    "axes[1].set_ylabel('Education Level')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Merging and Joining DataFrames\n",
    "\n",
    "Combining data from multiple sources is a critical skill."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex5-setup",
   "metadata": {},
   "source": [
    "### Setup: Create Sample DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex5-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrames for merging\n",
    "\n",
    "# Customer info\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'city': ['NYC', 'LA', 'Chicago', 'NYC', 'Houston']\n",
    "})\n",
    "\n",
    "# Orders (some customers have multiple orders, some have none)\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104, 105, 106],\n",
    "    'customer_id': [1, 1, 2, 3, 3, 6],  # Note: customer 6 doesn't exist in customers\n",
    "    'product': ['Widget', 'Gadget', 'Gizmo', 'Widget', 'Gadget', 'Gizmo'],\n",
    "    'amount': [100, 150, 200, 100, 150, 250]\n",
    "})\n",
    "\n",
    "# Product info\n",
    "products = pd.DataFrame({\n",
    "    'product': ['Widget', 'Gadget', 'Gizmo'],\n",
    "    'category': ['Electronics', 'Electronics', 'Home'],\n",
    "    'price': [25.99, 49.99, 19.99]\n",
    "})\n",
    "\n",
    "print(\"Customers:\")\n",
    "print(customers)\n",
    "print(\"\\nOrders:\")\n",
    "print(orders)\n",
    "print(\"\\nProducts:\")\n",
    "print(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex5-1",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Merge Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex5-1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1: Merge Operations\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Inner join - only matching records\n",
    "inner_merge = None         # pd.merge(customers, orders, on='customer_id', how='inner')\n",
    "\n",
    "# Left join - all customers, matching orders\n",
    "left_merge = None          # pd.merge(customers, orders, on='customer_id', how='left')\n",
    "\n",
    "# Right join - all orders, matching customers\n",
    "right_merge = None         # pd.merge(customers, orders, on='customer_id', how='right')\n",
    "\n",
    "# Outer join - all records from both\n",
    "outer_merge = None         # pd.merge(customers, orders, on='customer_id', how='outer')\n",
    "\n",
    "inner_merge = pd.merge(customers, orders, on='customer_id', how='inner')\n",
    "left_merge = pd.merge(customers, orders, on='customer_id', how='left')\n",
    "right_merge = pd.merge(customers, orders, on='customer_id', how='right')\n",
    "outer_merge = pd.merge(customers, orders, on='customer_id', how='outer')\n",
    "\n",
    "print(f\"Inner join ({len(inner_merge)} rows):\")\n",
    "print(inner_merge)\n",
    "print(f\"\\nLeft join ({len(left_merge)} rows):\")\n",
    "print(left_merge)\n",
    "print(f\"\\nRight join ({len(right_merge)} rows):\")\n",
    "print(right_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex5-2",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Multi-Table Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex5-2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.2: Multi-Table Joins\n",
    "\n",
    "# Join all three tables\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Step 1: Join customers with orders\n",
    "customer_orders = None     # pd.merge(customers, orders, on='customer_id', how='inner')\n",
    "\n",
    "# Step 2: Join result with products\n",
    "full_data = None           # pd.merge(customer_orders, products, on='product', how='left')\n",
    "\n",
    "customer_orders = pd.merge(customers, orders, on='customer_id', how='inner')\n",
    "full_data = pd.merge(customer_orders, products, on='product', how='left')\n",
    "\n",
    "print(\"Full merged data:\")\n",
    "print(full_data)\n",
    "\n",
    "# Analysis: Total spend per customer\n",
    "customer_spend = full_data.groupby('name')['amount'].sum().sort_values(ascending=False)\n",
    "print(\"\\nTotal spend per customer:\")\n",
    "print(customer_spend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex5-3",
   "metadata": {},
   "source": [
    "### Exercise 5.3: Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex5-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.3: Concatenation\n",
    "\n",
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
    "df3 = pd.DataFrame({'C': [9, 10], 'D': [11, 12]})\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Vertical concatenation (stack rows)\n",
    "vertical = None            # pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "\n",
    "# Horizontal concatenation (add columns)\n",
    "horizontal = None          # pd.concat([df1, df3], axis=1)\n",
    "\n",
    "vertical = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "horizontal = pd.concat([df1, df3], axis=1)\n",
    "\n",
    "print(\"df1:\")\n",
    "print(df1)\n",
    "print(\"\\ndf2:\")\n",
    "print(df2)\n",
    "print(\"\\nVertical concat (df1 + df2):\")\n",
    "print(vertical)\n",
    "print(\"\\nHorizontal concat (df1 + df3):\")\n",
    "print(horizontal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Preparing Data for ML\n",
    "\n",
    "Prepare data for machine learning pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex6-1",
   "metadata": {},
   "source": [
    "### Exercise 6.1: Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex6-1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6.1: Encoding Categorical Variables\n",
    "\n",
    "df_ml = df_clean[['age', 'income', 'education', 'city', 'purchases', 'satisfaction']].copy()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# One-hot encoding (creates dummy variables)\n",
    "df_encoded = None          # pd.get_dummies(df_ml, columns=['city', 'education'])\n",
    "\n",
    "# Label encoding (convert categories to numbers)\n",
    "education_mapping = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\n",
    "df_ml['education_encoded'] = None  # df_ml['education'].map(education_mapping)\n",
    "\n",
    "df_encoded = pd.get_dummies(df_ml, columns=['city', 'education'])\n",
    "df_ml['education_encoded'] = df_ml['education'].map(education_mapping)\n",
    "\n",
    "print(\"One-hot encoded columns:\")\n",
    "print(df_encoded.columns.tolist())\n",
    "print(\"\\nOne-hot encoded sample:\")\n",
    "print(df_encoded.head())\n",
    "\n",
    "print(\"\\nLabel encoded:\")\n",
    "print(df_ml[['education', 'education_encoded']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex6-2",
   "metadata": {},
   "source": [
    "### Exercise 6.2: Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex6-2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6.2: Feature Scaling\n",
    "\n",
    "# Select numeric columns for scaling\n",
    "numeric_cols = ['age', 'income', 'purchases']\n",
    "df_scale = df_ml[numeric_cols].copy()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Min-Max Normalization (scale to 0-1)\n",
    "def min_max_normalize(series):\n",
    "    return (series - series.min()) / (series.max() - series.min())\n",
    "\n",
    "df_normalized = None       # df_scale.apply(min_max_normalize)\n",
    "\n",
    "# Z-score Standardization (mean=0, std=1)\n",
    "def standardize(series):\n",
    "    return (series - series.mean()) / series.std()\n",
    "\n",
    "df_standardized = None     # df_scale.apply(standardize)\n",
    "\n",
    "df_normalized = df_scale.apply(min_max_normalize)\n",
    "df_standardized = df_scale.apply(standardize)\n",
    "\n",
    "print(\"Original data stats:\")\n",
    "print(df_scale.describe().round(2))\n",
    "print(\"\\nNormalized (0-1) stats:\")\n",
    "print(df_normalized.describe().round(2))\n",
    "print(\"\\nStandardized (mean=0, std=1) stats:\")\n",
    "print(df_standardized.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex6-viz",
   "metadata": {},
   "source": [
    "### Visualization: Scaling Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex6-viz-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaling effects\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original\n",
    "for col in numeric_cols:\n",
    "    axes[0].hist(df_scale[col], bins=20, alpha=0.5, label=col)\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].legend()\n",
    "\n",
    "# Normalized\n",
    "for col in numeric_cols:\n",
    "    axes[1].hist(df_normalized[col], bins=20, alpha=0.5, label=col)\n",
    "axes[1].set_title('Min-Max Normalized (0-1)')\n",
    "axes[1].set_xlabel('Value')\n",
    "axes[1].legend()\n",
    "\n",
    "# Standardized\n",
    "for col in numeric_cols:\n",
    "    axes[2].hist(df_standardized[col], bins=20, alpha=0.5, label=col)\n",
    "axes[2].set_title('Standardized (mean=0, std=1)')\n",
    "axes[2].set_xlabel('Value')\n",
    "axes[2].axvline(x=0, color='black', linestyle='--')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex6-3",
   "metadata": {},
   "source": [
    "### Exercise 6.3: Train-Test Split Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex6-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6.3: Prepare Features and Target\n",
    "\n",
    "# Prepare final dataset for ML\n",
    "df_final = pd.get_dummies(df_clean, columns=['city', 'education'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "target_col = 'satisfaction'\n",
    "feature_cols = [col for col in df_final.columns if col != target_col and col != 'customer_id']\n",
    "\n",
    "X = df_final[feature_cols]\n",
    "y = df_final[target_col]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns ({len(feature_cols)}):\")\n",
    "print(feature_cols)\n",
    "\n",
    "# Manual train-test split\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X))\n",
    "split_idx = int(0.8 * len(X))\n",
    "\n",
    "train_idx = indices[:split_idx]\n",
    "test_idx = indices[split_idx:]\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nTrain target distribution:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab Summary\n",
    "\n",
    "Congratulations! You've mastered essential Pandas operations for data manipulation:\n",
    "\n",
    "| Topic | Key Functions |\n",
    "|-------|---------------|\n",
    "| **Creation** | `pd.DataFrame()`, `pd.read_csv()`, `pd.read_json()` |\n",
    "| **Exploration** | `head()`, `tail()`, `info()`, `describe()`, `shape`, `dtypes` |\n",
    "| **Selection** | `[]`, `.loc[]`, `.iloc[]`, boolean indexing, `.query()` |\n",
    "| **Missing Data** | `isnull()`, `fillna()`, `dropna()` |\n",
    "| **Duplicates** | `duplicated()`, `drop_duplicates()` |\n",
    "| **Transformation** | `apply()`, `map()`, `np.where()`, `pd.cut()` |\n",
    "| **Aggregation** | `groupby()`, `agg()`, `pivot_table()` |\n",
    "| **Merging** | `pd.merge()`, `pd.concat()`, join types |\n",
    "| **ML Prep** | `pd.get_dummies()`, normalization, standardization |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Lab 2:** Machine Learning with PyTorch\n",
    "- **Lab 3:** Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "*Remember to save your work! (Ctrl+S)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
