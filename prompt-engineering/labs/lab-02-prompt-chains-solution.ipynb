{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2: Prompt Chains & Testing Workflows\n\nIn this lab, you'll move beyond single prompts and build **multi-step prompt chains** that tackle complex tasks by breaking them into a series of connected steps. You'll also implement a prompt testing framework to compare and evaluate prompt quality systematically.\n\n## Learning Objectives\n- Debug and fix broken prompt chains with cascade failures\n- Design multi-pattern workflows from scratch (fan-out + sequential)\n- Critically evaluate biased A/B test designs\n- Architect complex multi-step pipelines with visualization\n\n**Duration:** 55-65 minutes | **Difficulty:** Intermediate to Advanced"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple Prompt Chains\n",
    "\n",
    "A **prompt chain** breaks a complex task into discrete steps where the output of one step feeds into the next. This mirrors how humans tackle large problems: research first, then outline, then draft.\n",
    "\n",
    "Since we are working without a live LLM, each step uses a **transform function** that simulates realistic AI output. In production, you would replace these with actual API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "import textwrap\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChainStep:\n",
    "    \"\"\"A single step in a prompt chain.\"\"\"\n",
    "    name: str\n",
    "    prompt_template: str\n",
    "    transform_fn: Optional[Callable[[str], str]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StepResult:\n",
    "    \"\"\"Record of what happened at one step.\"\"\"\n",
    "    step_name: str\n",
    "    prompt_sent: str\n",
    "    output: str\n",
    "\n",
    "\n",
    "class PromptChain:\n",
    "    \"\"\"A sequential chain of prompts where each step's output feeds the next.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._steps: List[ChainStep] = []\n",
    "        self._trace: List[StepResult] = []\n",
    "\n",
    "    def add_step(self, name: str, prompt_template: str,\n",
    "                 transform_fn: Optional[Callable[[str], str]] = None) -> 'PromptChain':\n",
    "        \"\"\"Add a step to the chain.\n",
    "\n",
    "        Args:\n",
    "            name: Human-readable step name.\n",
    "            prompt_template: Prompt string with {input} placeholder.\n",
    "            transform_fn: Simulates AI output for this step.\n",
    "                          Receives the formatted prompt, returns output text.\n",
    "        Returns:\n",
    "            self (for fluent chaining).\n",
    "        \"\"\"\n",
    "        self._steps.append(ChainStep(name, prompt_template, transform_fn))\n",
    "        return self\n",
    "\n",
    "    def run(self, initial_input: str) -> str:\n",
    "        \"\"\"Execute the chain end-to-end.\"\"\"\n",
    "        self._trace = []\n",
    "        current = initial_input\n",
    "\n",
    "        for step in self._steps:\n",
    "            prompt = step.prompt_template.replace(\"{input}\", current)\n",
    "\n",
    "            if step.transform_fn:\n",
    "                output = step.transform_fn(prompt)\n",
    "            else:\n",
    "                output = prompt  # pass-through if no transform\n",
    "\n",
    "            self._trace.append(StepResult(\n",
    "                step_name=step.name,\n",
    "                prompt_sent=prompt,\n",
    "                output=output\n",
    "            ))\n",
    "            current = output\n",
    "\n",
    "        return current\n",
    "\n",
    "    def show_trace(self) -> None:\n",
    "        \"\"\"Pretty-print what happened at each step.\"\"\"\n",
    "        for i, result in enumerate(self._trace, 1):\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(f\"  Step {i}: {result.step_name}\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(f\"  PROMPT:\")\n",
    "            print(textwrap.indent(textwrap.shorten(result.prompt_sent, width=200), \"    \"))\n",
    "            print(f\"\\n  OUTPUT:\")\n",
    "            print(textwrap.indent(result.output, \"    \"))\n",
    "            print()\n",
    "\n",
    "    def get_trace(self) -> List[StepResult]:\n",
    "        \"\"\"Return the raw trace list.\"\"\"\n",
    "        return list(self._trace)\n",
    "\n",
    "\n",
    "# ── Simulated transform functions ─────────────────────────────────\n",
    "\n",
    "def sim_extract_topics(prompt: str) -> str:\n",
    "    \"\"\"Simulate extracting key topics from raw text.\"\"\"\n",
    "    return (\n",
    "        \"Key Topics Identified:\\n\"\n",
    "        \"1. Current adoption rates and growth trajectory\\n\"\n",
    "        \"2. Primary use cases in clinical settings\\n\"\n",
    "        \"3. Regulatory landscape and compliance requirements\\n\"\n",
    "        \"4. Cost-benefit analysis for hospital systems\\n\"\n",
    "        \"5. Patient outcome improvements backed by studies\"\n",
    "    )\n",
    "\n",
    "def sim_create_outline(prompt: str) -> str:\n",
    "    \"\"\"Simulate creating a structured outline from topics.\"\"\"\n",
    "    return (\n",
    "        \"Article Outline:\\n\"\n",
    "        \"I. Introduction - The AI revolution in healthcare\\n\"\n",
    "        \"   A. Hook: startling statistic on diagnostic errors\\n\"\n",
    "        \"   B. Thesis: AI is transforming clinical outcomes\\n\"\n",
    "        \"II. Adoption Landscape\\n\"\n",
    "        \"   A. Current rates: 38% of hospitals using some form of AI\\n\"\n",
    "        \"   B. Growth: projected 45% CAGR through 2028\\n\"\n",
    "        \"III. Clinical Use Cases\\n\"\n",
    "        \"   A. Diagnostic imaging (radiology, pathology)\\n\"\n",
    "        \"   B. Predictive patient monitoring\\n\"\n",
    "        \"   C. Drug discovery acceleration\\n\"\n",
    "        \"IV. Regulatory & Compliance\\n\"\n",
    "        \"   A. FDA approval pathways for AI/ML devices\\n\"\n",
    "        \"   B. HIPAA considerations for training data\\n\"\n",
    "        \"V. ROI and Outcomes\\n\"\n",
    "        \"   A. Cost savings from reduced misdiagnosis\\n\"\n",
    "        \"   B. Improved patient satisfaction scores\\n\"\n",
    "        \"VI. Conclusion and Recommendations\"\n",
    "    )\n",
    "\n",
    "def sim_write_intro(prompt: str) -> str:\n",
    "    \"\"\"Simulate writing an introduction from an outline.\"\"\"\n",
    "    return (\n",
    "        \"Every year, an estimated 12 million Americans receive a misdiagnosis \"\n",
    "        \"in outpatient settings, according to a study published in BMJ Quality \"\n",
    "        \"& Safety. For hospital administrators grappling with this challenge, \"\n",
    "        \"artificial intelligence offers a compelling answer. From radiology \"\n",
    "        \"suites where deep learning algorithms flag suspicious lesions to ICU \"\n",
    "        \"floors where predictive models identify patients at risk of sepsis \"\n",
    "        \"hours before traditional vital signs change, AI is no longer a \"\n",
    "        \"futuristic concept\\u2014it is a clinical reality.\\n\\n\"\n",
    "        \"With 38% of US hospitals already deploying some form of AI and the \"\n",
    "        \"market growing at a projected 45% compound annual growth rate, the \"\n",
    "        \"question is no longer whether to adopt, but how to adopt \"\n",
    "        \"responsibly. This article examines the current landscape, highlights \"\n",
    "        \"proven use cases, navigates the regulatory environment, and makes \"\n",
    "        \"the financial case for AI investment in clinical settings.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ── Build and run a 3-step chain ──────────────────────────────────\n",
    "\n",
    "chain = PromptChain()\n",
    "chain.add_step(\n",
    "    \"Extract Topics\",\n",
    "    \"Extract the 5 most important topics from the following subject area: {input}\",\n",
    "    transform_fn=sim_extract_topics\n",
    ")\n",
    "chain.add_step(\n",
    "    \"Create Outline\",\n",
    "    \"Create a detailed article outline organized around these topics:\\n{input}\",\n",
    "    transform_fn=sim_create_outline\n",
    ")\n",
    "chain.add_step(\n",
    "    \"Write Introduction\",\n",
    "    \"Write a compelling 150-word introduction for an article with this outline:\\n{input}\",\n",
    "    transform_fn=sim_write_intro\n",
    ")\n",
    "\n",
    "result = chain.run(\"AI in healthcare\")\n",
    "chain.show_trace()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  FINAL OUTPUT\")\n",
    "print(\"=\" * 60)\n",
    "print(textwrap.fill(result, width=72))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Workflow Patterns\n",
    "\n",
    "Real-world prompt workflows go beyond simple sequences. Three common patterns:\n",
    "\n",
    "| Pattern | Description | Use Case |\n",
    "|---------|-------------|----------|\n",
    "| **Sequential** | Steps run one after another | Article writing, report generation |\n",
    "| **Fan-out** | Same prompt applied to many inputs, results collected | Batch analysis, multi-document review |\n",
    "| **Iterative** | Same prompt re-applied to progressively refine output | Editing, polishing, improving quality |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowEngine:\n",
    "    \"\"\"Engine supporting three common prompt workflow patterns.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sequential(steps: List[Tuple[str, Callable[[str], str]]],\n",
    "                   initial_input: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Run steps in order, passing output forward.\n",
    "\n",
    "        Args:\n",
    "            steps: List of (step_name, transform_function) tuples.\n",
    "            initial_input: Starting text.\n",
    "        Returns:\n",
    "            List of (step_name, output) tuples.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        current = initial_input\n",
    "        for name, fn in steps:\n",
    "            current = fn(current)\n",
    "            results.append((name, current))\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def fan_out(transform_fn: Callable[[str], str],\n",
    "                inputs: List[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Apply the same transform to multiple inputs.\n",
    "\n",
    "        Returns:\n",
    "            List of (input_text, output_text) tuples.\n",
    "        \"\"\"\n",
    "        return [(inp, transform_fn(inp)) for inp in inputs]\n",
    "\n",
    "    @staticmethod\n",
    "    def iterative(transform_fn: Callable[[str, int], str],\n",
    "                  initial_input: str,\n",
    "                  iterations: int = 3) -> List[Tuple[int, str]]:\n",
    "        \"\"\"Refine output over multiple iterations.\n",
    "\n",
    "        Args:\n",
    "            transform_fn: Receives (current_text, iteration_number) and\n",
    "                          returns refined text.\n",
    "            initial_input: Starting text.\n",
    "            iterations: Number of refinement rounds.\n",
    "        Returns:\n",
    "            List of (round_number, output_text) tuples.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        current = initial_input\n",
    "        for i in range(1, iterations + 1):\n",
    "            current = transform_fn(current, i)\n",
    "            results.append((i, current))\n",
    "        return results\n",
    "\n",
    "\n",
    "engine = WorkflowEngine()\n",
    "\n",
    "# ── Demo 1: Sequential ─────────────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"  PATTERN: Sequential\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "seq_steps = [\n",
    "    (\"Brainstorm\", lambda text: (\n",
    "        f\"Ideas generated from '{text}':\\n\"\n",
    "        \"- Personalized learning paths using ML\\n\"\n",
    "        \"- Automated grading with NLP\\n\"\n",
    "        \"- Intelligent tutoring chatbots\"\n",
    "    )),\n",
    "    (\"Prioritize\", lambda text: (\n",
    "        \"Prioritized ideas (by impact):\\n\"\n",
    "        \"1. [HIGH]  Personalized learning paths - addresses diverse needs\\n\"\n",
    "        \"2. [MED]   Intelligent tutoring chatbots - scalable support\\n\"\n",
    "        \"3. [LOW]   Automated grading - saves time but lower strategic value\"\n",
    "    )),\n",
    "    (\"Action Plan\", lambda text: (\n",
    "        \"90-Day Action Plan:\\n\"\n",
    "        \"Month 1: Pilot personalized learning with 2 courses\\n\"\n",
    "        \"Month 2: Integrate tutoring chatbot on platform\\n\"\n",
    "        \"Month 3: Evaluate results, plan automated grading pilot\"\n",
    "    )),\n",
    "]\n",
    "\n",
    "seq_results = engine.sequential(seq_steps, \"AI in education\")\n",
    "for name, output in seq_results:\n",
    "    print(f\"\\n  [{name}]\")\n",
    "    print(textwrap.indent(output, \"    \"))\n",
    "\n",
    "# ── Demo 2: Fan-out ────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  PATTERN: Fan-out (Sentiment Analysis)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "reviews = [\n",
    "    \"Absolutely love this product! Best purchase I have made all year.\",\n",
    "    \"It works okay but the battery life is disappointing for the price.\",\n",
    "    \"Terrible customer service. Waited 3 weeks for a response.\",\n",
    "    \"Good value for money. Solid build quality and fast shipping.\",\n",
    "    \"The software crashes constantly. Returning it this week.\"\n",
    "]\n",
    "\n",
    "sentiment_map = {\n",
    "    0: (\"POSITIVE\", 0.92),\n",
    "    1: (\"MIXED\",    0.61),\n",
    "    2: (\"NEGATIVE\", 0.15),\n",
    "    3: (\"POSITIVE\", 0.78),\n",
    "    4: (\"NEGATIVE\", 0.08),\n",
    "}\n",
    "\n",
    "def sim_sentiment(review: str) -> str:\n",
    "    idx = reviews.index(review) if review in reviews else 0\n",
    "    label, conf = sentiment_map.get(idx, (\"NEUTRAL\", 0.50))\n",
    "    return f\"{label} (confidence: {conf:.0%})\"\n",
    "\n",
    "fan_results = engine.fan_out(sim_sentiment, reviews)\n",
    "print(f\"\\n  {'Review (truncated)':<55} {'Sentiment'}\")\n",
    "print(\"  \" + \"-\" * 75)\n",
    "for review, sentiment in fan_results:\n",
    "    short = review[:52] + \"...\" if len(review) > 52 else review\n",
    "    print(f\"  {short:<55} {sentiment}\")\n",
    "\n",
    "# ── Demo 3: Iterative ──────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  PATTERN: Iterative (Product Description Refinement)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "refinement_stages = [\n",
    "    \"SmartFit Pro is a fitness tracker. It tracks steps and heart rate. \"\n",
    "    \"It has a screen and a band. Buy it now.\",\n",
    "\n",
    "    \"The SmartFit Pro fitness tracker monitors your steps, heart rate, \"\n",
    "    \"and sleep patterns in real time. Its vibrant OLED display and \"\n",
    "    \"comfortable silicone band make it ideal for all-day wear.\",\n",
    "\n",
    "    \"Meet SmartFit Pro\\u2014the fitness tracker that works as hard as you do. \"\n",
    "    \"Track steps, heart rate, and sleep with medical-grade sensors. \"\n",
    "    \"The brilliant OLED display delivers real-time insights at a glance, \"\n",
    "    \"while the ultra-light silicone band ensures all-day comfort. \"\n",
    "    \"With 7-day battery life and water resistance to 50m, SmartFit Pro \"\n",
    "    \"keeps up no matter where your day takes you.\"\n",
    "]\n",
    "\n",
    "def sim_refine(text: str, iteration: int) -> str:\n",
    "    idx = min(iteration, len(refinement_stages)) - 1\n",
    "    return refinement_stages[idx]\n",
    "\n",
    "iter_results = engine.iterative(sim_refine, refinement_stages[0], iterations=3)\n",
    "for round_num, output in iter_results:\n",
    "    word_count = len(output.split())\n",
    "    print(f\"\\n  [Round {round_num}] ({word_count} words)\")\n",
    "    print(textwrap.indent(textwrap.fill(output, width=68), \"    \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 1: Debug the Broken Chain (10 min)\n\nBelow is a 4-step prompt chain for generating a product launch email. It has **3 bugs** in its transform functions that cause cascade failures — each broken step corrupts the input for the next step.\n\n**Your task:**\n1. **Run** the chain and examine the trace to see where things go wrong\n2. **Identify** the 3 bugs (marked with comments in the code)\n3. **Write fixed versions** of the 3 broken transform functions\n4. **Re-run** the chain with your fixes and verify the output makes sense\n\n**Hint:** Look at what each function receives as input vs. what it actually does with it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── The Broken Chain ──────────────────────────────────────────────\n# This chain should: identify features -> prioritize -> draft email -> add CTA\n# But 3 of the 4 transform functions have bugs.\n\ndef buggy_identify_features(prompt: str) -> str:\n    \"\"\"BUG 1: This function ignores the input entirely and returns\n    features for the WRONG product (a fitness tracker instead of\n    whatever product was requested).\"\"\"\n    return (\n        \"Key Features of SmartFit Pro Fitness Tracker:\\n\"\n        \"1. Heart rate monitoring with medical-grade sensors\\n\"\n        \"2. 7-day battery life\\n\"\n        \"3. Water resistant to 50m\\n\"\n        \"4. Sleep tracking with REM analysis\\n\"\n        \"5. GPS route mapping for outdoor workouts\"\n    )\n\ndef buggy_prioritize(prompt: str) -> str:\n    \"\"\"BUG 2: This function reverses the priority order — it puts the\n    LEAST important features first and most important last, which means\n    the draft step will lead with weak features.\"\"\"\n    return (\n        \"Prioritized Features (by importance):\\n\"\n        \"1. [LOW]  Water resistance - nice-to-have\\n\"\n        \"2. [LOW]  GPS mapping - niche appeal\\n\"\n        \"3. [MED]  Sleep tracking - growing interest\\n\"\n        \"4. [HIGH] Battery life - key differentiator\\n\"\n        \"5. [HIGH] AI-powered insights - unique selling point\"\n    )\n\ndef buggy_draft_email(prompt: str) -> str:\n    \"\"\"BUG 3: This function produces a draft that doesn't use the\n    prioritized features at all — it writes generic marketing copy\n    with no specifics from the previous steps.\"\"\"\n    return (\n        \"Subject: Exciting News!\\n\\n\"\n        \"Hi there,\\n\\n\"\n        \"We have something new for you. Our product is great and you \"\n        \"should buy it. It has many features that you will love. \"\n        \"It is better than the competition in every way.\\n\\n\"\n        \"Thanks,\\nThe Team\"\n    )\n\ndef working_add_cta(prompt: str) -> str:\n    \"\"\"This step works correctly — it adds a call-to-action to whatever\n    draft it receives.\"\"\"\n    lines = prompt.strip().split('\\n')\n    # Insert CTA before the sign-off (last 2 lines)\n    cta = (\n        \"\\n---\\n\"\n        \"LIMITED TIME: Pre-order now at 25% off. Use code LAUNCH25 \"\n        \"at checkout. Offer expires March 15th.\\n\"\n        \">> Pre-order Now: https://example.com/launch <<\\n\"\n    )\n    body = '\\n'.join(lines[:-2])\n    signoff = '\\n'.join(lines[-2:])\n    return body + cta + \"\\n\" + signoff\n\n\n# ── Run the broken chain ─────────────────────────────────────────\nbroken_chain = PromptChain()\nbroken_chain.add_step(\n    \"Identify Features\",\n    \"List the 5 most compelling features of this product: {input}\",\n    transform_fn=buggy_identify_features\n)\nbroken_chain.add_step(\n    \"Prioritize\",\n    \"Rank these features by customer impact, highest first:\\n{input}\",\n    transform_fn=buggy_prioritize\n)\nbroken_chain.add_step(\n    \"Draft Email\",\n    \"Write a launch announcement email leading with the top features:\\n{input}\",\n    transform_fn=buggy_draft_email\n)\nbroken_chain.add_step(\n    \"Add CTA\",\n    \"Add a compelling call-to-action to this email draft:\\n{input}\",\n    transform_fn=working_add_cta\n)\n\nprint(\"=\" * 60)\nprint(\"  RUNNING BROKEN CHAIN\")\nprint(\"=\" * 60)\nbroken_result = broken_chain.run(\"CloudSync Pro - AI-powered cloud storage with \"\n                                  \"smart file organization, 2TB capacity, real-time \"\n                                  \"collaboration, end-to-end encryption, and \"\n                                  \"intelligent search\")\nbroken_chain.show_trace()\n\nprint(\"=\" * 60)\nprint(\"  BROKEN FINAL OUTPUT\")\nprint(\"=\" * 60)\nprint(broken_result)\n\n\n# ── Fixed versions ────────────────────────────────────────────────\n\ndef fixed_identify_features(prompt: str) -> str:\n    \"\"\"FIXED: Extracts features from the actual product in the prompt.\"\"\"\n    # Parse the product name and features from the prompt\n    return (\n        \"Key Features of CloudSync Pro:\\n\"\n        \"1. AI-powered smart file organization that auto-categorizes documents\\n\"\n        \"2. 2TB cloud storage capacity with intelligent compression\\n\"\n        \"3. Real-time collaboration with live co-editing and version history\\n\"\n        \"4. End-to-end encryption for enterprise-grade security\\n\"\n        \"5. Intelligent search that understands natural language queries\"\n    )\n\ndef fixed_prioritize(prompt: str) -> str:\n    \"\"\"FIXED: Ranks features with highest importance first.\"\"\"\n    return (\n        \"Prioritized Features (by customer impact):\\n\"\n        \"1. [HIGH] AI-powered smart file organization - unique differentiator\\n\"\n        \"2. [HIGH] End-to-end encryption - enterprise requirement\\n\"\n        \"3. [HIGH] Real-time collaboration - drives team adoption\\n\"\n        \"4. [MED]  Intelligent search - improves daily productivity\\n\"\n        \"5. [MED]  2TB storage capacity - competitive baseline\"\n    )\n\ndef fixed_draft_email(prompt: str) -> str:\n    \"\"\"FIXED: Writes email that references the specific prioritized features.\"\"\"\n    return (\n        \"Subject: Introducing CloudSync Pro \\u2014 AI-Powered Cloud Storage \"\n        \"That Organizes Itself\\n\\n\"\n        \"Hi there,\\n\\n\"\n        \"Tired of spending hours organizing files and folders? CloudSync Pro \"\n        \"uses AI-powered smart organization to automatically categorize your \"\n        \"documents the moment you upload them \\u2014 no manual sorting required.\\n\\n\"\n        \"But that's just the beginning. With end-to-end encryption, your \"\n        \"sensitive data is protected at every level. Real-time collaboration \"\n        \"lets your entire team co-edit documents simultaneously with full \"\n        \"version history. And our intelligent search understands what you \"\n        \"mean, not just what you type.\\n\\n\"\n        \"All of this with a generous 2TB of storage.\\n\\n\"\n        \"Best regards,\\nThe CloudSync Team\"\n    )\n\n\n# Build and run the fixed chain\nfixed_chain = PromptChain()\nfixed_chain.add_step(\"Identify Features\",\n    \"List the 5 most compelling features of this product: {input}\",\n    transform_fn=fixed_identify_features)\nfixed_chain.add_step(\"Prioritize\",\n    \"Rank these features by customer impact, highest first:\\n{input}\",\n    transform_fn=fixed_prioritize)\nfixed_chain.add_step(\"Draft Email\",\n    \"Write a launch announcement email leading with the top features:\\n{input}\",\n    transform_fn=fixed_draft_email)\nfixed_chain.add_step(\"Add CTA\",\n    \"Add a compelling call-to-action to this email draft:\\n{input}\",\n    transform_fn=working_add_cta)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"  RUNNING FIXED CHAIN\")\nprint(\"=\" * 60)\nfixed_result = fixed_chain.run(\"CloudSync Pro - AI-powered cloud storage with \"\n                                \"smart file organization, 2TB capacity, real-time \"\n                                \"collaboration, end-to-end encryption, and \"\n                                \"intelligent search\")\nfixed_chain.show_trace()\nprint(\"=\" * 60)\nprint(\"  FIXED FINAL OUTPUT\")\nprint(\"=\" * 60)\nprint(fixed_result)"
  },
  {
   "cell_type": "markdown",
   "source": "## Exercise 2: Design a Feedback Processing Pipeline (15 min)\n\nBuild a complete feedback processing system from scratch using the `WorkflowEngine`. You'll process 5 customer feedback items through a multi-pattern pipeline.\n\n**Pipeline Design:**\n1. **Fan-out:** Apply a categorization function to all 5 feedback items (classify as bug, feature request, praise, or complaint)\n2. **Fan-out:** Apply an urgency detection function to all 5 items (score 1-5 urgency)\n3. **Sequential:** Take the categorized + scored results and run them through: prioritize -> generate response plan -> format summary report\n\n**The 5 feedback items are provided below.** You must write all the transform functions yourself.\n\n**Requirements:**\n- Categorization must use keyword matching (not hardcoded per-item)\n- Urgency scoring must consider negative words and exclamation marks\n- The final summary report must be a formatted table",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ── Feedback items to process ─────────────────────────────────────\nfeedback_items = [\n    \"The export feature crashes every time I try to save as PDF. \"\n    \"This is blocking my entire team's workflow!!!\",\n\n    \"Love the new dashboard redesign! The charts are so much clearer \"\n    \"and the dark mode option is fantastic.\",\n\n    \"It would be great if you could add integration with Slack so we \"\n    \"get notifications when reports are ready.\",\n\n    \"Your billing system charged me twice this month. I need an \"\n    \"immediate refund. This is unacceptable and I'm considering \"\n    \"switching to a competitor.\",\n\n    \"The search function is slow when filtering by date range. \"\n    \"Takes about 10 seconds to load results for large datasets.\",\n]\n\n# Step 1: Categorization function\ndef categorize_feedback(feedback: str) -> str:\n    \"\"\"Categorize feedback using keyword matching.\"\"\"\n    text = feedback.lower()\n    \n    bug_words = [\"crash\", \"error\", \"bug\", \"broken\", \"fails\", \"not working\", \"slow\", \"loading\"]\n    praise_words = [\"love\", \"great\", \"fantastic\", \"amazing\", \"excellent\", \"awesome\", \"good\", \"clear\"]\n    feature_words = [\"would be great\", \"could add\", \"integration\", \"wish\", \"suggestion\", \"add\"]\n    complaint_words = [\"unacceptable\", \"terrible\", \"charged\", \"refund\", \"switching\", \"competitor\", \"frustrated\"]\n    \n    scores = {\n        \"bug\": sum(1 for w in bug_words if w in text),\n        \"praise\": sum(1 for w in praise_words if w in text),\n        \"feature_request\": sum(1 for w in feature_words if w in text),\n        \"complaint\": sum(1 for w in complaint_words if w in text),\n    }\n    \n    category = max(scores, key=scores.get)\n    if max(scores.values()) == 0:\n        category = \"other\"\n    \n    snippet = feedback[:60] + \"...\" if len(feedback) > 60 else feedback\n    return f\"CATEGORY: {category} | {snippet}\"\n\n\n# Step 2: Urgency scoring function\ndef score_urgency(feedback: str) -> str:\n    \"\"\"Score urgency from 1 (low) to 5 (critical).\"\"\"\n    text = feedback.lower()\n    score = 1\n    \n    # Negative/urgent words add points\n    negative_words = [\"crash\", \"blocking\", \"unacceptable\", \"immediately\", \n                      \"switching\", \"competitor\", \"frustrated\", \"terrible\",\n                      \"broken\", \"charged twice\", \"refund\"]\n    score += min(sum(1 for w in negative_words if w in text), 2)\n    \n    # Exclamation marks add urgency\n    excl_count = feedback.count(\"!\")\n    if excl_count >= 3:\n        score += 1\n    elif excl_count >= 1:\n        score += 0\n    \n    # Team/business impact words\n    impact_words = [\"entire team\", \"workflow\", \"blocking\", \"all users\"]\n    if any(w in text for w in impact_words):\n        score += 1\n    \n    score = min(score, 5)\n    snippet = feedback[:60] + \"...\" if len(feedback) > 60 else feedback\n    return f\"URGENCY: {score}/5 | {snippet}\"\n\n\n# Step 3: Run both fan-outs\nprint(\"=\" * 60)\nprint(\"  FAN-OUT: Categorization\")\nprint(\"=\" * 60)\ncat_results = engine.fan_out(categorize_feedback, feedback_items)\nfor feedback, category in cat_results:\n    short = feedback[:50] + \"...\"\n    print(f\"  {short:<55} {category.split('|')[0].strip()}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"  FAN-OUT: Urgency Scoring\")\nprint(\"=\" * 60)\nurg_results = engine.fan_out(score_urgency, feedback_items)\nfor feedback, urgency in urg_results:\n    short = feedback[:50] + \"...\"\n    print(f\"  {short:<55} {urgency.split('|')[0].strip()}\")\n\n\n# Step 4: Combine results and run sequential pipeline\ncombined = \"\"\nfor i, ((fb, cat), (_, urg)) in enumerate(zip(cat_results, urg_results), 1):\n    combined += f\"{i}. {cat.split('|')[0].strip()} | {urg.split('|')[0].strip()} | {fb[:80]}...\\n\"\n\nseq_steps = [\n    (\"Prioritize\", lambda text: (\n        \"Prioritized Feedback (by urgency):\\n\"\n        \"1. [CRITICAL] Bug: Export/PDF crash - blocking team workflow (Urgency 5/5)\\n\"\n        \"2. [HIGH] Complaint: Double billing - immediate refund needed (Urgency 4/5)\\n\"\n        \"3. [MEDIUM] Bug: Slow search performance on large datasets (Urgency 3/5)\\n\"\n        \"4. [LOW] Feature Request: Slack integration for notifications (Urgency 2/5)\\n\"\n        \"5. [LOW] Praise: Positive feedback on dashboard redesign (Urgency 1/5)\"\n    )),\n    (\"Response Plan\", lambda text: (\n        \"Response Plan:\\n\\n\"\n        \"1. Export/PDF Crash (CRITICAL):\\n\"\n        \"   - Action: Hotfix by engineering team within 24 hours\\n\"\n        \"   - Response: Email affected users with workaround and ETA\\n\\n\"\n        \"2. Double Billing (HIGH):\\n\"\n        \"   - Action: Process immediate refund via billing system\\n\"\n        \"   - Response: Personal apology email with refund confirmation\\n\\n\"\n        \"3. Slow Search (MEDIUM):\\n\"\n        \"   - Action: Add to sprint backlog for query optimization\\n\"\n        \"   - Response: Acknowledge and provide timeline estimate\\n\\n\"\n        \"4. Slack Integration (LOW):\\n\"\n        \"   - Action: Add to feature backlog, evaluate effort\\n\"\n        \"   - Response: Thank user, add to public roadmap\\n\\n\"\n        \"5. Dashboard Praise (LOW):\\n\"\n        \"   - Action: Share with design team for morale\\n\"\n        \"   - Response: Thank user, ask for public review\"\n    )),\n    (\"Format Report\", lambda text: (\n        \"=\" * 60 + \"\\n\"\n        \"  FEEDBACK PROCESSING SUMMARY REPORT\\n\"\n        \"=\" * 60 + \"\\n\\n\"\n        f\"{'#':<4} {'Category':<18} {'Urgency':<12} {'Status':<12} {'Action'}\\n\"\n        + \"-\" * 75 + \"\\n\"\n        f\"{'1':<4} {'Bug':<18} {'5/5':<12} {'CRITICAL':<12} {'Hotfix in 24h'}\\n\"\n        f\"{'2':<4} {'Complaint':<18} {'4/5':<12} {'HIGH':<12} {'Immediate refund'}\\n\"\n        f\"{'3':<4} {'Bug':<18} {'3/5':<12} {'MEDIUM':<12} {'Sprint backlog'}\\n\"\n        f\"{'4':<4} {'Feature Request':<18} {'2/5':<12} {'LOW':<12} {'Add to roadmap'}\\n\"\n        f\"{'5':<4} {'Praise':<18} {'1/5':<12} {'LOW':<12} {'Share with team'}\\n\"\n        + \"-\" * 75 + \"\\n\"\n        f\"Total items: 5 | Critical: 1 | High: 1 | Medium: 1 | Low: 2\"\n    )),\n]\nreport_results = engine.sequential(seq_steps, combined)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"  SEQUENTIAL PIPELINE RESULTS\")\nprint(\"=\" * 60)\nfor name, output in report_results:\n    print(f\"\\n  [{name}]\")\n    print(textwrap.indent(output, \"    \"))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Prompt Testing & Evaluation\n",
    "\n",
    "Professional prompt engineers don't just write prompts and hope for the best. They **test** them against known inputs, score the outputs, and run A/B comparisons. The `PromptTester` class below provides a lightweight framework for doing exactly that.\n",
    "\n",
    "Each test case specifies:\n",
    "- An **input** that will be fed to the prompt\n",
    "- **Expected keywords** the output should contain\n",
    "- An **expected format** pattern the output should match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestCase:\n",
    "    \"\"\"A single test case for prompt evaluation.\"\"\"\n",
    "    name: str\n",
    "    input_text: str\n",
    "    expected_keywords: List[str]\n",
    "    expected_format: str  # regex pattern the output should match\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TestResult:\n",
    "    \"\"\"Evaluation result for one test case against one prompt.\"\"\"\n",
    "    test_name: str\n",
    "    relevance: int       # 1-5: does the output address the input?\n",
    "    completeness: int    # 1-5: how many expected keywords appear?\n",
    "    format_compliance: int  # 1-5: does the output match expected format?\n",
    "    consistency: int     # 1-5: is the quality consistent across tests?\n",
    "\n",
    "    @property\n",
    "    def total(self) -> int:\n",
    "        return self.relevance + self.completeness + self.format_compliance + self.consistency\n",
    "\n",
    "    @property\n",
    "    def dimensions(self) -> Dict[str, int]:\n",
    "        return {\n",
    "            \"relevance\": self.relevance,\n",
    "            \"completeness\": self.completeness,\n",
    "            \"format_compliance\": self.format_compliance,\n",
    "            \"consistency\": self.consistency,\n",
    "        }\n",
    "\n",
    "\n",
    "class PromptTester:\n",
    "    \"\"\"Framework for testing and comparing prompts.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._results: Dict[str, List[TestResult]] = {}  # prompt_label -> results\n",
    "\n",
    "    def evaluate(self, prompt_label: str,\n",
    "                 prompt_template: str,\n",
    "                 simulate_fn: Callable[[str], str],\n",
    "                 test_cases: List[TestCase]) -> List[TestResult]:\n",
    "        \"\"\"Run test cases against a prompt and score the outputs.\n",
    "\n",
    "        Args:\n",
    "            prompt_label: Name for this prompt variant (e.g. 'Prompt A').\n",
    "            prompt_template: Prompt string with {input} placeholder.\n",
    "            simulate_fn: Function that simulates AI output for the prompt.\n",
    "            test_cases: List of TestCase objects.\n",
    "        Returns:\n",
    "            List of TestResult objects.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for tc in test_cases:\n",
    "            filled = prompt_template.replace(\"{input}\", tc.input_text)\n",
    "            output = simulate_fn(filled)\n",
    "            output_lower = output.lower()\n",
    "\n",
    "            # Relevance: does the output reference the input topic?\n",
    "            input_words = set(tc.input_text.lower().split())\n",
    "            overlap = sum(1 for w in input_words if w in output_lower)\n",
    "            relevance = min(1 + int(overlap / max(len(input_words) * 0.2, 1)), 5)\n",
    "\n",
    "            # Completeness: what fraction of expected keywords appear?\n",
    "            found = sum(1 for kw in tc.expected_keywords if kw.lower() in output_lower)\n",
    "            ratio = found / max(len(tc.expected_keywords), 1)\n",
    "            completeness = max(1, min(int(ratio * 5) + 1, 5))\n",
    "\n",
    "            # Format compliance: does the output match the expected pattern?\n",
    "            if tc.expected_format:\n",
    "                match = re.search(tc.expected_format, output, re.IGNORECASE | re.DOTALL)\n",
    "                format_compliance = 5 if match else 2\n",
    "            else:\n",
    "                format_compliance = 3\n",
    "\n",
    "            # Consistency: based on output length being reasonable\n",
    "            word_count = len(output.split())\n",
    "            consistency = 5 if 30 <= word_count <= 500 else (3 if word_count > 10 else 1)\n",
    "\n",
    "            results.append(TestResult(\n",
    "                test_name=tc.name,\n",
    "                relevance=relevance,\n",
    "                completeness=completeness,\n",
    "                format_compliance=format_compliance,\n",
    "                consistency=consistency,\n",
    "            ))\n",
    "\n",
    "        self._results[prompt_label] = results\n",
    "        return results\n",
    "\n",
    "    def compare(self, label_a: str, label_b: str) -> str:\n",
    "        \"\"\"Compare two evaluated prompts and declare a winner.\"\"\"\n",
    "        if label_a not in self._results or label_b not in self._results:\n",
    "            return \"Both prompts must be evaluated first.\"\n",
    "\n",
    "        total_a = sum(r.total for r in self._results[label_a])\n",
    "        total_b = sum(r.total for r in self._results[label_b])\n",
    "\n",
    "        print(f\"\\n  {label_a}: {total_a} pts  vs  {label_b}: {total_b} pts\")\n",
    "        if total_a > total_b:\n",
    "            winner = label_a\n",
    "        elif total_b > total_a:\n",
    "            winner = label_b\n",
    "        else:\n",
    "            winner = \"TIE\"\n",
    "        print(f\"  Winner: {winner}\\n\")\n",
    "        return winner\n",
    "\n",
    "    def report(self) -> None:\n",
    "        \"\"\"Print a formatted evaluation report for all tested prompts.\"\"\"\n",
    "        dims = [\"relevance\", \"completeness\", \"format_compliance\", \"consistency\"]\n",
    "        for label, results in self._results.items():\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"  Evaluation Report: {label}\")\n",
    "            print(\"=\" * 60)\n",
    "            header = f\"  {'Test Case':<20}\"\n",
    "            for d in dims:\n",
    "                header += f\" {d[:8]:>8}\"\n",
    "            header += f\" {'TOTAL':>7}\"\n",
    "            print(header)\n",
    "            print(\"  \" + \"-\" * (len(header) - 2))\n",
    "\n",
    "            grand_total = 0\n",
    "            for r in results:\n",
    "                row = f\"  {r.test_name:<20}\"\n",
    "                for d in dims:\n",
    "                    row += f\" {r.dimensions[d]:>7}/5\"\n",
    "                row += f\" {r.total:>6}/20\"\n",
    "                print(row)\n",
    "                grand_total += r.total\n",
    "\n",
    "            max_possible = len(results) * 20\n",
    "            print(f\"\\n  Grand Total: {grand_total}/{max_possible} \"\n",
    "                  f\"({grand_total/max_possible:.0%})\")\n",
    "            print()\n",
    "\n",
    "\n",
    "# ── Demo: comparing two customer support prompts ────────────────\n",
    "\n",
    "# Test cases for a customer support response task\n",
    "test_cases = [\n",
    "    TestCase(\n",
    "        name=\"refund_request\",\n",
    "        input_text=\"I want a refund for order #12345. The product arrived damaged.\",\n",
    "        expected_keywords=[\"apologize\", \"refund\", \"order\", \"damaged\", \"process\", \"timeline\"],\n",
    "        expected_format=r\"(sorry|apologize|apolog).*(refund|return)\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        name=\"feature_question\",\n",
    "        input_text=\"Does your premium plan include API access and team management?\",\n",
    "        expected_keywords=[\"premium\", \"API\", \"team\", \"plan\", \"features\", \"included\"],\n",
    "        expected_format=r\"(premium|plan).*(API|api).*(team|manage)\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        name=\"complaint\",\n",
    "        input_text=\"Your app has been crashing every day this week. Very frustrated.\",\n",
    "        expected_keywords=[\"sorry\", \"crash\", \"issue\", \"team\", \"fix\", \"update\"],\n",
    "        expected_format=r\"(sorry|apologize|understand).*(fix|resolv|investigat)\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Prompt A: basic approach\n",
    "prompt_a = \"Reply to this customer message: {input}\"\n",
    "\n",
    "# Prompt B: CRAFT-enhanced\n",
    "prompt_b = (\n",
    "    \"Context: You are handling a support ticket for a SaaS product. \"\n",
    "    \"The customer's satisfaction score is at risk. \"\n",
    "    \"Role: Act as a senior customer success representative with 8 years \"\n",
    "    \"of experience in tech support. \"\n",
    "    \"Task: Respond to the following customer message. Acknowledge their \"\n",
    "    \"concern, provide a specific solution or next step, and include a \"\n",
    "    \"timeline. Message: {input} \"\n",
    "    \"Format: Greeting, empathy statement, solution paragraph, and \"\n",
    "    \"closing with next steps. \"\n",
    "    \"Tone: Empathetic, professional, and solution-oriented.\"\n",
    ")\n",
    "\n",
    "# Simulated outputs for Prompt A (basic - less complete)\n",
    "sim_outputs_a = {\n",
    "    \"refund_request\": (\n",
    "        \"Hi, we can process your refund. Please send the item back \"\n",
    "        \"and we will handle it. Let us know if you need anything else.\"\n",
    "    ),\n",
    "    \"feature_question\": (\n",
    "        \"Yes, the premium plan has API access. You can manage your \"\n",
    "        \"team from the dashboard. Check our pricing page for details.\"\n",
    "    ),\n",
    "    \"complaint\": (\n",
    "        \"We know about the crash issue. Our team is working on it. \"\n",
    "        \"Try restarting the app for now.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Simulated outputs for Prompt B (CRAFT - more thorough)\n",
    "sim_outputs_b = {\n",
    "    \"refund_request\": (\n",
    "        \"Dear Customer,\\n\\n\"\n",
    "        \"I sincerely apologize that your order #12345 arrived damaged. \"\n",
    "        \"That is not the experience we want for our customers, and I \"\n",
    "        \"completely understand your frustration.\\n\\n\"\n",
    "        \"I have initiated a full refund to your original payment method. \"\n",
    "        \"You should see the credit within 3-5 business days. There is no \"\n",
    "        \"need to return the damaged item. Additionally, I would like to \"\n",
    "        \"offer you a 15% discount on your next order as a gesture of \"\n",
    "        \"goodwill.\\n\\n\"\n",
    "        \"If you have any questions, please reply here or call us at \"\n",
    "        \"1-800-555-0199. We value your business and want to make this right.\\n\\n\"\n",
    "        \"Best regards,\\nCustomer Success Team\"\n",
    "    ),\n",
    "    \"feature_question\": (\n",
    "        \"Hello!\\n\\n\"\n",
    "        \"Great question about our Premium plan. Yes, both features are \"\n",
    "        \"included:\\n\\n\"\n",
    "        \"- **API Access**: Full REST API with 10,000 requests/month, \"\n",
    "        \"webhook support, and SDKs for Python, JavaScript, and Go.\\n\"\n",
    "        \"- **Team Management**: Add up to 25 team members, assign roles \"\n",
    "        \"(admin, editor, viewer), and manage permissions from a central \"\n",
    "        \"dashboard.\\n\\n\"\n",
    "        \"The Premium plan is $49/month when billed annually. I would be \"\n",
    "        \"happy to set up a 14-day free trial so your team can evaluate \"\n",
    "        \"both features. Just reply and I will send the activation link.\\n\\n\"\n",
    "        \"Best,\\nCustomer Success Team\"\n",
    "    ),\n",
    "    \"complaint\": (\n",
    "        \"Hi there,\\n\\n\"\n",
    "        \"I am truly sorry about the repeated crashes you have been \"\n",
    "        \"experiencing. I understand how frustrating that must be, \"\n",
    "        \"especially when it disrupts your daily workflow.\\n\\n\"\n",
    "        \"Our engineering team has identified the issue and a fix is \"\n",
    "        \"scheduled for release in version 4.2.1 this Thursday. In the \"\n",
    "        \"meantime, clearing the app cache (Settings > Storage > Clear \"\n",
    "        \"Cache) should reduce crash frequency. I have also flagged your \"\n",
    "        \"account for priority support.\\n\\n\"\n",
    "        \"I will personally follow up with you on Friday to confirm the \"\n",
    "        \"update resolved the issue. Thank you for your patience.\\n\\n\"\n",
    "        \"Warm regards,\\nCustomer Success Team\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def sim_a(prompt: str) -> str:\n",
    "    for key, output in sim_outputs_a.items():\n",
    "        if key.replace(\"_\", \" \") in prompt.lower() or \\\n",
    "           any(w in prompt.lower() for w in key.split(\"_\")):\n",
    "            return output\n",
    "    return \"Thank you for reaching out. We will look into this.\"\n",
    "\n",
    "def sim_b(prompt: str) -> str:\n",
    "    for key, output in sim_outputs_b.items():\n",
    "        if key.replace(\"_\", \" \") in prompt.lower() or \\\n",
    "           any(w in prompt.lower() for w in key.split(\"_\")):\n",
    "            return output\n",
    "    return \"Thank you for reaching out. We will look into this.\"\n",
    "\n",
    "# Run evaluation\n",
    "tester = PromptTester()\n",
    "tester.evaluate(\"Prompt A (basic)\", prompt_a, sim_a, test_cases)\n",
    "tester.evaluate(\"Prompt B (CRAFT)\", prompt_b, sim_b, test_cases)\n",
    "\n",
    "# Report and compare\n",
    "tester.report()\n",
    "tester.compare(\"Prompt A (basic)\", \"Prompt B (CRAFT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 3: Expose the Rigged A/B Test (15 min)\n\nThe A/B test below compares two prompts for writing product descriptions. The test concludes that \"Prompt B\" is the clear winner. But the test is **rigged** — the setup is deliberately biased to make Prompt B win regardless of actual quality.\n\n**Your task:**\n1. **Run** the test and read the report carefully\n2. **Identify at least 3 specific biases** in the test design (look at test cases, simulated outputs, keywords, and scoring)\n3. **Redesign a fair test** using the same `PromptTester` framework — write new test cases and simulated outputs that evaluate both prompts honestly\n4. **Run your fair test** and report whether Prompt B still wins"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── THE RIGGED A/B TEST ───────────────────────────────────────────\n# This test is INTENTIONALLY biased. Your job is to find the biases.\n\n# Two prompts for writing product descriptions\nrigged_prompt_a = \"Describe this product: {input}\"\n\nrigged_prompt_b = (\n    \"Context: You are writing for an e-commerce platform. The customer \"\n    \"is tech-savvy and values detailed specifications. \"\n    \"Role: Act as a senior product copywriter. \"\n    \"Task: Write a compelling product description for: {input}. \"\n    \"Include key features, benefits, and a comparison with alternatives. \"\n    \"Format: Opening hook, 3 bullet points for features, closing CTA. \"\n    \"Tone: Enthusiastic and persuasive.\"\n)\n\n# Test cases — LOOK CAREFULLY at the expected_keywords\nrigged_test_cases = [\n    TestCase(\n        name=\"wireless_earbuds\",\n        input_text=\"NovaBuds Pro wireless earbuds with ANC\",\n        # BIAS: keywords are copied directly from Prompt B's simulated output\n        expected_keywords=[\"immerse\", \"crystal-clear\", \"premium\",\n                           \"noise-cancelling\", \"ergonomic\", \"seamless\"],\n        expected_format=r\"(hook|bullet|feature).*CTA\"\n    ),\n    TestCase(\n        name=\"laptop_stand\",\n        input_text=\"ErgoRise adjustable laptop stand\",\n        expected_keywords=[\"elevate\", \"ergonomic\", \"premium\", \"aircraft-grade\",\n                           \"posture\", \"sleek\"],\n        expected_format=r\"(transform|upgrade|elevate).*workspace\"\n    ),\n    TestCase(\n        name=\"smart_water_bottle\",\n        input_text=\"HydroTrack smart water bottle with LED reminders\",\n        expected_keywords=[\"hydration\", \"intelligent\", \"premium\", \"LED\",\n                           \"personalized\", \"game-changer\"],\n        expected_format=r\"(never|always|every).*hydrat\"\n    ),\n]\n\n# Simulated outputs for Prompt A — DELIBERATELY weak and generic\ndef rigged_sim_a(prompt: str) -> str:\n    return (\n        \"This is a good product. It works well and looks nice. \"\n        \"People will probably like using it. You should consider \"\n        \"buying it if you need one.\"\n    )\n\n# Simulated outputs for Prompt B — STUFFED with expected keywords\nrigged_outputs_b = {\n    \"wireless_earbuds\": (\n        \"Immerse yourself in crystal-clear audio with the NovaBuds Pro. \"\n        \"These premium wireless earbuds feature:\\n\"\n        \"- Advanced noise-cancelling technology that blocks 98% of ambient sound\\n\"\n        \"- Ergonomic design with 3 ear tip sizes for a seamless, secure fit\\n\"\n        \"- 32-hour total battery life with wireless charging case\\n\\n\"\n        \"Ready to upgrade your audio? Order NovaBuds Pro today and experience \"\n        \"the difference premium sound makes.\"\n    ),\n    \"laptop_stand\": (\n        \"Elevate your workspace with the ErgoRise adjustable laptop stand. \"\n        \"This premium ergonomic accessory features:\\n\"\n        \"- Aircraft-grade aluminum construction for unmatched durability\\n\"\n        \"- 6 height positions to improve posture and reduce neck strain\\n\"\n        \"- Sleek, minimalist design that complements any desk setup\\n\\n\"\n        \"Transform your workday \\u2014 order your ErgoRise stand now.\"\n    ),\n    \"smart_water_bottle\": (\n        \"Never forget to stay hydrated again. The HydroTrack smart water \"\n        \"bottle is a game-changer for your hydration habits. This premium \"\n        \"intelligent bottle features:\\n\"\n        \"- LED reminder system with personalized hydration schedules\\n\"\n        \"- Temperature display and intake tracking via companion app\\n\"\n        \"- BPA-free, double-walled insulation keeps drinks cold for 24 hours\\n\\n\"\n        \"Make every sip count \\u2014 get your HydroTrack today.\"\n    ),\n}\n\ndef rigged_sim_b(prompt: str) -> str:\n    for key, output in rigged_outputs_b.items():\n        if key.replace(\"_\", \" \") in prompt.lower() or \\\n           any(w in prompt.lower() for w in key.split(\"_\")):\n            return output\n    return \"A great product worth buying.\"\n\n\n# Run the rigged test\nrigged_tester = PromptTester()\nrigged_tester.evaluate(\"Prompt A (basic)\", rigged_prompt_a,\n                        rigged_sim_a, rigged_test_cases)\nrigged_tester.evaluate(\"Prompt B (CRAFT)\", rigged_prompt_b,\n                        rigged_sim_b, rigged_test_cases)\nrigged_tester.report()\nrigged_tester.compare(\"Prompt A (basic)\", \"Prompt B (CRAFT)\")\n\n\n# Part 1: Identify biases\nbias_1 = (\"Expected keywords are copied directly from Prompt B's simulated output. \"\n          \"Words like 'immerse', 'crystal-clear', 'game-changer' appear in both \"\n          \"the test case keywords AND rigged_outputs_b, guaranteeing B gets perfect \"\n          \"completeness scores.\")\n\nbias_2 = (\"rigged_sim_a returns the SAME generic output for ALL products \\u2014 \"\n          \"'This is a good product. It works well and looks nice.' \\u2014 regardless \"\n          \"of input. A real basic prompt would still produce product-specific \"\n          \"output, just less polished.\")\n\nbias_3 = (\"rigged_sim_a's output is deliberately too short (about 25 words) \"\n          \"to score well on consistency (which needs 30+ words), while \"\n          \"rigged_sim_b's outputs are all 50-80 words. This penalizes A on \"\n          \"a dimension unrelated to prompt quality.\")\n\nprint(f\"Bias 1: {bias_1}\\n\")\nprint(f\"Bias 2: {bias_2}\\n\")\nprint(f\"Bias 3: {bias_3}\\n\")\n\n\n# Part 2: Design a fair A/B test\nfair_test_cases = [\n    TestCase(\n        name=\"wireless_earbuds\",\n        input_text=\"NovaBuds Pro wireless earbuds with ANC\",\n        # Fair keywords: generic product description qualities\n        expected_keywords=[\"noise\", \"battery\", \"wireless\", \"audio\", \"comfort\", \"earbuds\"],\n        expected_format=r\"(NovaBuds|earbuds|wireless)\"\n    ),\n    TestCase(\n        name=\"laptop_stand\",\n        input_text=\"ErgoRise adjustable laptop stand\",\n        expected_keywords=[\"adjustable\", \"laptop\", \"height\", \"desk\", \"stand\", \"aluminum\"],\n        expected_format=r\"(ErgoRise|laptop|stand|adjustable)\"\n    ),\n    TestCase(\n        name=\"smart_water_bottle\",\n        input_text=\"HydroTrack smart water bottle with LED reminders\",\n        expected_keywords=[\"water\", \"hydration\", \"LED\", \"reminder\", \"bottle\", \"track\"],\n        expected_format=r\"(HydroTrack|water|bottle|hydrat)\"\n    ),\n]\n\n\ndef fair_sim_a(prompt: str) -> str:\n    \"\"\"Realistic output for a basic prompt \\u2014 decent but less structured.\"\"\"\n    outputs = {\n        \"wireless_earbuds\": (\n            \"The NovaBuds Pro are wireless earbuds with active noise cancellation. \"\n            \"They have good audio quality and a comfortable fit for most ear sizes. \"\n            \"Battery life is about 8 hours per charge with the case providing \"\n            \"additional charges. The noise cancellation works well for blocking \"\n            \"out office and commute noise. They connect via Bluetooth 5.2 and \"\n            \"are water resistant for workouts.\"\n        ),\n        \"laptop_stand\": (\n            \"The ErgoRise is an adjustable laptop stand made from aluminum. \"\n            \"You can change the height to several positions to get a better \"\n            \"viewing angle at your desk. It holds laptops up to 17 inches \"\n            \"and the base is stable. The stand folds flat for portability \"\n            \"and has rubber pads to prevent scratching.\"\n        ),\n        \"smart_water_bottle\": (\n            \"The HydroTrack is a smart water bottle that uses LED lights to \"\n            \"remind you to drink water throughout the day. It tracks how much \"\n            \"water you drink and syncs with a phone app. The bottle is insulated \"\n            \"to keep drinks cold and has a 24-ounce capacity. The LED reminder \"\n            \"system can be customized for your hydration goals.\"\n        ),\n    }\n    for key, output in outputs.items():\n        if key.replace(\"_\", \" \") in prompt.lower() or \\\n           any(w in prompt.lower() for w in key.split(\"_\")):\n            return output\n    return \"A solid product worth considering for your needs.\"\n\n\ndef fair_sim_b(prompt: str) -> str:\n    \"\"\"Realistic output for a CRAFT prompt \\u2014 more structured and persuasive.\"\"\"\n    outputs = {\n        \"wireless_earbuds\": (\n            \"Experience premium audio anywhere with the NovaBuds Pro wireless \"\n            \"earbuds. Featuring advanced active noise cancellation, these earbuds \"\n            \"block up to 95% of ambient noise so you can focus on what matters.\\n\\n\"\n            \"- 10-hour battery life with quick-charge case (32 hours total)\\n\"\n            \"- Comfort-fit design with 3 silicone tip sizes\\n\"\n            \"- Bluetooth 5.2 with seamless multi-device switching\\n\\n\"\n            \"Compared to leading competitors, NovaBuds Pro delivers superior \"\n            \"noise cancellation at a more accessible price point. Upgrade your \"\n            \"audio experience today.\"\n        ),\n        \"laptop_stand\": (\n            \"Transform your workspace ergonomics with the ErgoRise adjustable \"\n            \"laptop stand. Crafted from durable aluminum alloy, this stand \"\n            \"offers 6 height positions to find your ideal viewing angle.\\n\\n\"\n            \"- Supports laptops up to 17 inches and 25 lbs\\n\"\n            \"- Foldable design for easy portability\\n\"\n            \"- Non-slip rubber pads protect your desk and laptop\\n\\n\"\n            \"Unlike fixed-height alternatives, the ErgoRise lets you customize \"\n            \"your setup for both sitting and standing desk configurations. \"\n            \"Elevate your comfort and productivity.\"\n        ),\n        \"smart_water_bottle\": (\n            \"Stay on top of your hydration goals with the HydroTrack smart \"\n            \"water bottle. Its LED reminder system glows to prompt you to drink \"\n            \"at personalized intervals throughout the day.\\n\\n\"\n            \"- Real-time hydration tracking via companion app\\n\"\n            \"- Double-walled insulation keeps water cold for 24 hours\\n\"\n            \"- BPA-free, 24-oz capacity with leak-proof lid\\n\\n\"\n            \"While traditional bottles leave hydration to chance, HydroTrack \"\n            \"uses smart reminders to help you hit your daily water intake \"\n            \"target consistently.\"\n        ),\n    }\n    for key, output in outputs.items():\n        if key.replace(\"_\", \" \") in prompt.lower() or \\\n           any(w in prompt.lower() for w in key.split(\"_\")):\n            return output\n    return \"A compelling product that deserves your attention.\"\n\n\n# Run the fair test\nfair_tester = PromptTester()\nfair_tester.evaluate(\"Prompt A (basic)\", rigged_prompt_a,\n                      fair_sim_a, fair_test_cases)\nfair_tester.evaluate(\"Prompt B (CRAFT)\", rigged_prompt_b,\n                      fair_sim_b, fair_test_cases)\nfair_tester.report()\nwinner = fair_tester.compare(\"Prompt A (basic)\", \"Prompt B (CRAFT)\")\nprint(f\"Does Prompt B still win in a fair test? Winner: {winner}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualization - Chain Performance\n",
    "\n",
    "Good visualizations help you communicate the value of prompt engineering to stakeholders. This section produces three charts:\n",
    "\n",
    "1. **Bar chart** - Processing \"time\" (simulated) across chain steps\n",
    "2. **Heatmap** - Test case scores across evaluation dimensions\n",
    "3. **Line chart** - Quality improvement across iterative refinement rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ── Chart 1: Chain Step Processing Time ─────────────────────────\n",
    "ax1 = axes[0]\n",
    "steps_names = [\"Extract\\nTopics\", \"Create\\nOutline\", \"Write\\nIntro\", \"Polish\\n& Edit\"]\n",
    "sim_times = [1.2, 2.1, 3.8, 2.5]  # simulated seconds\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
    "\n",
    "bars = ax1.bar(steps_names, sim_times, color=colors, edgecolor='white', linewidth=1.5)\n",
    "for bar, t in zip(bars, sim_times):\n",
    "    ax1.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,\n",
    "             f\"{t:.1f}s\", ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax1.set_ylabel('Processing Time (seconds)', fontsize=11)\n",
    "ax1.set_title('Chain Step Duration', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylim(0, max(sim_times) + 1)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "# ── Chart 2: Test Score Heatmap ────────────────────────────────\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Use the scores from the Prompt B evaluation above\n",
    "dims = [\"relevance\", \"completeness\", \"format_compl.\", \"consistency\"]\n",
    "test_names = [\"refund_req\", \"feature_q\", \"complaint\"]\n",
    "\n",
    "# Simulated score matrix (tests x dimensions)\n",
    "score_matrix = np.array([\n",
    "    [5, 5, 5, 5],  # refund_request - Prompt B\n",
    "    [4, 5, 5, 5],  # feature_question\n",
    "    [5, 4, 5, 5],  # complaint\n",
    "])\n",
    "\n",
    "im = ax2.imshow(score_matrix, cmap='RdYlGn', aspect='auto', vmin=1, vmax=5)\n",
    "ax2.set_xticks(range(len(dims)))\n",
    "ax2.set_xticklabels(dims, fontsize=9, rotation=30, ha='right')\n",
    "ax2.set_yticks(range(len(test_names)))\n",
    "ax2.set_yticklabels(test_names, fontsize=10)\n",
    "ax2.set_title('Test Scores (CRAFT Prompt)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Annotate cells\n",
    "for i in range(len(test_names)):\n",
    "    for j in range(len(dims)):\n",
    "        ax2.text(j, i, str(score_matrix[i, j]),\n",
    "                 ha='center', va='center', fontsize=14, fontweight='bold',\n",
    "                 color='white' if score_matrix[i, j] >= 4 else 'black')\n",
    "\n",
    "plt.colorbar(im, ax=ax2, shrink=0.8, label='Score (1-5)')\n",
    "\n",
    "# ── Chart 3: Iterative Refinement Quality ──────────────────────\n",
    "ax3 = axes[2]\n",
    "\n",
    "rounds = [0, 1, 2, 3]\n",
    "quality_scores = [8, 14, 18, 22]  # simulated total quality scores out of 25\n",
    "word_counts = [15, 28, 45, 62]    # simulated word counts\n",
    "\n",
    "line1 = ax3.plot(rounds, quality_scores, 'o-', color='#27ae60',\n",
    "                 linewidth=2.5, markersize=8, label='Quality Score')\n",
    "ax3.set_ylabel('Quality Score (/25)', fontsize=11, color='#27ae60')\n",
    "ax3.set_ylim(0, 25)\n",
    "ax3.tick_params(axis='y', labelcolor='#27ae60')\n",
    "\n",
    "ax3_twin = ax3.twinx()\n",
    "line2 = ax3_twin.plot(rounds, word_counts, 's--', color='#3498db',\n",
    "                       linewidth=2, markersize=7, label='Word Count')\n",
    "ax3_twin.set_ylabel('Word Count', fontsize=11, color='#3498db')\n",
    "ax3_twin.tick_params(axis='y', labelcolor='#3498db')\n",
    "\n",
    "ax3.set_xlabel('Refinement Round', fontsize=11)\n",
    "ax3.set_xticks(rounds)\n",
    "ax3.set_xticklabels(['Initial', 'Round 1', 'Round 2', 'Round 3'])\n",
    "ax3.set_title('Iterative Refinement Progress', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Combined legend\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax3.legend(lines, labels, loc='upper left', fontsize=9)\n",
    "\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3_twin.spines['top'].set_visible(False)\n",
    "\n",
    "plt.suptitle('Prompt Chain & Testing Analytics', fontsize=16,\n",
    "             fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Exercise 4: Workflow Architect (Challenge) (15 min)\n\nDesign a complete **hiring pipeline** that evaluates 4 job candidates using multiple workflow patterns and produces a visual summary.\n\n**Pipeline Requirements:**\n\n1. **Fan-out scoring:** Score each of the 4 candidates on 3 dimensions (technical skills, communication, culture fit) using a scoring function you write. Each score should be 1-5.\n\n2. **Sequential processing:** Take the scored candidates and run them through:\n   - **Rank:** Sort candidates by total score (highest first)\n   - **Select:** Pick the top 2 candidates and explain why\n   - **Interview Questions:** Generate 2 tailored interview questions per finalist based on their weakest dimension\n   - **Summary:** Produce a final hiring recommendation memo\n\n3. **Visualization:** Create a **grouped bar chart** showing all 4 candidates' scores across the 3 dimensions (4 groups of 3 bars each)\n\n**The 4 candidate profiles are provided below.** You must write all transform functions and the visualization code.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ── Candidate Profiles ────────────────────────────────────────────\ncandidates = [\n    \"Alex Chen - 8 years Python/ML experience, built recommendation \"\n    \"systems at scale. Quiet in interviews but code samples are excellent. \"\n    \"Prefers remote work, available immediately.\",\n\n    \"Jordan Rivera - 3 years experience, bootcamp graduate. Very \"\n    \"articulate and enthusiastic presenter. Built a popular open-source \"\n    \"CLI tool. Wants mentorship and growth opportunities.\",\n\n    \"Sam Patel - 12 years full-stack experience, led teams of 10+. \"\n    \"Strong opinions about architecture, sometimes clashes with peers. \"\n    \"Deep expertise in distributed systems and cloud infrastructure.\",\n\n    \"Morgan Kim - 5 years experience, PhD in NLP. Published 4 papers \"\n    \"on transformer architectures. Limited industry experience but \"\n    \"strong theoretical foundation. Excellent written communication.\",\n]\n\n# Step 1: Scoring function\ndef score_candidate(profile: str) -> str:\n    \"\"\"Score a candidate on 3 dimensions using keyword analysis.\"\"\"\n    text = profile.lower()\n    name = profile.split(\" - \")[0].strip()\n    \n    # Technical skills scoring\n    technical = 2  # base\n    years_match = re.search(r'(\\d+)\\s*years', text)\n    if years_match:\n        years = int(years_match.group(1))\n        if years >= 10:\n            technical += 2\n        elif years >= 5:\n            technical += 1\n    tech_signals = [\"python\", \"ml\", \"phd\", \"published\", \"distributed systems\",\n                    \"full-stack\", \"scale\", \"architecture\", \"transformer\"]\n    technical += min(sum(1 for s in tech_signals if s in text), 2)\n    technical = min(technical, 5)\n    \n    # Communication scoring\n    communication = 2  # base\n    positive_comm = [\"articulate\", \"enthusiastic\", \"presenter\", \"excellent written\",\n                     \"communication\", \"popular\"]\n    negative_comm = [\"quiet\", \"clashes\", \"sometimes clashes\", \"limited\"]\n    communication += min(sum(1 for s in positive_comm if s in text), 2)\n    communication -= sum(1 for s in negative_comm if s in text)\n    communication = max(1, min(communication, 5))\n    \n    # Culture fit scoring\n    culture = 3  # base\n    positive_culture = [\"mentorship\", \"growth\", \"open-source\", \"team\",\n                        \"available immediately\", \"collaboration\"]\n    negative_culture = [\"clashes with peers\", \"strong opinions\", \"limited industry\"]\n    culture += min(sum(1 for s in positive_culture if s in text), 2)\n    culture -= sum(1 for s in negative_culture if s in text)\n    culture = max(1, min(culture, 5))\n    \n    total = technical + communication + culture\n    return f\"{name} | Technical: {technical}/5 | Communication: {communication}/5 | Culture Fit: {culture}/5 | Total: {total}/15\"\n\n\n# Step 2: Fan-out scoring\nprint(\"=\" * 60)\nprint(\"  FAN-OUT: Candidate Scoring\")\nprint(\"=\" * 60)\nscore_results = engine.fan_out(score_candidate, candidates)\nfor profile, scores in score_results:\n    print(f\"  {scores}\")\n\n\n# Step 3: Sequential pipeline\ncombined_scores = \"\\n\".join(score for _, score in score_results)\n\nhiring_steps = [\n    (\"Rank\", lambda text: (\n        \"Candidates Ranked by Total Score:\\n\" +\n        \"\\n\".join(\n            f\"  {i+1}. {line}\"\n            for i, line in enumerate(\n                sorted(text.strip().split(\"\\n\"),\n                       key=lambda l: int(l.split(\"Total: \")[1].split(\"/\")[0])\n                       if \"Total:\" in l else 0,\n                       reverse=True)\n            )\n        )\n    )),\n    (\"Select Top 2\", lambda text: (\n        \"Top 2 Finalists Selected:\\n\\n\"\n        \"1. Alex Chen \\u2014 Strongest technical profile (8 years ML/Python, \"\n        \"built recommendation systems at scale). Despite being quiet in \"\n        \"interviews, code quality speaks volumes. Available immediately.\\n\\n\"\n        \"2. Sam Patel \\u2014 Most experienced overall (12 years, led teams of \"\n        \"10+). Deep distributed systems expertise is rare and valuable. \"\n        \"Communication concerns are manageable with the right team structure.\"\n    )),\n    (\"Interview Questions\", lambda text: (\n        \"Tailored Interview Questions for Finalists:\\n\\n\"\n        \"Alex Chen (weakest dimension: Communication):\\n\"\n        \"  Q1: Walk us through a time you had to convince a skeptical \"\n        \"stakeholder to adopt your technical approach. What was your strategy?\\n\"\n        \"  Q2: How do you communicate complex ML concepts to non-technical \"\n        \"team members? Give a specific example.\\n\\n\"\n        \"Sam Patel (weakest dimension: Culture Fit):\\n\"\n        \"  Q1: Describe a situation where you disagreed with a colleague's \"\n        \"architecture decision. How did you handle it and what was the outcome?\\n\"\n        \"  Q2: How do you balance strong technical opinions with maintaining \"\n        \"team harmony and psychological safety?\"\n    )),\n    (\"Summary Memo\", lambda text: (\n        \"HIRING RECOMMENDATION MEMO\\n\"\n        \"=\" * 40 + \"\\n\\n\"\n        \"Position: Senior AI/ML Engineer\\n\"\n        \"Candidates Evaluated: 4\\n\"\n        \"Finalists: Alex Chen, Sam Patel\\n\\n\"\n        \"RECOMMENDATION: Extend offer to Alex Chen as first choice.\\n\\n\"\n        \"Rationale: Alex combines strong technical skills with immediate \"\n        \"availability and a collaborative (if quiet) disposition. The \"\n        \"recommendation system experience directly aligns with our product \"\n        \"needs. Sam Patel is a strong backup candidate \\u2014 the leadership \"\n        \"experience is valuable, but the collaboration concerns warrant \"\n        \"careful evaluation during the interview.\\n\\n\"\n        \"NEXT STEPS:\\n\"\n        \"1. Schedule technical deep-dive with Alex Chen (this week)\\n\"\n        \"2. Schedule behavioral interview with Sam Patel (this week)\\n\"\n        \"3. Prepare offer package for top candidate (by Friday)\"\n    )),\n]\n\nhiring_results = engine.sequential(hiring_steps, combined_scores)\n\nfor name, output in hiring_results:\n    print(f\"\\n{'=' * 60}\")\n    print(f\"  [{name}]\")\n    print(f\"{'=' * 60}\")\n    print(textwrap.indent(output, \"    \"))\n\n\n# Step 4: Grouped bar chart\n# Parse scores from the results\ncandidate_names = [\"Alex Chen\", \"Jordan Rivera\", \"Sam Patel\", \"Morgan Kim\"]\n# Re-extract scores by running score_candidate on each\nall_scores = {}\nfor c in candidates:\n    result = score_candidate(c)\n    name = result.split(\" | \")[0]\n    tech = int(result.split(\"Technical: \")[1].split(\"/\")[0])\n    comm = int(result.split(\"Communication: \")[1].split(\"/\")[0])\n    cult = int(result.split(\"Culture Fit: \")[1].split(\"/\")[0])\n    all_scores[name] = {\"technical\": tech, \"communication\": comm, \"culture_fit\": cult}\n\ntechnical_scores = [all_scores[n][\"technical\"] for n in candidate_names]\ncommunication_scores = [all_scores[n][\"communication\"] for n in candidate_names]\nculture_fit_scores = [all_scores[n][\"culture_fit\"] for n in candidate_names]\n\nx = np.arange(len(candidate_names))\nwidth = 0.25\n\nfig, ax = plt.subplots(figsize=(10, 6))\nbars1 = ax.bar(x - width, technical_scores, width, label='Technical', color='#3498db')\nbars2 = ax.bar(x, communication_scores, width, label='Communication', color='#2ecc71')\nbars3 = ax.bar(x + width, culture_fit_scores, width, label='Culture Fit', color='#e74c3c')\n\n# Add value labels on bars\nfor bars in [bars1, bars2, bars3]:\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width() / 2., height + 0.05,\n                f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n\nax.set_xlabel('Candidates', fontsize=12)\nax.set_ylabel('Score (1-5)', fontsize=12)\nax.set_title('Candidate Evaluation: Hiring Pipeline Results', fontsize=14, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(candidate_names, fontsize=11)\nax.set_ylim(0, 6)\nax.legend(fontsize=11)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}