{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Machine Learning with PyTorch\n",
    "\n",
    "**Duration:** 90-120 minutes | **Difficulty:** Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lab teaches PyTorch fundamentals through hands-on implementation of ML algorithms.\n",
    "\n",
    "### Lab Structure\n",
    "\n",
    "| Part | Topic | Key Concepts |\n",
    "|------|-------|---------------|\n",
    "| **Part 1** | PyTorch Tensors | Creating tensors, operations, autograd |\n",
    "| **Part 2** | Linear Regression | Training loop, MSE loss, nn.Module |\n",
    "| **Part 3** | Logistic Regression | Sigmoid, BCE loss, classification |\n",
    "| **Part 4** | Support Vector Machines | Kernels, margins, support vectors |\n",
    "| **Part 5** | Model Evaluation | Confusion matrix, precision, recall, F1 |\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Read each markdown cell carefully\n",
    "- Write your code in the empty code cells\n",
    "- Run cells with `Shift+Enter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cell below to import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: PyTorch Tensors\n",
    "\n",
    "Tensors are the fundamental data structure in PyTorch - similar to NumPy arrays but with GPU support and automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.1 Creating Tensors\n\nCreate tensors using these functions:\n\n| Function | Description | Example |\n|----------|-------------|----------|\n| `torch.tensor([...])` | From Python list | `torch.tensor([1, 2, 3])` |\n| `torch.zeros(shape)` | Zeros tensor | `torch.zeros(3, 4)` |\n| `torch.ones(shape)` | Ones tensor | `torch.ones(2, 3)` |\n| `torch.randn(shape)` | Random normal | `torch.randn(3, 3)` |\n| `torch.from_numpy(arr)` | From NumPy | `torch.from_numpy(np_array)` |\n\n**Your Task:** Create the following tensors:\n1. `t1`: A 1D tensor with values [1.0, 2.0, 3.0, 4.0, 5.0]\n2. `t2`: A 3×3 tensor of zeros\n3. `t3`: A 2×4 tensor of random values\n4. `t4`: Convert this NumPy array to a tensor: `np.array([[1, 2], [3, 4]])`\n\nPrint each tensor and its shape using `.shape`.\n\n**Expected Output:**\n```\nt1: tensor([1., 2., 3., 4., 5.])\nt1 shape: torch.Size([5])\n\nt2:\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\nt2 shape: torch.Size([3, 3])\n\nt3 shape: torch.Size([2, 4])\n\nt4:\ntensor([[1, 2],\n        [3, 4]])\nt4 shape: torch.Size([2, 2])\n```\n\n**Sample Code:**\n```python\n# Creating tensors in different ways\nmy_tensor = torch.tensor([10.0, 20.0, 30.0])\nmy_ones = torch.ones(2, 2)\nnp_arr = np.array([5, 6, 7])\nfrom_np = torch.from_numpy(np_arr)\nprint(\"Tensor:\", my_tensor)\nprint(\"Shape:\", my_tensor.shape)\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.2 Tensor Operations\n\nCommon tensor operations:\n\n| Operation | Description | Example |\n|-----------|-------------|----------|\n| `a + b` | Element-wise addition | `t1 + t2` |\n| `a * b` | Element-wise multiplication | `t1 * t2` |\n| `a @ b` or `torch.matmul(a, b)` | Matrix multiplication | `t1 @ t2` |\n| `t.sum()` | Sum all elements | `tensor.sum()` |\n| `t.mean()` | Mean of elements | `tensor.mean()` |\n| `t.reshape(shape)` | Reshape tensor | `t.reshape(2, 3)` |\n\n**Your Task:** \n1. Create two tensors: `a = torch.tensor([[1., 2.], [3., 4.]])` and `b = torch.tensor([[5., 6.], [7., 8.]])`\n2. Calculate and print:\n   - `add_result`: Element-wise addition of a and b\n   - `mult_result`: Element-wise multiplication of a and b\n   - `matmul_result`: Matrix multiplication of a and b\n   - `sum_a`: Sum of all elements in a\n   - `mean_b`: Mean of all elements in b\n\n**Expected Output:**\n```\nAddition:\ntensor([[ 6.,  8.],\n        [10., 12.]])\n\nMultiplication:\ntensor([[ 5., 12.],\n        [21., 32.]])\n\nMatrix Multiplication:\ntensor([[19., 22.],\n        [43., 50.]])\n\nSum of a: tensor(10.)\nMean of b: tensor(6.5000)\n```\n\n**Sample Code:**\n```python\n# Tensor operations\nx = torch.tensor([[1., 2.], [3., 4.]])\ny = torch.tensor([[2., 0.], [1., 3.]])\nadded = x + y                    # Element-wise add\nproduct = x * y                  # Element-wise multiply\nmatrix_prod = x @ y              # Matrix multiply\ntotal = x.sum()                  # Sum all\nprint(\"Sum:\", total)\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Automatic Differentiation (Autograd)\n",
    "\n",
    "PyTorch can automatically compute gradients - this is essential for training neural networks.\n",
    "\n",
    "Key concepts:\n",
    "- `requires_grad=True`: Tell PyTorch to track operations for gradient computation\n",
    "- `.backward()`: Compute gradients\n",
    "- `.grad`: Access the computed gradient\n",
    "\n",
    "Example:\n",
    "```python\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x ** 2  # y = x^2\n",
    "y.backward()  # Compute dy/dx\n",
    "print(x.grad)  # Should be 4.0 (derivative of x^2 is 2x)\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Create a tensor `x = torch.tensor([3.0], requires_grad=True)`\n",
    "2. Compute `y = x ** 3` (y = x cubed)\n",
    "3. Call `y.backward()` to compute the gradient\n",
    "4. Print `x.grad` (should be 27.0, since derivative of x³ is 3x² = 3×9 = 27)\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "x = tensor([3.], requires_grad=True)\n",
    "y = x^3 = tensor([27.], grad_fn=<PowBackward0>)\n",
    "Gradient (dy/dx) = tensor([27.])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Linear Regression\n",
    "\n",
    "Linear regression finds the best-fit line: `y = wx + b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Generate Data\n",
    "\n",
    "Run the cell below to create training data with a known relationship: `y = 3x + 1 + noise`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linear data: y = 3x + 1 + noise\n",
    "np.random.seed(42)\n",
    "X_np = np.random.rand(100, 1) * 10  # 100 samples, values 0-10\n",
    "y_np = 3 * X_np + 1 + np.random.randn(100, 1) * 2  # y = 3x + 1 + noise\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.FloatTensor(X_np)\n",
    "y = torch.FloatTensor(y_np)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(X_np, y_np, alpha=0.6)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Training Data (y = 3x + 1 + noise)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define a Linear Model\n",
    "\n",
    "Create a model using `nn.Module`:\n",
    "\n",
    "```python\n",
    "class ModelName(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "```\n",
    "\n",
    "**Your Task:** Create a `LinearRegression` class:\n",
    "1. Define `__init__` with `nn.Linear(1, 1)` (1 input feature, 1 output)\n",
    "2. Define `forward` that returns `self.linear(x)`\n",
    "3. Create an instance: `model = LinearRegression()`\n",
    "4. Print the model\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "LinearRegression(\n",
    "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Train the Model\n",
    "\n",
    "The PyTorch training loop:\n",
    "\n",
    "```python\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update weights\n",
    "```\n",
    "\n",
    "**Your Task:** Train your linear regression model:\n",
    "1. Create `criterion = nn.MSELoss()`\n",
    "2. Create `optimizer = optim.SGD(model.parameters(), lr=0.01)`\n",
    "3. Train for 100 epochs using the loop pattern above\n",
    "4. Print the loss every 20 epochs\n",
    "5. After training, print the learned weight and bias:\n",
    "   - `model.linear.weight.item()` (should be close to 3)\n",
    "   - `model.linear.bias.item()` (should be close to 1)\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Epoch 0, Loss: XX.XXXX\n",
    "Epoch 20, Loss: X.XXXX\n",
    "Epoch 40, Loss: X.XXXX\n",
    "Epoch 60, Loss: X.XXXX\n",
    "Epoch 80, Loss: X.XXXX\n",
    "\n",
    "Learned weight: ~3.0 (close to 3)\n",
    "Learned bias: ~1.0 (close to 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Logistic Regression\n",
    "\n",
    "Logistic regression is used for binary classification. It uses the sigmoid function to output probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Generate Classification Data\n",
    "\n",
    "Run the cell below to create a binary classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=200, n_features=2, n_redundant=0, \n",
    "    n_informative=2, n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_t = torch.FloatTensor(X_train)\n",
    "y_train_t = torch.FloatTensor(y_train).reshape(-1, 1)\n",
    "X_test_t = torch.FloatTensor(X_test)\n",
    "y_test_t = torch.FloatTensor(y_test).reshape(-1, 1)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='coolwarm', alpha=0.6)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Binary Classification Data')\n",
    "plt.colorbar(label='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define Logistic Regression Model\n",
    "\n",
    "Logistic regression applies sigmoid to a linear transformation:\n",
    "\n",
    "```python\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Create a `LogisticRegression` class with 2 input features (matching our data)\n",
    "2. Use `torch.sigmoid()` in the forward method\n",
    "3. Create an instance: `log_model = LogisticRegression(2)`\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "LogisticRegression(\n",
    "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Train Logistic Regression\n",
    "\n",
    "For binary classification, use Binary Cross Entropy loss:\n",
    "\n",
    "```python\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Create BCELoss criterion and SGD optimizer (lr=0.1)\n",
    "2. Train for 200 epochs\n",
    "3. Print loss every 40 epochs\n",
    "4. After training, evaluate on test set:\n",
    "   - Get predictions: `y_pred = (log_model(X_test_t) >= 0.5).float()`\n",
    "   - Calculate accuracy: `(y_pred == y_test_t).float().mean()`\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Epoch 0, Loss: 0.XXXX\n",
    "Epoch 40, Loss: 0.XXXX\n",
    "Epoch 80, Loss: 0.XXXX\n",
    "Epoch 120, Loss: 0.XXXX\n",
    "Epoch 160, Loss: 0.XXXX\n",
    "\n",
    "Test Accuracy: ~0.95 (around 95%)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Support Vector Machines\n",
    "\n",
    "SVMs find the optimal hyperplane that separates classes with maximum margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 SVM with Different Kernels\n",
    "\n",
    "scikit-learn's SVC (Support Vector Classifier):\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear')  # or 'rbf', 'poly'\n",
    "svm.fit(X_train, y_train)\n",
    "predictions = svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "```\n",
    "\n",
    "Available kernels:\n",
    "- `'linear'`: Linear decision boundary\n",
    "- `'rbf'`: Radial Basis Function (good for non-linear data)\n",
    "- `'poly'`: Polynomial kernel\n",
    "\n",
    "**Your Task:**\n",
    "1. Train an SVM with `kernel='linear'` on X_train, y_train\n",
    "2. Predict on X_test\n",
    "3. Calculate and print the accuracy\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Linear SVM Accuracy: ~0.95 (around 95%)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Compare Kernels on Non-Linear Data\n",
    "\n",
    "Run the cell below to create moon-shaped data that's not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-linear (moon-shaped) data\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='coolwarm', alpha=0.6)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Moon-shaped Data (Non-linear)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4.3 Train and Compare Kernels\n\n**Your Task:** Train SVMs with different kernels and compare accuracies:\n1. Train an SVM with `kernel='linear'`\n2. Train an SVM with `kernel='rbf'`\n3. Train an SVM with `kernel='poly'`\n4. Print the accuracy for each kernel\n\nWhich kernel works best for this non-linear data?\n\n**Expected Output:**\n```\nLinear kernel accuracy: ~0.88\nRBF kernel accuracy: ~1.00 (best for non-linear data)\nPoly kernel accuracy: ~0.93\n```\n\n**Sample Code:**\n```python\n# Training SVMs with different kernels\nsvm_linear = SVC(kernel='linear')\nsvm_linear.fit(X_train_m, y_train_m)\npred_linear = svm_linear.predict(X_test_m)\nacc_linear = accuracy_score(y_test_m, pred_linear)\nprint(f\"Linear accuracy: {acc_linear:.2f}\")\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Model Evaluation\n",
    "\n",
    "Beyond accuracy: precision, recall, F1-score, and confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Confusion Matrix\n",
    "\n",
    "A confusion matrix shows the breakdown of predictions:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "# [[TN, FP],\n",
    "#  [FN, TP]]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- TN: True Negatives (correctly predicted 0)\n",
    "- FP: False Positives (incorrectly predicted 1)\n",
    "- FN: False Negatives (incorrectly predicted 0)\n",
    "- TP: True Positives (correctly predicted 1)\n",
    "\n",
    "**Your Task:**\n",
    "1. Use your best SVM model from Part 4 to get predictions on the moon test data\n",
    "2. Create a confusion matrix\n",
    "3. Print the confusion matrix\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Confusion Matrix:\n",
    "[[29  0]\n",
    " [ 0 31]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Classification Report\n",
    "\n",
    "Get precision, recall, and F1-score in one report:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)\n",
    "```\n",
    "\n",
    "Metrics:\n",
    "- **Precision**: Of predicted positives, how many are actually positive?\n",
    "- **Recall**: Of actual positives, how many did we predict correctly?\n",
    "- **F1-score**: Harmonic mean of precision and recall\n",
    "\n",
    "**Your Task:**\n",
    "1. Generate a classification report for your SVM predictions\n",
    "2. Print the report\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00        29\n",
    "           1       1.00      1.00      1.00        31\n",
    "\n",
    "    accuracy                           1.00        60\n",
    "   macro avg       1.00      1.00      1.00        60\n",
    "weighted avg       1.00      1.00      1.00        60\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lab Complete!\n",
    "\n",
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- **PyTorch Tensors**: Create, manipulate, and use autograd\n",
    "- **Linear Regression**: nn.Module, training loop, MSE loss\n",
    "- **Logistic Regression**: Sigmoid, BCE loss, classification\n",
    "- **SVMs**: Different kernels for linear/non-linear data\n",
    "- **Evaluation**: Confusion matrix, precision, recall, F1-score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}