{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3: Natural Language Processing\n",
        "\n",
        "**Duration:** 45-60 minutes | **Difficulty:** Intermediate\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This lab teaches NLP fundamentals through hands-on implementation.\n",
        "\n",
        "### Lab Structure\n",
        "\n",
        "| Section | Topic | Key Concepts |\n",
        "|---------|-------|---------------|\n",
        "| **1** | Text Preprocessing | Cleaning, tokenization, normalization |\n",
        "| **2** | Bag of Words | CountVectorizer, document-term matrix |\n",
        "| **3** | TF-IDF | Term frequency, inverse document frequency |\n",
        "| **4** | Sentiment Analysis | Classification dataset |\n",
        "| **5** | Naive Bayes | Text classification |\n",
        "| **6** | Prediction | Classifying new text |\n",
        "\n",
        "### Instructions\n",
        "\n",
        "- Read each markdown cell carefully\n",
        "- Write your code in the empty code cells\n",
        "- Run cells with `Shift+Enter`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run the cell below to import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"Setup complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n# Part 6: Natural Language Processing\n\nLearn to process text data for machine learning: tokenization, vectorization, and text classification."
    },
    {
      "cell_type": "markdown",
      "source": "## 6.1 Text Preprocessing\n\nBefore feeding text to ML models, we must clean and normalize it.\n\nCommon preprocessing steps:\n\n| Step | Description | Example |\n|------|-------------|---------|\n| Lowercase | Convert to lowercase | \"Hello World\" \u2192 \"hello world\" |\n| Remove punctuation | Strip special characters | \"Hello!\" \u2192 \"Hello\" |\n| Tokenization | Split into words | \"hello world\" \u2192 [\"hello\", \"world\"] |\n| Remove stopwords | Remove common words | [\"the\", \"is\", \"a\"] removed |\n\n**Your Task:** Write a function `preprocess_text(text)` that:\n1. Converts text to lowercase\n2. Removes all non-alphanumeric characters (keep spaces)\n3. Returns the cleaned text\n\nTest it on: `\"Hello, World! This is NLP 101.\"`\n\n**Expected Output:**\n```\nOriginal: Hello, World! This is NLP 101.\nCleaned: hello world this is nlp 101\n```\n\n**Sample Code:**\n```python\nimport re\n\ndef clean_example(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Keep only alphanumeric and spaces\n    return text\n\nresult = clean_example(\"Test! 123\")\nprint(result)  # \"test 123\"\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Your code here\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 6.2 Bag of Words with CountVectorizer\n\nBag of Words converts text to numerical vectors by counting word occurrences.\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(documents)  # Returns sparse matrix\nprint(vectorizer.get_feature_names_out())  # Vocabulary\nprint(X.toarray())  # Dense matrix\n```\n\n**Your Task:**\n1. Create a list of 3 documents:\n   - \"I love machine learning\"\n   - \"Machine learning is great\"\n   - \"I love programming\"\n2. Create a `CountVectorizer` and fit_transform the documents\n3. Print the vocabulary (feature names)\n4. Print the document-term matrix as an array\n\n**Expected Output:**\n```\nVocabulary: ['great' 'is' 'learning' 'love' 'machine' 'programming']\n\nDocument-Term Matrix:\n[[0 0 1 1 1 0]\n [1 1 1 0 1 0]\n [0 0 0 1 0 1]]\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Your code here\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 6.3 TF-IDF Vectorization\n\nTF-IDF (Term Frequency - Inverse Document Frequency) weights words by importance:\n- **TF**: How often a word appears in a document\n- **IDF**: How rare a word is across all documents\n- Words common in one doc but rare overall get high scores\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer()\nX = tfidf.fit_transform(documents)\nprint(X.toarray())  # TF-IDF weighted matrix\n```\n\n**Your Task:**\n1. Use the same 3 documents from 6.2\n2. Create a `TfidfVectorizer` and fit_transform the documents\n3. Print the TF-IDF matrix (rounded to 2 decimals)\n4. Observe: Which words have higher weights? Why?\n\n**Expected Output:**\n```\nTF-IDF Matrix:\n[[0.   0.   0.52 0.68 0.52 0.  ]\n [0.55 0.55 0.42 0.   0.42 0.  ]\n [0.   0.   0.   0.61 0.   0.79]]\n\nNote: 'programming' has high weight in doc 3 (unique to it)\n      'love' has lower weight (appears in docs 1 and 3)\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Your code here\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 6.4 Sentiment Analysis Dataset\n\nRun the cell below to create a simple sentiment analysis dataset.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Sentiment Analysis Dataset\nreviews = [\n    \"This movie was amazing and wonderful\",\n    \"I loved this film, it was great\",\n    \"Excellent movie, highly recommended\",\n    \"Best film I have ever seen\",\n    \"Wonderful story and great acting\",\n    \"This movie was terrible and boring\",\n    \"I hated this film, it was awful\",\n    \"Worst movie ever, do not watch\",\n    \"Boring and disappointing film\",\n    \"Terrible acting and bad story\",\n    \"The movie was okay, nothing special\",\n    \"It was an average film\",\n    \"Not bad but not great either\",\n    \"Mediocre movie with some good moments\",\n    \"Fantastic cinematography and brilliant performances\",\n    \"Absolutely dreadful, waste of time\",\n]\n\n# Labels: 1 = positive, 0 = negative\nlabels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0]\n\n# Split into train and test\nX_train_text, X_test_text, y_train_nlp, y_test_nlp = train_test_split(\n    reviews, labels, test_size=0.25, random_state=42\n)\n\nprint(f\"Training samples: {len(X_train_text)}\")\nprint(f\"Test samples: {len(X_test_text)}\")\nprint(f\"\\nSample reviews:\")\nfor i in range(3):\n    sentiment = \"Positive\" if y_train_nlp[i] == 1 else \"Negative\"\n    print(f\"  [{sentiment}] {X_train_text[i]}\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 6.5 Text Classification with Naive Bayes\n\nNaive Bayes is a probabilistic classifier that works well with text data.\n\n```python\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Vectorize text\nvectorizer = TfidfVectorizer()\nX_train_vec = vectorizer.fit_transform(X_train_text)\nX_test_vec = vectorizer.transform(X_test_text)\n\n# Train classifier\nclf = MultinomialNB()\nclf.fit(X_train_vec, y_train)\n\n# Predict\npredictions = clf.predict(X_test_vec)\n```\n\n**Your Task:**\n1. Create a `TfidfVectorizer` and vectorize the training and test text\n2. Create a `MultinomialNB` classifier and train it\n3. Make predictions on the test set\n4. Calculate and print the accuracy\n5. Print the classification report\n\n**Expected Output:**\n```\nAccuracy: ~0.75 (75%)\n\nClassification Report:\n              precision    recall  f1-score   support\n           0       X.XX      X.XX      X.XX         X\n           1       X.XX      X.XX      X.XX         X\n```\n\n**Sample Code:**\n```python\n# Complete pipeline example\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('clf', MultinomialNB())\n])\npipeline.fit(X_train_text, y_train_nlp)\naccuracy = pipeline.score(X_test_text, y_test_nlp)\nprint(f\"Accuracy: {accuracy:.2f}\")\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Your code here\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 6.6 Predict on New Text\n\nUse your trained model to classify new reviews.\n\n**Your Task:**\n1. Create a list of 3 new reviews (make up your own!)\n2. Vectorize them using the same vectorizer (use `.transform()`, not `.fit_transform()`)\n3. Use your classifier to predict the sentiment\n4. Print each review with its predicted sentiment\n\n**Expected Output:**\n```\nReview: \"This was the best experience ever!\"\nPredicted: Positive\n\nReview: \"Horrible waste of my time\"\nPredicted: Negative\n\nReview: \"It was pretty good overall\"\nPredicted: Positive\n```\n\n**Sample Code:**\n```python\nnew_reviews = [\"Your review here\", \"Another review\"]\nnew_vectors = vectorizer.transform(new_reviews)\npredictions = clf.predict(new_vectors)\n\nfor review, pred in zip(new_reviews, predictions):\n    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n    print(f\"Review: {review}\")\n    print(f\"Predicted: {sentiment}\\n\")\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Your code here\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "---\n# Lab Complete!\n\n## Summary\n\nYou learned:\n- **PyTorch Tensors**: Create, manipulate, and use autograd\n- **Linear Regression**: nn.Module, training loop, MSE loss\n- **Logistic Regression**: Sigmoid, BCE loss, classification\n- **SVMs**: Different kernels for linear/non-linear data\n- **Evaluation**: Confusion matrix, precision, recall, F1-score\n- **NLP**: Text preprocessing, Bag of Words, TF-IDF, Naive Bayes classification",
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}