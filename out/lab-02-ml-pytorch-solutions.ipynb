{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2: Machine Learning with PyTorch - SOLUTIONS\n\n## From Classical ML to Deep Learning Foundations\n\n**Duration:** 90-120 minutes | **Difficulty:** Intermediate | **Prerequisites:** Lab 1\n\n**This notebook contains all solutions. Use for reference after attempting the exercises.**\n\n---\n\n## Overview\n\nThis lab bridges classical machine learning and deep learning by teaching PyTorch fundamentals through hands-on implementation of ML algorithms. You'll learn to build, train, and evaluate models using the same patterns used in production deep learning systems.\n\n### Lab Structure\n\n| Part | Topic | Key Concepts |\n|------|-------|--------------|\n| **Part 1** | PyTorch Tensors | Creating tensors, tensor operations, automatic differentiation (autograd) |\n| **Part 2** | Linear Regression | Training loop from scratch, MSE loss, nn.Module, gradient descent |\n| **Part 3** | Logistic Regression | Sigmoid function, BCE loss, binary classification, decision boundaries |\n| **Part 4** | Support Vector Machines | Kernel trick (linear, RBF, polynomial), margins, support vectors |\n| **Part 5** | Model Evaluation | Confusion matrix, precision, recall, F1-score, classification report |\n| **Part 6** | Natural Language Processing | Text preprocessing, TF-IDF, Naive Bayes, sentiment analysis |\n\n### Key Pattern You'll Learn\n\nThe PyTorch training loop used in all deep learning:\n\n```python\nfor epoch in range(n_epochs):\n    y_pred = model(X)           # Forward pass\n    loss = criterion(y_pred, y) # Compute loss\n    optimizer.zero_grad()       # Clear gradients\n    loss.backward()             # Backward pass\n    optimizer.step()            # Update weights\n```\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SETUP\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import make_classification, make_moons\n\n# NLP imports\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nimport re\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [10, 6]\nplt.rcParams['font.size'] = 12\n\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nprint(\"Setup Complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: PyTorch Tensors - Solutions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 1.1 Solution: Creating PyTorch Tensors\n\nTensors are the fundamental data structure in PyTorch - like NumPy arrays\nbut with GPU acceleration and automatic differentiation support.\n\"\"\"\n\n# Create a 1D tensor from a Python list\n# torch.tensor() converts Python/NumPy data to a PyTorch tensor\ntensor_a = torch.tensor([10, 20, 30, 40, 50])\n\n# Create a 3x4 tensor filled with zeros\n# Useful for initializing weight matrices or placeholders\ntensor_b = torch.zeros(3, 4)\n\n# Create a 2x5 tensor with random values between 0 and 1\n# torch.rand() uses uniform distribution [0, 1)\ntensor_c = torch.rand(2, 5)\n\n# Print results to verify\nprint(f\"a) {tensor_a}\")\nprint(f\"b) Shape: {tensor_b.shape}\")\nprint(f\"c) Random tensor with shape {tensor_c.shape}\")"
  },
  {
   "cell_type": "markdown",
   "source": "\"\"\"\nExercise 1.2 Solution: Tensor Operations\n\nPyTorch tensors support element-wise operations, aggregations,\nand shape transformations - all essential for neural network computations.\n\"\"\"\n\nx = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\ny = torch.tensor([[7., 8., 9.], [10., 11., 12.]])\n\nprint(\"x =\")\nprint(x)\nprint(\"\\ny =\")\nprint(y)\nprint()\n\n# Element-wise addition: adds corresponding elements\n# Same as x.add(y) or torch.add(x, y)\nresult_add = x + y\n\n# Mean of all elements: returns single scalar value\n# Can also specify dimension: x.mean(dim=0) for column means\nresult_mean = x.mean()\n\n# Sum along dimension 1 (rows): collapses columns\n# dim=0 would sum columns, dim=1 sums rows\nresult_row_sum = x.sum(dim=1)\n\n# Reshape: change tensor dimensions while preserving data\n# Total elements must remain same: 2*3 = 3*2 = 6\nresult_reshape = x.reshape(3, 2)\n\n# Print results\nprint(f\"a) x + y =\\n{result_add}\")\nprint(f\"\\nb) Mean of x = {result_mean}\")\nprint(f\"\\nc) Sum of each row = {result_row_sum}\")\nprint(f\"\\nd) x reshaped to 3x2:\\n{result_reshape}\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "\"\"\"\nExercise 1.3 Solution: Automatic Differentiation (Autograd)\n\nAutograd is PyTorch's automatic differentiation engine that powers neural network training.\nIt tracks operations on tensors and computes gradients automatically.\n\"\"\"\n\n# Create tensor with gradient tracking enabled\n# requires_grad=True tells PyTorch to track all operations on this tensor\nx = torch.tensor([2.0], requires_grad=True)\n\n# Compute y = 3x² - 4x + 5\n# PyTorch builds a computation graph as we perform operations\ny = 3*x**2 - 4*x + 5\n\n# Compute gradients via backpropagation\n# This calculates dy/dx and stores it in x.grad\ny.backward()\n\n# Print results\n# At x=2: y = 3(4) - 4(2) + 5 = 12 - 8 + 5 = 9\n# dy/dx = 6x - 4, at x=2: 6(2) - 4 = 8\nprint(f\"y = 3x² - 4x + 5 at x=2: y = {y.item()}\")\nprint(f\"dy/dx = 6x - 4 at x=2: dy/dx = {x.grad.item()}\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Code Explanation: Exercise 1.3\n\n| Line | Code | Explanation |\n|------|------|-------------|\n| 1 | `x = torch.tensor([2.0], requires_grad=True)` | **Creates tensor with gradient tracking.** PyTorch will record all operations on x for backpropagation. |\n| 2 | `y = 3*x**2 - 4*x + 5` | **Builds computation graph.** Each operation creates a node. PyTorch stores how to compute gradients. |\n| 3 | `y.backward()` | **Backpropagation.** Computes dy/dx using chain rule and stores result in x.grad. |\n| 4 | `x.grad` | **Access computed gradient.** Contains the derivative value after backward() is called. |\n\n**Mathematical Verification:**\n```\nFunction:    y = 3x² - 4x + 5\nDerivative:  dy/dx = 6x - 4\n\nAt x = 2:\n  y = 3(2)² - 4(2) + 5 = 12 - 8 + 5 = 9\n  dy/dx = 6(2) - 4 = 12 - 4 = 8\n```\n\n**The Computation Graph:**\n```\nx (leaf) → x² → 3x² → 3x² - 4x → 3x² - 4x + 5 (y)\n                ↑\n               4x ←\n```\n\n**Why autograd matters:**\n- Eliminates manual gradient calculation (error-prone for complex networks)\n- Enables training of deep networks with millions of parameters\n- Foundation of all deep learning: gradients tell us how to adjust weights",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 1.2 SOLUTION: Tensor Operations\n",
    "# ============================================\n",
    "\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
    "y = torch.tensor([[7., 8., 9.], [10., 11., 12.]])\n",
    "\n",
    "print(\"x =\")\n",
    "print(x)\n",
    "print(\"\\ny =\")\n",
    "print(y)\n",
    "print()\n",
    "\n",
    "# a) Add x and y element-wise\n",
    "result_add = x + y\n",
    "\n",
    "# b) Calculate the mean of x\n",
    "result_mean = x.mean()\n",
    "\n",
    "# c) Calculate the sum of each row of x (dim=1)\n",
    "result_row_sum = x.sum(dim=1)\n",
    "\n",
    "# d) Reshape x to be 3 rows x 2 columns\n",
    "result_reshape = x.reshape(3, 2)\n",
    "\n",
    "# ---- Test ----\n",
    "print(f\"a) x + y =\\n{result_add}\")\n",
    "print(f\"\\nb) Mean of x = {result_mean}\")\n",
    "print(f\"\\nc) Sum of each row = {result_row_sum}\")\n",
    "print(f\"\\nd) x reshaped to 3x2:\\n{result_reshape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 2.1 Solution: Linear Regression Model\n\nThis solution implements linear regression using PyTorch's nn.Module.\nThe training loop demonstrates the core pattern used in all deep learning.\n\"\"\"\n\ntorch.manual_seed(123)\n\nclass MyLinearRegression(nn.Module):\n    \"\"\"\n    Linear regression model: y = wx + b\n    \n    Uses nn.Linear which contains learnable weight and bias parameters.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        # nn.Linear(in_features, out_features) creates y = xW^T + b\n        # 1 input feature (x), 1 output (y)\n        self.linear = nn.Linear(1, 1)\n    \n    def forward(self, x):\n        # Forward pass: compute prediction given input\n        return self.linear(x)\n\n# Instantiate the model\nmy_model = MyLinearRegression()\n\n# Mean Squared Error loss: measures average squared difference\n# L = (1/n) * Σ(y_pred - y_true)²\nmy_criterion = nn.MSELoss()\n\n# Stochastic Gradient Descent optimizer\n# lr=0.01 means weights update by: w = w - 0.01 * gradient\nmy_optimizer = optim.SGD(my_model.parameters(), lr=0.01)\n\n# Training loop: the core pattern of all deep learning\nmy_losses = []\nfor epoch in range(100):\n    # STEP 1: Forward pass - compute predictions\n    y_pred = my_model(X)\n    \n    # STEP 2: Compute loss - how wrong are we?\n    loss = my_criterion(y_pred, y)\n    \n    # STEP 3: Zero gradients - clear previous gradients\n    # (gradients accumulate by default, which we don't want)\n    my_optimizer.zero_grad()\n    \n    # STEP 4: Backward pass - compute gradients via backprop\n    loss.backward()\n    \n    # STEP 5: Update weights - apply gradient descent step\n    my_optimizer.step()\n    \n    my_losses.append(loss.item())\n    \n    if (epoch + 1) % 20 == 0:\n        print(f'Epoch {epoch+1}/100 | Loss: {loss.item():.4f}')\n\n# Print learned parameters\nprint(f\"\\nLearned: w = {my_model.linear.weight.item():.4f}, b = {my_model.linear.bias.item():.4f}\")\nprint(f\"True:    w = 3.0000, b = 2.0000\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Code Explanation: Exercise 2.1\n\n**Model Definition:**\n| Line | Code | Explanation |\n|------|------|-------------|\n| 1 | `class MyLinearRegression(nn.Module)` | **Inherits from nn.Module.** All PyTorch models must inherit from this base class. |\n| 2 | `super().__init__()` | **Initializes parent class.** Required for nn.Module to set up internal tracking. |\n| 3 | `self.linear = nn.Linear(1, 1)` | **Creates linear layer.** Parameters: (input_features, output_features). Contains weight w and bias b. |\n| 4 | `def forward(self, x)` | **Defines forward pass.** Called when you do `model(x)`. Returns predictions. |\n\n**Training Components:**\n| Component | Code | Purpose |\n|-----------|------|---------|\n| Loss Function | `nn.MSELoss()` | Measures prediction error: mean of (prediction - target)² |\n| Optimizer | `optim.SGD(..., lr=0.01)` | Updates weights using gradients. lr = learning rate. |\n\n**Training Loop (The 5 Essential Steps):**\n```python\nfor epoch in range(n_epochs):\n    y_pred = model(X)           # 1. Forward: compute predictions\n    loss = criterion(y_pred, y) # 2. Loss: how wrong are we?\n    optimizer.zero_grad()       # 3. Zero: clear old gradients\n    loss.backward()             # 4. Backward: compute new gradients\n    optimizer.step()            # 5. Update: adjust weights\n```\n\n**Why each step matters:**\n1. **Forward pass** - runs input through model to get predictions\n2. **Compute loss** - quantifies error (lower is better)\n3. **Zero gradients** - gradients accumulate by default; we want fresh ones\n4. **Backward pass** - autograd computes ∂loss/∂weights\n5. **Optimizer step** - applies gradient descent: w_new = w_old - lr × gradient",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Linear Regression - Solutions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 3.1 Solution: Logistic Regression Classifier\n\nLogistic regression extends linear regression for binary classification\nby applying sigmoid function to squash outputs to [0, 1] probabilities.\n\"\"\"\n\ntorch.manual_seed(42)\n\nclass MyClassifier(nn.Module):\n    \"\"\"\n    Binary classifier using logistic regression.\n    \n    Architecture: Linear layer → Sigmoid activation\n    Output is probability of class 1.\n    \"\"\"\n    def __init__(self, input_dim):\n        super().__init__()\n        # Linear layer: maps input features to single output\n        self.linear = nn.Linear(input_dim, 1)\n        # Sigmoid: squashes output to (0, 1) range\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x):\n        # Apply linear transformation then sigmoid\n        z = self.linear(x)    # z = wx + b (can be any real number)\n        return self.sigmoid(z) # probability in (0, 1)\n\n# Create model with 2 input features\nmy_classifier = MyClassifier(input_dim=2)\n\n# Binary Cross Entropy loss for classification\n# Penalizes confident wrong predictions heavily\nmy_bce_loss = nn.BCELoss()\n\n# Adam optimizer: adaptive learning rate, often faster than SGD\nmy_opt = optim.Adam(my_classifier.parameters(), lr=0.1)\n\n# Training loop\nfor epoch in range(100):\n    # Forward pass: get predicted probabilities\n    y_pred = my_classifier(X_train_t)\n    \n    # Compute BCE loss\n    loss = my_bce_loss(y_pred, y_train_t)\n    \n    # Standard backprop steps\n    my_opt.zero_grad()\n    loss.backward()\n    my_opt.step()\n    \n    if (epoch + 1) % 20 == 0:\n        # Calculate accuracy: threshold at 0.5\n        acc = ((y_pred >= 0.5).float() == y_train_t).float().mean()\n        print(f'Epoch {epoch+1}/100 | Loss: {loss.item():.4f} | Acc: {acc.item():.4f}')\n\n# Evaluate on test set\nmy_classifier.eval()  # Switch to evaluation mode\nwith torch.no_grad():  # Disable gradient tracking for inference\n    y_pred = my_classifier(X_test_t)\n    y_class = (y_pred >= 0.5).float()  # Convert probabilities to 0/1\n    acc = (y_class == y_test_t).float().mean()\nprint(f\"\\nTest Accuracy: {acc.item():.4f}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Code Explanation: Exercise 3.1\n\n**Model Architecture:**\n| Component | Code | Purpose |\n|-----------|------|---------|\n| Linear layer | `nn.Linear(input_dim, 1)` | Computes z = w₁x₁ + w₂x₂ + b |\n| Sigmoid | `nn.Sigmoid()` | Converts z to probability: σ(z) = 1/(1+e^(-z)) |\n\n**The Sigmoid Function:**\n```\nσ(z) = 1 / (1 + e^(-z))\n\nz = -∞ → σ(z) = 0\nz = 0  → σ(z) = 0.5\nz = +∞ → σ(z) = 1\n```\n\n**Loss and Optimizer:**\n| Component | Code | Explanation |\n|-----------|------|-------------|\n| `nn.BCELoss()` | Binary Cross Entropy | L = -[y·log(p) + (1-y)·log(1-p)]. Heavily penalizes confident wrong predictions. |\n| `optim.Adam()` | Adaptive optimizer | Combines momentum and adaptive learning rates. Often faster than SGD. |\n\n**Inference Mode:**\n| Code | Purpose |\n|------|---------|\n| `model.eval()` | Disables dropout, uses running stats for batch norm |\n| `torch.no_grad()` | Disables gradient tracking (faster, less memory) |\n| `y_pred >= 0.5` | Converts probabilities to class labels |\n\n**Why logistic regression for classification:**\n- Linear regression outputs can be any real number\n- Classification needs probabilities (0 to 1)\n- Sigmoid transforms linear output to valid probability\n- Decision boundary: predict 1 if probability ≥ 0.5",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 2.1 SOLUTION: Build Linear Regression\n",
    "# ============================================\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the model\n",
    "class MyLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # SOLUTION: Create a linear layer (1 input, 1 output)\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # SOLUTION: Return the output of the linear layer\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create model, loss function, and optimizer\n",
    "my_model = MyLinearRegression()\n",
    "my_criterion = nn.MSELoss()  # SOLUTION\n",
    "my_optimizer = optim.SGD(my_model.parameters(), lr=0.01)  # SOLUTION\n",
    "\n",
    "# Training loop\n",
    "my_losses = []\n",
    "for epoch in range(100):\n",
    "    # SOLUTION: Complete the training loop\n",
    "    # 1. Forward pass\n",
    "    y_pred = my_model(X)\n",
    "    \n",
    "    # 2. Compute loss\n",
    "    loss = my_criterion(y_pred, y)\n",
    "    \n",
    "    # 3. Zero gradients\n",
    "    my_optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Update weights\n",
    "    my_optimizer.step()\n",
    "    \n",
    "    my_losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch {epoch+1}/100 | Loss: {loss.item():.4f}')\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nLearned: w = {my_model.linear.weight.item():.4f}, b = {my_model.linear.bias.item():.4f}\")\n",
    "print(f\"True:    w = 3.0000, b = 2.0000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\"\"\"\nExercise 4.1 Solution: Support Vector Machine\n\nSVMs find the optimal hyperplane that separates classes with maximum margin.\nThe RBF kernel enables non-linear decision boundaries.\n\"\"\"\n\n# Create SVM with RBF (Radial Basis Function) kernel\n# kernel='rbf': uses Gaussian kernel for non-linear boundaries\n# C=10: regularization parameter (higher = less regularization, fits training data more closely)\nmy_svm = SVC(kernel='rbf', C=10)\n\n# Train the SVM on the training data\n# SVM finds support vectors: points closest to decision boundary\nmy_svm.fit(X_train_m, y_train_m)\n\n# Evaluate on test data\n# .score() returns accuracy: (correct predictions / total)\naccuracy = my_svm.score(X_test_m, y_test_m)\n\n# Get number of support vectors\n# These are the training points that define the decision boundary\nn_support = len(my_svm.support_vectors_)\n\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Number of Support Vectors: {n_support}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Code Explanation: Exercise 4.1\n\n| Line | Code | Explanation |\n|------|------|-------------|\n| 1 | `SVC(kernel='rbf', C=10)` | **Creates SVM classifier.** RBF kernel enables non-linear boundaries. C controls regularization. |\n| 2 | `my_svm.fit(X_train_m, y_train_m)` | **Trains the SVM.** Finds optimal hyperplane and identifies support vectors. |\n| 3 | `my_svm.score(X_test_m, y_test_m)` | **Computes test accuracy.** Returns fraction of correctly classified samples. |\n| 4 | `my_svm.support_vectors_` | **Accesses support vectors.** Training points that lie on or within the margin. |\n\n**SVM Kernels:**\n| Kernel | Formula | Use Case |\n|--------|---------|----------|\n| `linear` | K(x,y) = x·y | Linearly separable data |\n| `rbf` | K(x,y) = exp(-γ‖x-y‖²) | Most common, handles non-linear |\n| `poly` | K(x,y) = (x·y + c)^d | Polynomial decision boundaries |\n\n**The C Parameter:**\n```\nC = 0.1  → High regularization, wider margin, may underfit\nC = 1    → Balanced (default)\nC = 10   → Low regularization, narrow margin, fits training data closely\nC = 100  → Very low regularization, may overfit\n```\n\n**What are Support Vectors?**\n- Training points that lie on or inside the margin boundary\n- The decision boundary depends ONLY on these points\n- Removing non-support vectors doesn't change the model\n- Fewer support vectors = simpler, more generalizable model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Generate classification data\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=300, n_features=2, n_redundant=0, n_informative=2,\n",
    "    n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 5.1 Solution: Model Evaluation\n\nThis solution demonstrates comprehensive model evaluation using\nconfusion matrix and classification report for detailed metrics.\n\"\"\"\n\n# Set model to evaluation mode and get predictions\nlog_model.eval()\nwith torch.no_grad():\n    # Get probability predictions\n    y_pred_prob = log_model(X_test_t)\n    # Convert to class labels: 1 if probability >= 0.5, else 0\n    y_pred_log = (y_pred_prob >= 0.5).numpy().astype(int).flatten()\n\n# Create confusion matrix\n# Rows = actual class, Columns = predicted class\n# [[TN, FP], [FN, TP]]\ncm_log = confusion_matrix(y_test, y_pred_log)\n\nprint(\"Confusion Matrix:\")\nprint(cm_log)\nprint()\n\n# Print detailed classification report\n# Shows precision, recall, and F1-score for each class\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred_log, target_names=['Class 0', 'Class 1']))"
  },
  {
   "cell_type": "markdown",
   "source": "### Code Explanation: Exercise 5.1\n\n| Line | Code | Explanation |\n|------|------|-------------|\n| 1 | `confusion_matrix(y_test, y_pred_log)` | **Creates 2×2 confusion matrix.** Compares actual vs predicted labels. |\n| 2 | `classification_report(...)` | **Generates detailed metrics.** Precision, recall, F1-score per class. |\n| 3 | `target_names=['Class 0', 'Class 1']` | **Labels for the report.** Makes output more readable. |\n\n**Understanding the Confusion Matrix:**\n```\n                  Predicted\n                  0     1\nActual  0      [[TN,   FP],    TN = True Negative (correct 0)\n        1       [FN,   TP]]    FP = False Positive (wrong 1)\n                               FN = False Negative (wrong 0)\n                               TP = True Positive (correct 1)\n```\n\n**Classification Metrics:**\n| Metric | Formula | Interpretation |\n|--------|---------|----------------|\n| **Precision** | TP / (TP + FP) | Of predicted positives, how many are correct? |\n| **Recall** | TP / (TP + FN) | Of actual positives, how many did we find? |\n| **F1-Score** | 2 × (P × R) / (P + R) | Harmonic mean of precision and recall |\n| **Support** | Count | Number of actual instances per class |\n\n**When to prioritize each metric:**\n- **Precision**: When false positives are costly (spam detection)\n- **Recall**: When false negatives are costly (disease detection)\n- **F1-Score**: When you need balance between precision and recall\n- **Accuracy**: When classes are balanced and errors are equally costly",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Support Vector Machines - Solutions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Generate moons data\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 4.1 SOLUTION: Train an SVM\n",
    "# ============================================\n",
    "\n",
    "# a) Create an RBF SVM with C=10\n",
    "my_svm = SVC(kernel='rbf', C=10)  # SOLUTION\n",
    "\n",
    "# b) Fit it on the training data\n",
    "my_svm.fit(X_train_m, y_train_m)  # SOLUTION\n",
    "\n",
    "# c) Calculate test accuracy\n",
    "accuracy = my_svm.score(X_test_m, y_test_m)\n",
    "n_support = len(my_svm.support_vectors_)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Number of Support Vectors: {n_support}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Model Evaluation - Solutions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Train a logistic regression model for evaluation\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "\n",
    "log_model = LogisticRegressionModel(input_dim=2)\n",
    "criterion_bce = nn.BCELoss()\n",
    "optimizer_log = optim.Adam(log_model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred_prob = log_model(X_train_t)\n",
    "    loss = criterion_bce(y_pred_prob, y_train_t)\n",
    "    optimizer_log.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_log.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 5.1 SOLUTION: Evaluate Your Model\n",
    "# ============================================\n",
    "\n",
    "# Get predictions from the logistic regression model\n",
    "log_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_prob = log_model(X_test_t)\n",
    "    y_pred_log = (y_pred_prob >= 0.5).numpy().astype(int).flatten()\n",
    "\n",
    "# a) Create confusion matrix\n",
    "cm_log = confusion_matrix(y_test, y_pred_log)  # SOLUTION\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_log)\n",
    "print()\n",
    "\n",
    "# b) Print classification report\n",
    "print(\"Classification Report:\")  # SOLUTION\n",
    "print(classification_report(y_test, y_pred_log, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Part 6: Natural Language Processing - Solutions\n---"
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nExercise 6.1 Solution: Text Preprocessing\n\nText preprocessing cleans and normalizes raw text before feeding it to ML models.\nThis is a critical step - garbage in, garbage out!\n\"\"\"\n\ndef preprocess_text(text):\n    \"\"\"\n    Preprocess text for NLP tasks.\n    \n    Steps:\n    1. Convert to lowercase - makes 'Hello' and 'hello' the same token\n    2. Remove punctuation - keeps only alphanumeric and spaces\n    \n    Args:\n        text: Raw input string\n    \n    Returns:\n        Cleaned string\n    \"\"\"\n    # Step 1: Convert to lowercase\n    text = text.lower()\n    \n    # Step 2: Remove non-alphanumeric characters (keep spaces)\n    # [^a-z0-9\\s] matches anything that's NOT a-z, 0-9, or whitespace\n    text = re.sub(r'[^a-z0-9\\s]', '', text)\n    \n    return text\n\n# Test the function\ntest_text = \"Hello, World! This is NLP 101.\"\ncleaned = preprocess_text(test_text)\n\nprint(f\"Original: {test_text}\")\nprint(f\"Cleaned: {cleaned}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nExercise 6.2 Solution: Bag of Words with CountVectorizer\n\nBag of Words converts text into numerical vectors by counting word occurrences.\nEach document becomes a vector where each dimension represents a word in the vocabulary.\n\"\"\"\n\n# Create sample documents\ndocuments = [\n    \"I love machine learning\",\n    \"Machine learning is great\",\n    \"I love programming\"\n]\n\n# Create CountVectorizer\n# This builds a vocabulary from all documents and counts occurrences\ncount_vectorizer = CountVectorizer()\n\n# fit_transform: builds vocabulary AND transforms documents to vectors\nX_counts = count_vectorizer.fit_transform(documents)\n\n# Get the vocabulary (feature names)\nvocabulary = count_vectorizer.get_feature_names_out()\n\nprint(\"Vocabulary:\", vocabulary)\nprint(f\"\\nDocument-Term Matrix:\")\nprint(X_counts.toarray())\n\n# Explanation of the matrix:\nprint(\"\\nInterpretation:\")\nprint(\"Row 0 (Doc 1 'I love machine learning'):\", X_counts.toarray()[0])\nprint(\"Row 1 (Doc 2 'Machine learning is great'):\", X_counts.toarray()[1])\nprint(\"Row 2 (Doc 3 'I love programming'):\", X_counts.toarray()[2])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nExercise 6.3 Solution: TF-IDF Vectorization\n\nTF-IDF weighs words by their importance:\n- TF (Term Frequency): How often a word appears in a document\n- IDF (Inverse Document Frequency): How rare the word is across all documents\n\nWords that are common in one document but rare overall get HIGH scores.\nWords that appear everywhere (like 'the', 'is') get LOW scores.\n\"\"\"\n\n# Create TF-IDF vectorizer\ntfidf_vectorizer = TfidfVectorizer()\n\n# Transform the same documents\nX_tfidf = tfidf_vectorizer.fit_transform(documents)\n\nprint(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\nprint(f\"\\nTF-IDF Matrix (rounded to 2 decimals):\")\nprint(np.round(X_tfidf.toarray(), 2))\n\n# Key observation\nprint(\"\\nKey Observations:\")\nprint(\"- 'programming' has HIGH weight (0.79) in Doc 3 - it's unique to that document\")\nprint(\"- 'love' has LOWER weight - it appears in multiple documents\")\nprint(\"- 'machine' and 'learning' have moderate weights - appear in 2 docs\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Setup: Sentiment Analysis Dataset\nreviews = [\n    \"This movie was amazing and wonderful\",\n    \"I loved this film, it was great\",\n    \"Excellent movie, highly recommended\",\n    \"Best film I have ever seen\",\n    \"Wonderful story and great acting\",\n    \"This movie was terrible and boring\",\n    \"I hated this film, it was awful\",\n    \"Worst movie ever, do not watch\",\n    \"Boring and disappointing film\",\n    \"Terrible acting and bad story\",\n    \"The movie was okay, nothing special\",\n    \"It was an average film\",\n    \"Not bad but not great either\",\n    \"Mediocre movie with some good moments\",\n    \"Fantastic cinematography and brilliant performances\",\n    \"Absolutely dreadful, waste of time\",\n]\n\n# Labels: 1 = positive, 0 = negative\nlabels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0]\n\n# Split into train and test\nX_train_text, X_test_text, y_train_nlp, y_test_nlp = train_test_split(\n    reviews, labels, test_size=0.25, random_state=42\n)\n\nprint(f\"Training samples: {len(X_train_text)}\")\nprint(f\"Test samples: {len(X_test_text)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nExercise 6.5 Solution: Text Classification with Naive Bayes\n\nNaive Bayes is a probabilistic classifier based on Bayes' Theorem.\nIt works particularly well with text because:\n1. It's fast to train and predict\n2. It handles high-dimensional sparse data (like TF-IDF vectors)\n3. It works well even with limited training data\n\nThe 'naive' assumption: words are conditionally independent given the class.\n\"\"\"\n\n# Step 1: Create TF-IDF vectorizer and transform text\nvectorizer = TfidfVectorizer()\nX_train_vec = vectorizer.fit_transform(X_train_text)  # fit AND transform training data\nX_test_vec = vectorizer.transform(X_test_text)        # only transform test data (use same vocabulary!)\n\nprint(f\"Training vectors shape: {X_train_vec.shape}\")\nprint(f\"Test vectors shape: {X_test_vec.shape}\")\nprint(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n\n# Step 2: Create and train Naive Bayes classifier\nclf = MultinomialNB()\nclf.fit(X_train_vec, y_train_nlp)\n\n# Step 3: Make predictions\ny_pred_nlp = clf.predict(X_test_vec)\n\n# Step 4: Calculate accuracy\naccuracy = accuracy_score(y_test_nlp, y_pred_nlp)\nprint(f\"\\nAccuracy: {accuracy:.2f}\")\n\n# Step 5: Print classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_nlp, y_pred_nlp, target_names=['Negative', 'Positive']))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nExercise 6.6 Solution: Predict on New Text\n\nUse the trained model to classify completely new reviews.\nImportant: Use .transform() (not .fit_transform()) to use the same vocabulary!\n\"\"\"\n\n# Create new reviews to classify\nnew_reviews = [\n    \"This was the best experience ever!\",\n    \"Horrible waste of my time\",\n    \"It was pretty good overall\",\n    \"I absolutely loved every moment\",\n    \"Disappointing and frustrating\"\n]\n\n# Vectorize using the SAME vectorizer (important!)\nnew_vectors = vectorizer.transform(new_reviews)\n\n# Predict sentiment\npredictions = clf.predict(new_vectors)\n\n# Get probability scores\nprobabilities = clf.predict_proba(new_vectors)\n\n# Display results\nprint(\"Predictions on New Reviews:\\n\")\nfor review, pred, prob in zip(new_reviews, predictions, probabilities):\n    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n    confidence = max(prob) * 100\n    print(f\"Review: \\\"{review}\\\"\")\n    print(f\"Predicted: {sentiment} (confidence: {confidence:.1f}%)\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Code Explanation: NLP Pipeline\n\n| Step | Code | Explanation |\n|------|------|-------------|\n| 1 | `TfidfVectorizer()` | Creates a vectorizer that converts text to TF-IDF weighted vectors |\n| 2 | `.fit_transform(X_train)` | Builds vocabulary from training data AND transforms it to vectors |\n| 3 | `.transform(X_test)` | Transforms test data using the SAME vocabulary (no fitting!) |\n| 4 | `MultinomialNB()` | Creates a Naive Bayes classifier suited for discrete features like word counts |\n| 5 | `.fit(X, y)` | Trains the classifier on vectorized text and labels |\n| 6 | `.predict(X)` | Makes predictions on new vectorized text |\n| 7 | `.predict_proba(X)` | Returns probability scores for each class |\n\n**Why use .transform() not .fit_transform() for test data?**\n- `fit_transform()` creates a NEW vocabulary from the data\n- `transform()` uses the EXISTING vocabulary from training\n- If we fit on test data, we might have different words/features\n- The model was trained on the training vocabulary - we must use the same one!\n\n**Common NLP Pipeline Pattern:**\n```python\n# Training\nvectorizer = TfidfVectorizer()\nX_train_vec = vectorizer.fit_transform(X_train)  # fit + transform\nclf = MultinomialNB()\nclf.fit(X_train_vec, y_train)\n\n# Testing/Inference\nX_test_vec = vectorizer.transform(X_test)  # only transform!\npredictions = clf.predict(X_test_vec)\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Solutions Summary\n\n## Exercise 1.1: Create Tensors\n```python\ntensor_a = torch.tensor([10, 20, 30, 40, 50])\ntensor_b = torch.zeros(3, 4)\ntensor_c = torch.rand(2, 5)\n```\n\n## Exercise 1.2: Tensor Operations\n```python\nresult_add = x + y\nresult_mean = x.mean()\nresult_row_sum = x.sum(dim=1)\nresult_reshape = x.reshape(3, 2)\n```\n\n## Exercise 1.3: Autograd\n```python\nx = torch.tensor([2.0], requires_grad=True)\ny = 3*x**2 - 4*x + 5\ny.backward()\n```\n\n## Exercise 2.1: Linear Regression\n```python\nself.linear = nn.Linear(1, 1)\nreturn self.linear(x)\nmy_criterion = nn.MSELoss()\nmy_optimizer = optim.SGD(my_model.parameters(), lr=0.01)\n```\n\n## Exercise 3.1: Logistic Regression\n```python\nself.linear = nn.Linear(input_dim, 1)\nself.sigmoid = nn.Sigmoid()\nreturn self.sigmoid(self.linear(x))\nmy_bce_loss = nn.BCELoss()\nmy_opt = optim.Adam(my_classifier.parameters(), lr=0.1)\n```\n\n## Exercise 4.1: SVM\n```python\nmy_svm = SVC(kernel='rbf', C=10)\nmy_svm.fit(X_train_m, y_train_m)\n```\n\n## Exercise 5.1: Evaluation\n```python\ncm_log = confusion_matrix(y_test, y_pred_log)\nprint(classification_report(y_test, y_pred_log))\n```\n\n## Exercise 6.1-6.6: NLP\n```python\n# Text preprocessing\ntext = text.lower()\ntext = re.sub(r'[^a-z0-9\\s]', '', text)\n\n# Bag of Words\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(documents)\n\n# TF-IDF\ntfidf = TfidfVectorizer()\nX_tfidf = tfidf.fit_transform(documents)\n\n# Naive Bayes Classification\nvectorizer = TfidfVectorizer()\nX_train_vec = vectorizer.fit_transform(X_train_text)\nX_test_vec = vectorizer.transform(X_test_text)\nclf = MultinomialNB()\nclf.fit(X_train_vec, y_train_nlp)\npredictions = clf.predict(X_test_vec)\n```",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}