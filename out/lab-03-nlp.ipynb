{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 3: Natural Language Processing\n\n**Duration:** 60-75 minutes | **Difficulty:** Intermediate\n\n---\n\n## Overview\n\nThis lab teaches NLP fundamentals through hands-on implementation.\n\n### Lab Structure\n\n| Section | Topic | Key Concepts |\n|---------|-------|---------------|\n| **1** | Text Preprocessing | Cleaning, tokenization, normalization |\n| **2** | Bag of Words | CountVectorizer, document-term matrix |\n| **3** | TF-IDF | Term frequency, inverse document frequency |\n| **4** | Sentiment Analysis | Classification dataset |\n| **5** | Naive Bayes | Text classification |\n| **6** | Prediction | Classifying new text |\n| **7** | POS Tagging | Part-of-speech identification |\n| **8** | Named Entity Recognition | Extracting entities from text |\n\n### Instructions\n\n- Read each markdown cell carefully\n- Write your code in the empty code cells\n- Run cells with `Shift+Enter`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cell below to import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\nimport pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Import spaCy for POS tagging and NER\nimport spacy\n\n# Load the English language model\n# If not installed, run: python -m spacy download en_core_web_sm\nnlp = spacy.load(\"en_core_web_sm\")\n\nprint(\"Setup complete!\")\nprint(f\"spaCy version: {spacy.__version__}\")\nprint(f\"Model loaded: en_core_web_sm\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Part 6: Natural Language Processing\n\nLearn to process text data for machine learning: tokenization, vectorization, and text classification."
  },
  {
   "cell_type": "markdown",
   "source": "## 6.1 Text Preprocessing\n\nBefore feeding text to ML models, we must clean and normalize it.\n\nCommon preprocessing steps:\n\n| Step | Description | Example |\n|------|-------------|---------|\n| Lowercase | Convert to lowercase | \"Hello World\" → \"hello world\" |\n| Remove punctuation | Strip special characters | \"Hello!\" → \"Hello\" |\n| Tokenization | Split into words | \"hello world\" → [\"hello\", \"world\"] |\n| Remove stopwords | Remove common words | [\"the\", \"is\", \"a\"] removed |\n\n**Your Task:** Write a function `preprocess_text(text)` that:\n1. Converts text to lowercase\n2. Removes all non-alphanumeric characters (keep spaces)\n3. Returns the cleaned text\n\nTest it on: `\"Hello, World! This is NLP 101.\"`\n\n**Expected Output:**\n```\nOriginal: Hello, World! This is NLP 101.\nCleaned: hello world this is nlp 101\n```\n\n**Sample Code:**\n```python\nimport re\n\ndef clean_example(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Keep only alphanumeric and spaces\n    return text\n\nresult = clean_example(\"Test! 123\")\nprint(result)  # \"test 123\"\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Your code here\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.2 Bag of Words with CountVectorizer\n\nBag of Words converts text to numerical vectors by counting word occurrences.\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(documents)  # Returns sparse matrix\nprint(vectorizer.get_feature_names_out())  # Vocabulary\nprint(X.toarray())  # Dense matrix\n```\n\n**Your Task:**\n1. Create a list of 3 documents:\n   - \"I love machine learning\"\n   - \"Machine learning is great\"\n   - \"I love programming\"\n2. Create a `CountVectorizer` and fit_transform the documents\n3. Print the vocabulary (feature names)\n4. Print the document-term matrix as an array\n\n**Expected Output:**\n```\nVocabulary: ['great' 'is' 'learning' 'love' 'machine' 'programming']\n\nDocument-Term Matrix:\n[[0 0 1 1 1 0]\n [1 1 1 0 1 0]\n [0 0 0 1 0 1]]\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Your code here\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.3 TF-IDF Vectorization\n\nTF-IDF (Term Frequency - Inverse Document Frequency) weights words by importance:\n- **TF**: How often a word appears in a document\n- **IDF**: How rare a word is across all documents\n- Words common in one doc but rare overall get high scores\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer()\nX = tfidf.fit_transform(documents)\nprint(X.toarray())  # TF-IDF weighted matrix\n```\n\n**Your Task:**\n1. Use the same 3 documents from 6.2\n2. Create a `TfidfVectorizer` and fit_transform the documents\n3. Print the TF-IDF matrix (rounded to 2 decimals)\n4. Observe: Which words have higher weights? Why?\n\n**Expected Output:**\n```\nTF-IDF Matrix:\n[[0.   0.   0.52 0.68 0.52 0.  ]\n [0.55 0.55 0.42 0.   0.42 0.  ]\n [0.   0.   0.   0.61 0.   0.79]]\n\nNote: 'programming' has high weight in doc 3 (unique to it)\n      'love' has lower weight (appears in docs 1 and 3)\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Your code here\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.4 Sentiment Analysis Dataset\n\nRun the cell below to create a simple sentiment analysis dataset.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sentiment Analysis Dataset\nreviews = [\n    \"This movie was amazing and wonderful\",\n    \"I loved this film, it was great\",\n    \"Excellent movie, highly recommended\",\n    \"Best film I have ever seen\",\n    \"Wonderful story and great acting\",\n    \"This movie was terrible and boring\",\n    \"I hated this film, it was awful\",\n    \"Worst movie ever, do not watch\",\n    \"Boring and disappointing film\",\n    \"Terrible acting and bad story\",\n    \"The movie was okay, nothing special\",\n    \"It was an average film\",\n    \"Not bad but not great either\",\n    \"Mediocre movie with some good moments\",\n    \"Fantastic cinematography and brilliant performances\",\n    \"Absolutely dreadful, waste of time\",\n]\n\n# Labels: 1 = positive, 0 = negative\nlabels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0]\n\n# Split into train and test\nX_train_text, X_test_text, y_train_nlp, y_test_nlp = train_test_split(\n    reviews, labels, test_size=0.25, random_state=42\n)\n\nprint(f\"Training samples: {len(X_train_text)}\")\nprint(f\"Test samples: {len(X_test_text)}\")\nprint(f\"\\nSample reviews:\")\nfor i in range(3):\n    sentiment = \"Positive\" if y_train_nlp[i] == 1 else \"Negative\"\n    print(f\"  [{sentiment}] {X_train_text[i]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.5 Text Classification with Naive Bayes\n\nNaive Bayes is a probabilistic classifier that works well with text data.\n\n```python\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Vectorize text\nvectorizer = TfidfVectorizer()\nX_train_vec = vectorizer.fit_transform(X_train_text)\nX_test_vec = vectorizer.transform(X_test_text)\n\n# Train classifier\nclf = MultinomialNB()\nclf.fit(X_train_vec, y_train)\n\n# Predict\npredictions = clf.predict(X_test_vec)\n```\n\n**Your Task:**\n1. Create a `TfidfVectorizer` and vectorize the training and test text\n2. Create a `MultinomialNB` classifier and train it\n3. Make predictions on the test set\n4. Calculate and print the accuracy\n5. Print the classification report\n\n**Expected Output:**\n```\nAccuracy: ~0.75 (75%)\n\nClassification Report:\n              precision    recall  f1-score   support\n           0       X.XX      X.XX      X.XX         X\n           1       X.XX      X.XX      X.XX         X\n```\n\n**Sample Code:**\n```python\n# Complete pipeline example\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('clf', MultinomialNB())\n])\npipeline.fit(X_train_text, y_train_nlp)\naccuracy = pipeline.score(X_test_text, y_test_nlp)\nprint(f\"Accuracy: {accuracy:.2f}\")\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Your code here\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.6 Predict on New Text\n\nUse your trained model to classify new reviews.\n\n**Your Task:**\n1. Create a list of 3 new reviews (make up your own!)\n2. Vectorize them using the same vectorizer (use `.transform()`, not `.fit_transform()`)\n3. Use your classifier to predict the sentiment\n4. Print each review with its predicted sentiment\n\n**Expected Output:**\n```\nReview: \"This was the best experience ever!\"\nPredicted: Positive\n\nReview: \"Horrible waste of my time\"\nPredicted: Negative\n\nReview: \"It was pretty good overall\"\nPredicted: Positive\n```\n\n**Sample Code:**\n```python\nnew_reviews = [\"Your review here\", \"Another review\"]\nnew_vectors = vectorizer.transform(new_reviews)\npredictions = clf.predict(new_vectors)\n\nfor review, pred in zip(new_reviews, predictions):\n    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n    print(f\"Review: {review}\")\n    print(f\"Predicted: {sentiment}\\n\")\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Your code here\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7wj5aimgjqw",
   "source": "---\n# Lab Complete!\n\n## Summary\n\nYou learned:\n- **Text Preprocessing**: Cleaning, tokenization, normalization\n- **Bag of Words**: CountVectorizer, document-term matrix representation\n- **TF-IDF**: Term frequency-inverse document frequency weighting\n- **Naive Bayes**: Probabilistic text classification\n- **Part-of-Speech Tagging**: Identifying grammatical roles (nouns, verbs, adjectives)\n- **Named Entity Recognition**: Extracting people, organizations, locations, dates, money\n\n## Key Libraries\n\n| Library | Purpose |\n|---------|---------|\n| `sklearn` | ML algorithms, vectorizers |\n| `spaCy` | POS tagging, NER, NLP pipeline |\n| `re` | Regular expressions for text cleaning |\n\n## Next Steps\n\n- Try processing your own text documents\n- Explore other spaCy models (`en_core_web_md`, `en_core_web_lg`) for better accuracy\n- Combine NER with sentiment analysis for opinion mining\n- Build a simple chatbot or Q&A system using these techniques",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "yxfi3u3j03m",
   "source": "# Your code here - POS Tagging\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "h902shzhofu",
   "source": "---\n## 6.8 Named Entity Recognition (NER)\n\nNER identifies and classifies named entities in text into predefined categories.\n\n### Common Entity Types\n\n| Entity | Description | Example |\n|--------|-------------|---------|\n| **PERSON** | People, including fictional | Bill Gates, Harry Potter |\n| **ORG** | Organizations | Google, United Nations |\n| **GPE** | Countries, cities, states | France, New York, Texas |\n| **LOC** | Non-GPE locations | Mount Everest, Pacific Ocean |\n| **DATE** | Dates or periods | January 2024, last week |\n| **TIME** | Times | 3pm, midnight |\n| **MONEY** | Monetary values | $100, 50 euros |\n| **PERCENT** | Percentages | 25%, fifty percent |\n| **PRODUCT** | Products | iPhone, Windows 10 |\n| **EVENT** | Named events | World War II, Olympics |\n\n### Why NER Matters\n\n- **Information Extraction**: Extract key facts from documents\n- **Question Answering**: Find answers about who, what, where, when\n- **Knowledge Graphs**: Build structured data from unstructured text\n- **Content Classification**: Categorize articles by entities mentioned\n\n```python\ndoc = nlp(\"Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.\")\n\nfor ent in doc.ents:\n    print(f\"{ent.text:20} {ent.label_:10} {spacy.explain(ent.label_)}\")\n```\n\n**Your Task:**\n1. Process this text:\n   ```\n   \"Elon Musk, CEO of Tesla and SpaceX, announced on Tuesday that the company \n   will invest $10 billion in a new factory in Austin, Texas. The project is \n   expected to create 5,000 jobs by December 2025.\"\n   ```\n2. Extract and print all named entities with their labels\n3. Group entities by type (all PERSON entities, all ORG entities, etc.)\n4. Create a function `extract_entities(text)` that returns a dictionary with entity types as keys and lists of entities as values\n\n**Expected Output:**\n```\nEntity               Label      Description\nElon Musk            PERSON     People, including fictional\nTesla                ORG        Companies, agencies, institutions\nSpaceX               ORG        Companies, agencies, institutions\nTuesday              DATE       Absolute or relative dates\n$10 billion          MONEY      Monetary values\nAustin               GPE        Countries, cities, states\nTexas                GPE        Countries, cities, states\nDecember 2025        DATE       Absolute or relative dates\n\nEntities by Type:\nPERSON: ['Elon Musk']\nORG: ['Tesla', 'SpaceX']\nGPE: ['Austin', 'Texas']\nDATE: ['Tuesday', 'December 2025']\nMONEY: ['$10 billion']\nCARDINAL: ['5,000']\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3h6gfbwvj7c",
   "source": "# Your code here - Named Entity Recognition\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "muwo0oozv7o",
   "source": "---\n## 6.9 Bonus Challenge: Combining POS and NER\n\nCombine POS tagging and NER to extract structured information from text.\n\n**Challenge:** Create a text analyzer that processes a news article and extracts:\n1. All named entities grouped by type\n2. All action verbs (what's happening)\n3. All adjectives (descriptive words)\n4. A summary showing: \"WHO did WHAT WHERE WHEN\"\n\n**Sample Text:**\n```\n\"Microsoft announced on Monday that CEO Satya Nadella will visit London next week \nto discuss a $5 billion investment in artificial intelligence research. The tech \ngiant plans to hire 2,000 engineers across Europe by 2026.\"\n```\n\n**Expected Output:**\n```\n=== TEXT ANALYSIS REPORT ===\n\nENTITIES:\n  Organizations: Microsoft\n  People: Satya Nadella\n  Locations: London, Europe\n  Dates: Monday, next week, 2026\n  Money: $5 billion\n  Numbers: 2,000\n\nACTIONS (Verbs):\n  announced, visit, discuss, plans, hire\n\nDESCRIPTORS (Adjectives):\n  artificial\n\nSUMMARY:\n  WHO: Microsoft, Satya Nadella\n  WHAT: announced, visit, discuss investment, hire engineers\n  WHERE: London, Europe\n  WHEN: Monday, next week, 2026\n```\n\n**Sample Code:**\n```python\ndef analyze_text(text):\n    doc = nlp(text)\n    \n    # Extract entities\n    entities = {}\n    for ent in doc.ents:\n        if ent.label_ not in entities:\n            entities[ent.label_] = []\n        entities[ent.label_].append(ent.text)\n    \n    # Extract verbs\n    verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n    \n    # Extract adjectives  \n    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n    \n    return {\n        \"entities\": entities,\n        \"verbs\": list(set(verbs)),\n        \"adjectives\": list(set(adjectives))\n    }\n\nresult = analyze_text(sample_text)\nprint(result)\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "smn9kpsyp7e",
   "source": "# Your code here - Bonus Challenge: Text Analyzer\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# Lab Complete!\n\n## Summary\n\nYou learned:\n- **PyTorch Tensors**: Create, manipulate, and use autograd\n- **Linear Regression**: nn.Module, training loop, MSE loss\n- **Logistic Regression**: Sigmoid, BCE loss, classification\n- **SVMs**: Different kernels for linear/non-linear data\n- **Evaluation**: Confusion matrix, precision, recall, F1-score\n- **NLP**: Text preprocessing, Bag of Words, TF-IDF, Naive Bayes classification",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}