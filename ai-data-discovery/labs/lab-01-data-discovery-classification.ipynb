{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 1: Data Discovery & Classification\n\n**Data Discovery: Harnessing AI, AGI & Vector Databases - Day 1**\n\n| Duration | Framework | Sections |\n|---|---|---|\n| 90 min | pandas, scikit-learn, sentence-transformers, chromadb | 5 |\n\nIn this lab, you'll explore:\n- Profiling synthetic data assets and identifying quality issues\n- Building a text classifier with TF-IDF and RandomForest\n- Extracting metadata using regex patterns\n- Discovering data clusters with KMeans and PCA\n- Building a vector catalogue with semantic search\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "## Student Notes & Background\n\n### What is Data Discovery?\n\n**Data discovery** is the process of finding, understanding, and cataloguing data assets across an organisation. In a typical enterprise, data lives in dozens of systems — relational databases, cloud storage buckets, shared drives, SaaS platforms, and legacy mainframes. Without a systematic discovery process, teams waste hours searching for data they need, duplicate efforts, and risk using stale or incorrect information.\n\nModern data discovery goes beyond simple file listings. It combines **profiling** (understanding what's in the data), **classification** (labelling data by department, sensitivity, or type), and **search** (finding relevant assets using natural language queries). In this lab, you'll build all three capabilities from scratch.\n\n### Key Concepts\n\n#### 1. Data Profiling\n**Data profiling** examines a dataset to collect statistics and identify quality issues. A thorough profile includes:\n- **Shape and structure** — how many rows, columns, and what data types?\n- **Value distributions** — what are the most common values in each column?\n- **Missing data** — which fields have gaps, and how severe are they?\n- **Sensitivity breakdown** — how much data is public vs. restricted?\n\nProfiling is always the first step in data discovery because you cannot classify or govern data you don't understand.\n\n#### 2. TF-IDF (Term Frequency–Inverse Document Frequency)\n**TF-IDF** converts text into numerical vectors by measuring how important each word is to a document relative to the entire corpus:\n- **Term Frequency (TF):** How often a word appears in a single document (higher = more relevant to that document)\n- **Inverse Document Frequency (IDF):** How rare a word is across all documents (rarer words carry more information)\n- **TF-IDF = TF × IDF** — words that are frequent in one document but rare overall get the highest scores\n\nFor example, \"employee\" in an HR document scores high on TF, but if it appears across many departments, its IDF is lower. \"Payroll,\" which appears almost exclusively in HR documents, gets a very high TF-IDF score for HR assets.\n\n#### 3. Random Forest Classification\nA **Random Forest** is an ensemble of decision trees that vote on the final prediction. For text classification:\n1. Each tree is trained on a random subset of the TF-IDF features\n2. Each tree makes an independent prediction\n3. The forest takes a majority vote across all trees\n\nRandom Forests are robust to overfitting, handle high-dimensional data well (TF-IDF can produce thousands of features), and provide feature importance scores that reveal which words are most predictive of each category.\n\n#### 4. Metadata Extraction with Regex\n**Regular expressions (regex)** are pattern-matching rules that can extract structured information from unstructured text. In data discovery, regex is used to:\n- Find business terms (e.g., \"salary,\" \"revenue,\" \"compliance\") in asset descriptions\n- Detect PII patterns (e.g., SSN format `XXX-XX-XXXX`, email addresses)\n- Extract identifiers (e.g., invoice numbers, employee IDs)\n\nRegex extraction is fast and deterministic — the same pattern always produces the same result, making it ideal for automated metadata tagging.\n\n#### 5. KMeans Clustering & PCA\n**KMeans clustering** groups data points into *k* clusters by minimising the distance between each point and its assigned cluster centre. Applied to TF-IDF vectors, KMeans discovers natural groupings in the catalogue — assets with similar descriptions end up in the same cluster, even without labelled training data.\n\n**PCA (Principal Component Analysis)** reduces high-dimensional vectors to 2D or 3D for visualisation. It finds the axes of maximum variance in the data, so the 2D projection preserves as much structure as possible. The resulting scatter plot reveals whether clusters are well-separated or overlapping.\n\n#### 6. Vector Catalogues & Semantic Search\nA **vector catalogue** stores data asset descriptions as dense embeddings (from a model like `all-MiniLM-L6-v2`) in a vector database (ChromaDB). Unlike TF-IDF, these embeddings capture **semantic meaning** — \"employee compensation\" and \"staff salary\" will have similar vectors even though they share no words.\n\n**Semantic search** queries the catalogue with a natural language question and returns the most similar assets by cosine distance. This is the foundation of modern data discovery platforms.\n\n### What You'll Build\n\nIn this lab, you will:\n1. **Profile** a synthetic catalogue of 500 data assets to understand its structure, distributions, and quality issues\n2. **Train** a TF-IDF + Random Forest classifier that predicts an asset's department from its text description\n3. **Extract** business terms from descriptions using regex patterns and analyse their frequency distribution\n4. **Cluster** assets with KMeans on TF-IDF vectors and visualise the clusters in 2D using PCA\n5. **Build** a ChromaDB vector catalogue with sentence-transformer embeddings and perform semantic searches\n\n### Prerequisites\n- Basic Python: lists, dictionaries, functions, f-strings\n- Familiarity with pandas DataFrames (selecting columns, filtering rows, value counts)\n- No prior machine learning experience required — all concepts are introduced as needed\n\n### Tips\n- All synthetic data uses `np.random.seed(42)` for reproducibility — your numbers should match the solution exactly\n- When evaluating the classifier, look at the **per-category F1 scores** in the classification report, not just overall accuracy\n- For clustering, try different values of `n_clusters` (3, 5, 7) and observe how the PCA visualisation changes\n- Semantic search results include a **distance** metric — lower distance means higher relevance\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Vector database\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Synthetic Data Assets\n",
    "\n",
    "We'll create a synthetic catalogue of ~500 data assets representing a typical enterprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "categories = ['HR', 'Finance', 'Marketing', 'Engineering', 'Legal']\n",
    "sources = ['PostgreSQL', 'S3 Bucket', 'SharePoint', 'Salesforce', 'MongoDB']\n",
    "data_types = ['Table', 'Document', 'Spreadsheet', 'Log File', 'Report']\n",
    "sensitivity_levels = ['Public', 'Internal', 'Confidential', 'Restricted']\n",
    "\n",
    "descriptions_pool = {\n",
    "    'HR': [\n",
    "        'Employee personal records including name address and date of birth',\n",
    "        'Annual performance review scores and manager feedback',\n",
    "        'Payroll data with salary deductions and tax withholdings',\n",
    "        'Recruitment pipeline tracking applicant status and interview notes',\n",
    "        'Benefits enrollment records for health dental and vision plans',\n",
    "        'Employee onboarding documentation and training completion',\n",
    "        'Workforce diversity and inclusion metrics by department',\n",
    "        'Time and attendance records with overtime calculations',\n",
    "        'Employee termination records and exit interview summaries',\n",
    "        'Compensation benchmarking data across industry roles',\n",
    "    ],\n",
    "    'Finance': [\n",
    "        'Quarterly revenue reports broken down by business unit',\n",
    "        'Accounts payable invoices and payment processing records',\n",
    "        'Annual budget forecasts with departmental allocations',\n",
    "        'Customer billing records including credit card transactions',\n",
    "        'Expense reimbursement claims with receipt attachments',\n",
    "        'General ledger entries and journal adjustments',\n",
    "        'Tax filing documents and regulatory compliance records',\n",
    "        'Cash flow projections and working capital analysis',\n",
    "        'Vendor payment terms and contract financial summaries',\n",
    "        'Audit trail logs for financial transaction approvals',\n",
    "    ],\n",
    "    'Marketing': [\n",
    "        'Campaign performance metrics including click rates and conversions',\n",
    "        'Customer segmentation profiles based on purchase behaviour',\n",
    "        'Social media analytics with engagement and reach data',\n",
    "        'Email marketing subscriber lists with opt-in preferences',\n",
    "        'Brand sentiment analysis from customer reviews and surveys',\n",
    "        'Website traffic analytics and user journey tracking',\n",
    "        'Lead scoring models and marketing qualified lead reports',\n",
    "        'Content calendar and editorial planning documents',\n",
    "        'Competitive intelligence reports and market research data',\n",
    "        'Event registration lists with attendee contact information',\n",
    "    ],\n",
    "    'Engineering': [\n",
    "        'Application server logs with error traces and stack dumps',\n",
    "        'CI/CD pipeline metrics including build times and failure rates',\n",
    "        'Infrastructure monitoring data from cloud resources',\n",
    "        'API usage statistics and rate limiting configurations',\n",
    "        'Database schema documentation and migration scripts',\n",
    "        'Code repository commit history and pull request reviews',\n",
    "        'Load testing results and performance benchmarks',\n",
    "        'Security vulnerability scan reports and remediation tracking',\n",
    "        'Microservice dependency maps and architecture diagrams',\n",
    "        'Incident response logs and post-mortem analysis documents',\n",
    "    ],\n",
    "    'Legal': [\n",
    "        'Active contract repository with vendor agreements and SLAs',\n",
    "        'Intellectual property filings including patents and trademarks',\n",
    "        'Regulatory compliance audit findings and remediation plans',\n",
    "        'Data processing agreements under GDPR Article 28',\n",
    "        'Litigation case files and legal correspondence records',\n",
    "        'Corporate governance meeting minutes and board resolutions',\n",
    "        'Privacy impact assessments for new data processing activities',\n",
    "        'Non-disclosure agreement tracking and expiration dates',\n",
    "        'Employment law compliance documentation by jurisdiction',\n",
    "        'Insurance policy records and claims history',\n",
    "    ],\n",
    "}\n",
    "\n",
    "n_assets = 500\n",
    "records = []\n",
    "\n",
    "for i in range(n_assets):\n",
    "    cat = np.random.choice(categories)\n",
    "    desc = np.random.choice(descriptions_pool[cat])\n",
    "    # Add slight variation\n",
    "    if np.random.random() < 0.3:\n",
    "        desc += ' updated ' + np.random.choice(['weekly', 'monthly', 'quarterly', 'annually'])\n",
    "    records.append({\n",
    "        'asset_id': f'ASSET-{i+1:04d}',\n",
    "        'name': f'{cat.lower()}_{np.random.choice([\"report\", \"dataset\", \"log\", \"file\", \"table\"])}_{i+1:04d}',\n",
    "        'description': desc,\n",
    "        'category': cat,\n",
    "        'source': np.random.choice(sources),\n",
    "        'data_type': np.random.choice(data_types),\n",
    "        'sensitivity': np.random.choice(sensitivity_levels, p=[0.15, 0.35, 0.30, 0.20]),\n",
    "        'owner': np.random.choice(['alice', 'bob', 'carol', 'dave', 'eve', None], p=[0.2, 0.2, 0.2, 0.2, 0.15, 0.05]),\n",
    "        'row_count': np.random.randint(100, 1_000_000) if np.random.random() > 0.2 else None,\n",
    "        'last_updated': pd.Timestamp('2023-01-01') + pd.Timedelta(days=int(np.random.randint(0, 730))),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(f\"Generated {len(df)} data asset records\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1.1: Data Profiling\n\nThe code below profiles the data asset catalogue — examining its shape, distributions, and quality issues. This is always the first step in data discovery because you cannot classify or govern data you don't understand."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Shape and data types\nprint(f\"Shape: {df.shape}\")\nprint(f\"\\nData Types:\")\nprint(df.dtypes)\nprint(f\"\\nMemory Usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Value counts\nprint(\"Category distribution:\")\nprint(df['category'].value_counts())\nprint(\"\\nSource distribution:\")\nprint(df['source'].value_counts())\nprint(\"\\nData type distribution:\")\nprint(df['data_type'].value_counts())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Missing values\nmissing = df.isnull().sum()\nmissing_pct = (missing / len(df) * 100).round(1)\nprint(\"Missing values:\")\nprint(pd.DataFrame({'Missing': missing, 'Percent': missing_pct}).query('Missing > 0'))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sensitivity distribution\nfig, ax = plt.subplots(figsize=(8, 5))\ndf['sensitivity'].value_counts().plot(kind='bar', ax=ax, color=['#10b981', '#3b82f6', '#f59e0b', '#ef4444'])\nax.set_title('Data Asset Sensitivity Distribution')\nax.set_xlabel('Sensitivity Level')\nax.set_ylabel('Count')\nplt.xticks(rotation=0)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "source": "### Analysis Questions\n\n1. Which categories have the most assets? Is the distribution balanced across departments?\n2. What percentage of assets are missing an owner? What governance risk does this create?\n3. How does the sensitivity distribution compare to what you'd expect in a real enterprise?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1.2: Text Classification with TF-IDF + RandomForest\n\nThe code below builds a text classifier that predicts an asset's department from its description. It uses TF-IDF to convert descriptions into numerical vectors, then trains a Random Forest ensemble to learn the mapping from text features to categories."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_classifier(df):\n    \"\"\"Build a TF-IDF + RandomForest text classifier.\"\"\"\n    # Vectorise descriptions\n    tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n    X_tfidf = tfidf.fit_transform(df['description'])\n    y = df['category']\n\n    # Train/test split\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_tfidf, y, test_size=0.2, random_state=42\n    )\n\n    # Train classifier\n    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n    clf.fit(X_train, y_train)\n\n    # Evaluate\n    y_pred = clf.predict(X_test)\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n\n    return tfidf, clf, X_tfidf\n\ntfidf, clf, X_tfidf = build_classifier(df)"
  },
  {
   "cell_type": "markdown",
   "source": "### Analysis Questions\n\n1. Which department has the highest F1-score? Which has the lowest? Why might some departments be harder to classify?\n2. Look at the confusion matrix — which categories get confused with each other? What do their descriptions have in common?\n3. If you were deploying this classifier in production, what accuracy threshold would you require?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2.1: Metadata Extraction\n\nThe code below extracts business terms from data asset descriptions using regex patterns. This automated metadata tagging helps data stewards quickly understand what each asset contains without reading every description manually."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_business_terms(text):\n    \"\"\"Extract business terms from a data asset description.\"\"\"\n    terms = [\n        'salary', 'revenue', 'customer', 'employee', 'invoice',\n        'compliance', 'contract', 'performance', 'billing', 'payroll',\n        'budget', 'marketing', 'security', 'legal', 'audit'\n    ]\n    found = []\n    for term in terms:\n        if re.search(rf'\\b{term}\\b', text, re.IGNORECASE):\n            found.append(term)\n    return found\n\n# Apply to all descriptions\ndf['business_terms'] = df['description'].apply(extract_business_terms)\n\n# Count most common terms\nall_terms = [term for terms_list in df['business_terms'] for term in terms_list]\nterm_counts = Counter(all_terms).most_common(10)\nprint(\"Top 10 business terms:\")\nfor term, count in term_counts:\n    print(f\"  {term:15} {count}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Analysis Questions\n\n1. Which business terms appear most frequently? Do they align with the department distribution?\n2. Are there terms that appear across multiple departments? What does this tell you about data overlap?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2.2: Unsupervised Discovery with Clustering\n\nThe code below uses KMeans clustering on TF-IDF vectors to discover natural groupings in the data catalogue, then visualises them in 2D using PCA. Unlike the supervised classifier in Section 1.2, clustering finds structure without any labels."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def cluster_and_visualise(X_tfidf, df, n_clusters=5):\n    \"\"\"Cluster data assets using KMeans and visualise with PCA.\"\"\"\n    # Fit KMeans\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    clusters = kmeans.fit_predict(X_tfidf)\n\n    # Reduce to 2D\n    pca = PCA(n_components=2)\n    coords = pca.fit_transform(X_tfidf.toarray())\n\n    # Plot\n    fig, ax = plt.subplots(figsize=(12, 8))\n    scatter = ax.scatter(coords[:, 0], coords[:, 1], c=clusters, cmap='viridis', alpha=0.6, s=30)\n    plt.colorbar(scatter, label='Cluster')\n    ax.set_title('Data Asset Clusters (PCA Projection)')\n    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n    plt.tight_layout()\n    plt.show()\n\n    # Print cluster composition\n    df_temp = df.copy()\n    df_temp['cluster'] = clusters\n    print(\"\\nCluster composition (actual categories):\")\n    print(pd.crosstab(df_temp['cluster'], df_temp['category']))\n\n    return clusters\n\nclusters = cluster_and_visualise(X_tfidf, df)"
  },
  {
   "cell_type": "markdown",
   "source": "### Analysis Questions\n\n1. Do the 5 clusters correspond to the 5 actual categories? Where do they diverge?\n2. How much variance do PC1 and PC2 capture? Is a 2D projection sufficient to understand the data?\n3. Look at the cluster-category crosstab — which cluster is the \"purest\" and which is the most mixed?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2.3: Vector Catalogue with Semantic Search\n\nThe code below builds a vector catalogue using SentenceTransformer embeddings stored in ChromaDB, then performs semantic searches with natural language queries. Unlike TF-IDF, these dense embeddings capture meaning — \"employee compensation\" and \"staff salary\" will have similar vectors even though they share no words."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_vector_catalogue(df):\n    \"\"\"Build a vector catalogue with ChromaDB.\"\"\"\n    # Load model\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n\n    # Encode descriptions\n    descriptions = df['description'].tolist()\n    embeddings = model.encode(descriptions)\n\n    # Create ChromaDB collection\n    client = chromadb.Client()\n    collection = client.create_collection(\"data_catalogue\")\n\n    # Add to collection\n    collection.add(\n        embeddings=embeddings.tolist(),\n        documents=descriptions,\n        ids=df['asset_id'].tolist(),\n        metadatas=[{'category': cat, 'sensitivity': sens}\n                   for cat, sens in zip(df['category'], df['sensitivity'])]\n    )\n\n    print(f\"Vector catalogue built with {collection.count()} assets\")\n    return collection, model\n\ncollection, model = build_vector_catalogue(df)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def semantic_search(collection, queries, n_results=5):\n    \"\"\"Perform semantic searches against the vector catalogue.\"\"\"\n    for query in queries:\n        results = collection.query(\n            query_texts=[query],\n            n_results=n_results\n        )\n        print(f\"\\nQuery: '{query}'\")\n        print(\"-\" * 60)\n        for i, (doc, dist, meta) in enumerate(zip(\n            results['documents'][0],\n            results['distances'][0],\n            results['metadatas'][0]\n        )):\n            print(f\"  {i+1}. [{meta['category']:12}] {doc[:70]}... (dist: {dist:.3f})\")\n\n# Test queries\ntest_queries = [\n    \"customer financial transactions\",\n    \"employee personal information\",\n    \"software development metrics\",\n]\n\nsemantic_search(collection, test_queries)"
  },
  {
   "cell_type": "markdown",
   "source": "### Analysis Questions\n\n1. For the query \"customer financial transactions\", why do certain results appear despite not containing those exact words?\n2. Compare the distance scores of the top 5 results — is there a clear drop-off between relevant and irrelevant results?\n3. How would semantic search help a data steward who doesn't know the exact terminology used in the catalogue?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Profile** synthetic data asset catalogues and identify quality issues\n",
    "2. **Classify** data assets using TF-IDF + RandomForest text classification\n",
    "3. **Extract** business metadata from descriptions using regex patterns\n",
    "4. **Cluster** data assets with KMeans to discover natural groupings\n",
    "5. **Build** a vector catalogue with semantic search using ChromaDB\n",
    "\n",
    "---\n",
    "\n",
    "*Data Discovery: Harnessing AI, AGI & Vector Databases | AI Elevate*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}