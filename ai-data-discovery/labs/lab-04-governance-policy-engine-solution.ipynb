{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Data Governance & Automated Policy Engine (Solutions)\n",
    "\n",
    "**Data Discovery: Harnessing AI, AGI & Vector Databases - Day 4**\n",
    "\n",
    "| Duration | Difficulty | Framework | Exercises |\n",
    "|---|---|---|---|\n",
    "| 90 min | Intermediate | pandas, numpy, matplotlib, networkx | 5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5vvwg4yiyn",
   "source": "## Student Notes & Background\n\n### Why Data Governance Matters\n\nAs organisations accumulate thousands of data assets across departments, the question shifts from *\"Can we find this data?\"* to *\"Should anyone be accessing this data, and is it compliant with our policies?\"* **Data governance** is the framework of policies, processes, and controls that ensures data is managed responsibly throughout its lifecycle.\n\nWithout automated governance, organisations face:\n- **Compliance violations** — PII stored without encryption, breaching GDPR, HIPAA, or CCPA\n- **Security risks** — unauthorised users accessing sensitive data\n- **Data decay** — stale, orphaned, or redundant data consuming resources\n- **Audit failures** — inability to demonstrate compliance to regulators\n\n### Key Concepts\n\n#### 1. Policy-as-Code\nTraditional governance relies on written policies that humans interpret and enforce manually. **Policy-as-code** encodes governance rules as executable functions that can be run automatically against every data asset in your catalogue. Each policy check takes an asset's metadata and returns a verdict: *compliant* or *violation* with a detail message.\n\nThe benefits are significant:\n- **Consistency** — every asset is evaluated by the same rules\n- **Speed** — hundreds of assets scanned in seconds rather than weeks of manual audit\n- **Auditability** — every check produces a traceable record\n- **Composability** — new policies can be added without modifying existing ones\n\n#### 2. Policy Severity Levels\nNot all violations are equal. A standard severity classification:\n\n| Severity | Description | Response Time |\n|---|---|---|\n| **Critical** | Immediate risk of data breach or regulatory penalty | Fix within 24 hours |\n| **High** | Significant compliance gap requiring prompt action | Fix within 1 week |\n| **Medium** | Best-practice deviation that should be addressed | Fix within 1 month |\n| **Low** | Minor improvement opportunity | Address in next review cycle |\n\n#### 3. Data Lineage\n**Data lineage** tracks how data flows through an organisation — from source systems (databases, APIs, files), through transformations (ETL jobs, ML pipelines, aggregations), to outputs (dashboards, reports, exports). Lineage is modelled as a **directed acyclic graph (DAG)** where:\n- **Nodes** represent data assets or processing steps\n- **Edges** represent data flow from upstream to downstream\n\nLineage analysis reveals:\n- **Bottleneck nodes** — transforms that many pipelines depend on (high failure impact)\n- **Long dependency chains** — complex pipelines that are harder to debug and audit\n- **Blast radius** — if a source system fails, which downstream outputs are affected?\n\n#### 4. Role-Based Access Control (RBAC)\n**RBAC** restricts data access based on a user's role rather than their individual identity. A typical enterprise model:\n\n| Role | Access Level | Typical Users |\n|---|---|---|\n| **Admin** | All sensitivity levels | Data platform team, CTO |\n| **Analyst** | Up to Confidential | Data analysts, business intelligence |\n| **Engineer** | Up to Internal | Software engineers, DevOps |\n| **Viewer** | Public only | General staff, external partners |\n\nA **violation** occurs when a user in a lower-privilege role accesses data above their clearance level. Monitoring access patterns helps detect both policy gaps and potential security incidents.\n\n#### 5. Compliance Dashboards\nA **compliance dashboard** provides at-a-glance visibility into your governance posture. Effective dashboards show:\n- Overall compliance score (percentage of clean assets)\n- Violations broken down by severity, category, and policy\n- Trends over time (is compliance improving or degrading?)\n- Sensitivity distribution of violating assets\n\n#### 6. Compliance Reports\nAutomated **compliance reports** translate raw violation data into actionable documents for different audiences:\n- **Executive summary** for leadership (score, headline numbers, rating)\n- **Critical findings** for the security team (specific assets and violations)\n- **Department breakdown** for data stewards (their team's compliance posture)\n- **Recommendations** for the governance committee (prioritised remediation actions)\n\n### What You'll Build\n\nIn this lab, you will:\n1. **Define** 6 governance policy check functions covering PII encryption, retention periods, stale data, owner assignment, access control, and backup compliance\n2. **Build** an automated scanner that runs all policies against 300 synthetic data assets and collects violations into a structured DataFrame\n3. **Model** data lineage as a NetworkX directed graph with source, transform, and output nodes, then analyse it for bottlenecks and long dependency chains\n4. **Simulate** 1,000 RBAC access requests across 4 roles and visualise violation rates as a heatmap\n5. **Create** a 6-panel governance dashboard with matplotlib showing severity breakdowns, compliance rates, and an overall score\n6. **Generate** a formatted compliance report with executive summary, critical findings, department breakdown, and actionable recommendations\n\n### Prerequisites\n- Familiarity with pandas DataFrames, numpy, and matplotlib\n- Understanding of set operations (for access control checks)\n- Concepts from Labs 1-2: data asset metadata, sensitivity levels, PII detection\n\n### Tips\n- Each policy function follows the same signature: `def check_policy(row) -> (bool, str)` — this makes them composable and testable\n- The synthetic data includes deliberate governance gaps (unencrypted PII, missing owners, stale assets) so you will find violations\n- For the lineage graph, `networkx` provides powerful analysis functions like `nx.dag_longest_path()`, `nx.descendants()`, and `nx.betweenness_centrality()`\n- When building the dashboard, use `plt.tight_layout()` to prevent label overlap in the 2×3 grid\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Synthetic Data Assets & Policies\n",
    "\n",
    "We create ~300 data assets with sensitivity, PII, encryption, access, and retention metadata,\n",
    "plus 8 governance policies to check automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "categories = ['HR', 'Finance', 'Marketing', 'Engineering', 'Legal']\n",
    "sensitivity_levels = ['Public', 'Internal', 'Confidential', 'Restricted']\n",
    "owners_pool = ['alice', 'bob', 'carol', 'dave', 'eve', 'frank',\n",
    "               'grace', 'heidi', 'ivan', 'judy', None]\n",
    "all_users = ['alice', 'bob', 'carol', 'dave', 'eve', 'frank',\n",
    "             'grace', 'heidi', 'ivan', 'judy', 'karl', 'liam',\n",
    "             'mona', 'nick', 'olivia', 'pat', 'quinn', 'ruth']\n",
    "\n",
    "asset_name_templates = {\n",
    "    'HR':          ['employee_records', 'payroll_data', 'benefits_enrollment',\n",
    "                    'performance_reviews', 'recruitment_pipeline', 'onboarding_docs',\n",
    "                    'termination_records', 'diversity_metrics', 'attendance_log'],\n",
    "    'Finance':     ['revenue_report', 'accounts_payable', 'budget_forecast',\n",
    "                    'billing_records', 'expense_claims', 'general_ledger',\n",
    "                    'tax_filings', 'cash_flow', 'audit_trail'],\n",
    "    'Marketing':   ['campaign_metrics', 'customer_segments', 'social_analytics',\n",
    "                    'email_subscribers', 'brand_sentiment', 'web_traffic',\n",
    "                    'lead_scores', 'content_calendar', 'event_registrations'],\n",
    "    'Engineering': ['server_logs', 'cicd_metrics', 'infra_monitoring',\n",
    "                    'api_usage_stats', 'db_schema_docs', 'commit_history',\n",
    "                    'load_test_results', 'vuln_scan_report', 'incident_logs'],\n",
    "    'Legal':       ['contract_repo', 'ip_filings', 'compliance_audits',\n",
    "                    'dpa_records', 'litigation_files', 'board_minutes',\n",
    "                    'privacy_assessments', 'nda_tracker', 'insurance_policies'],\n",
    "}\n",
    "\n",
    "description_pool = {\n",
    "    'HR':          'Employee and workforce management data',\n",
    "    'Finance':     'Financial records, transactions, and reporting data',\n",
    "    'Marketing':   'Campaign, customer, and market analytics data',\n",
    "    'Engineering': 'Infrastructure, code, and operational telemetry data',\n",
    "    'Legal':       'Contracts, compliance, and regulatory documentation',\n",
    "}\n",
    "\n",
    "n_assets = 300\n",
    "records = []\n",
    "for i in range(n_assets):\n",
    "    cat = np.random.choice(categories)\n",
    "    sens = np.random.choice(sensitivity_levels, p=[0.12, 0.33, 0.30, 0.25])\n",
    "    has_pii = bool(np.random.random() < (\n",
    "        0.80 if cat == 'HR' else 0.50 if cat == 'Finance' else\n",
    "        0.35 if cat == 'Marketing' else 0.10 if cat == 'Engineering' else 0.25))\n",
    "    has_encryption = bool(np.random.random() < (\n",
    "        0.95 if sens == 'Restricted' else 0.75 if sens == 'Confidential' else\n",
    "        0.30 if sens == 'Internal' else 0.10))\n",
    "    owner = np.random.choice(owners_pool, p=[0.12]*10 + [0.08])\n",
    "    n_approved = np.random.randint(2, 10)\n",
    "    approved = list(np.random.choice(all_users, size=n_approved, replace=False))\n",
    "    n_actual = np.random.randint(1, n_approved + 4)\n",
    "    actual = list(np.random.choice(all_users, size=n_actual, replace=False))\n",
    "    name_base = np.random.choice(asset_name_templates[cat])\n",
    "    records.append({\n",
    "        'asset_id': f'ASSET-{i+1:04d}', 'name': f'{name_base}_{i+1:04d}',\n",
    "        'description': description_pool[cat], 'category': cat, 'owner': owner,\n",
    "        'sensitivity': sens, 'has_pii': has_pii, 'has_encryption': has_encryption,\n",
    "        'last_access_days_ago': np.random.randint(0, 400),\n",
    "        'retention_days': np.random.choice([90, 180, 365, 730, 1095, 1825]),\n",
    "        'access_count_30d': np.random.randint(0, 200),\n",
    "        'approved_users': approved, 'actual_users': actual,\n",
    "        'has_backup': bool(np.random.random() < 0.70),\n",
    "        'created_days_ago': np.random.randint(30, 1200),\n",
    "    })\n",
    "assets_df = pd.DataFrame(records)\n",
    "\n",
    "policies = [\n",
    "    {'policy_id': 'POL-001', 'name': 'PII Encryption', 'description': 'Assets containing PII must be encrypted', 'check_function_name': 'check_pii_encryption', 'severity': 'Critical'},\n",
    "    {'policy_id': 'POL-002', 'name': 'Retention Compliance', 'description': 'Assets past retention must be flagged', 'check_function_name': 'check_retention_compliance', 'severity': 'High'},\n",
    "    {'policy_id': 'POL-003', 'name': 'Stale Data', 'description': 'Assets not accessed in 180+ days need review', 'check_function_name': 'check_stale_data', 'severity': 'Medium'},\n",
    "    {'policy_id': 'POL-004', 'name': 'Owner Assigned', 'description': 'Every asset must have an owner', 'check_function_name': 'check_owner_assigned', 'severity': 'High'},\n",
    "    {'policy_id': 'POL-005', 'name': 'Access Control', 'description': 'Only approved users should access assets', 'check_function_name': 'check_access_control', 'severity': 'Critical'},\n",
    "    {'policy_id': 'POL-006', 'name': 'Sensitivity Review', 'description': 'Restricted assets with >5 users need review', 'check_function_name': 'check_sensitivity_review', 'severity': 'High'},\n",
    "    {'policy_id': 'POL-007', 'name': 'Classification Required', 'description': 'All assets must have sensitivity set', 'check_function_name': 'check_classification', 'severity': 'Medium'},\n",
    "    {'policy_id': 'POL-008', 'name': 'Backup Compliance', 'description': 'Confidential+ assets need backups', 'check_function_name': 'check_backup_compliance', 'severity': 'High'},\n",
    "]\n",
    "policies_df = pd.DataFrame(policies)\n",
    "\n",
    "print(f\"Generated {len(assets_df)} data assets across {assets_df['category'].nunique()} categories\")\n",
    "print(f\"Defined  {len(policies_df)} governance policies\")\n",
    "print(f\"\\nAsset sensitivity distribution:\")\n",
    "print(assets_df['sensitivity'].value_counts())\n",
    "print(f\"\\nAsset category distribution:\")\n",
    "print(assets_df['category'].value_counts())\n",
    "print(f\"\\nSample asset:\")\n",
    "assets_df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Define Governance Policies\n",
    "\n",
    "Create a set of policy check functions. Each function receives a single asset row and returns\n",
    "a tuple `(bool, str)` where `True` means the asset **violates** the policy.\n",
    "\n",
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pii_encryption(asset):\n",
    "    \"\"\"POL-001: PII data must be encrypted.\"\"\"\n",
    "    if asset['has_pii'] and not asset['has_encryption']:\n",
    "        return True, f\"Asset contains PII but is NOT encrypted (sensitivity={asset['sensitivity']})\"\n",
    "    return False, \"\"\n",
    "\n",
    "\n",
    "def check_retention_compliance(asset):\n",
    "    \"\"\"POL-002: Assets older than their retention period must be flagged.\"\"\"\n",
    "    if asset['created_days_ago'] > asset['retention_days']:\n",
    "        over = asset['created_days_ago'] - asset['retention_days']\n",
    "        return True, f\"Asset is {over} days past its {asset['retention_days']}-day retention period\"\n",
    "    return False, \"\"\n",
    "\n",
    "\n",
    "def check_stale_data(asset):\n",
    "    \"\"\"POL-003: Assets not accessed in 180+ days should be reviewed.\"\"\"\n",
    "    if asset['last_access_days_ago'] >= 180:\n",
    "        return True, f\"Asset last accessed {asset['last_access_days_ago']} days ago (stale threshold: 180)\"\n",
    "    return False, \"\"\n",
    "\n",
    "\n",
    "def check_owner_assigned(asset):\n",
    "    \"\"\"POL-004: Every asset must have an assigned owner.\"\"\"\n",
    "    if asset['owner'] is None or (isinstance(asset['owner'], float) and np.isnan(asset['owner'])):\n",
    "        return True, \"Asset has no owner assigned\"\n",
    "    return False, \"\"\n",
    "\n",
    "\n",
    "def check_access_control(asset):\n",
    "    \"\"\"POL-005: Only approved users should access an asset.\"\"\"\n",
    "    approved = set(asset['approved_users'])\n",
    "    actual = set(asset['actual_users'])\n",
    "    unauthorized = actual - approved\n",
    "    if unauthorized:\n",
    "        return True, f\"{len(unauthorized)} unauthorized user(s): {', '.join(sorted(unauthorized))}\"\n",
    "    return False, \"\"\n",
    "\n",
    "\n",
    "def check_sensitivity_review(asset):\n",
    "    \"\"\"POL-006: Restricted assets accessed by >5 users need review.\"\"\"\n",
    "    if asset['sensitivity'] == 'Restricted' and len(asset['actual_users']) > 5:\n",
    "        return True, (f\"Restricted asset accessed by {len(asset['actual_users'])} users \"\n",
    "                      f\"(max recommended: 5)\")\n",
    "    return False, \"\"\n",
    "\n",
    "\n",
    "def check_classification(asset):\n",
    "    \"\"\"POL-007: All assets must have a sensitivity classification.\"\"\"\n",
    "    if not asset['sensitivity'] or asset['sensitivity'] == '':\n",
    "        return True, \"Asset has no sensitivity classification\"\n",
    "    return False, \"\"\n",
    "\n",
    "\n",
    "def check_backup_compliance(asset):\n",
    "    \"\"\"POL-008: Confidential and Restricted assets must have backups.\"\"\"\n",
    "    if asset['sensitivity'] in ('Confidential', 'Restricted') and not asset['has_backup']:\n",
    "        return True, f\"No backup for {asset['sensitivity']} asset\"\n",
    "    return False, \"\"\n",
    "\n",
    "\n",
    "POLICY_FUNCTIONS = {\n",
    "    'check_pii_encryption':      check_pii_encryption,\n",
    "    'check_retention_compliance': check_retention_compliance,\n",
    "    'check_stale_data':          check_stale_data,\n",
    "    'check_owner_assigned':      check_owner_assigned,\n",
    "    'check_access_control':      check_access_control,\n",
    "    'check_sensitivity_review':  check_sensitivity_review,\n",
    "    'check_classification':      check_classification,\n",
    "    'check_backup_compliance':   check_backup_compliance,\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(POLICY_FUNCTIONS)} policy check functions:\")\n",
    "for name, fn in POLICY_FUNCTIONS.items():\n",
    "    print(f\"  - {name:35s}  {fn.__doc__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2: Automated Policy Scanning\n",
    "\n",
    "Run every policy against every asset. Collect all violations into a DataFrame.\n",
    "\n",
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy_scan(assets_df, policies_df, policy_functions):\n",
    "    \"\"\"Scan all assets against all policies and collect violations.\"\"\"\n",
    "    violations = []\n",
    "    for _, policy in policies_df.iterrows():\n",
    "        fn = policy_functions[policy['check_function_name']]\n",
    "        for _, asset in assets_df.iterrows():\n",
    "            violated, detail = fn(asset)\n",
    "            if violated:\n",
    "                violations.append({\n",
    "                    'asset_id': asset['asset_id'], 'asset_name': asset['name'],\n",
    "                    'category': asset['category'], 'sensitivity': asset['sensitivity'],\n",
    "                    'policy_id': policy['policy_id'], 'policy_name': policy['name'],\n",
    "                    'severity': policy['severity'], 'detail': detail,\n",
    "                })\n",
    "    return pd.DataFrame(violations)\n",
    "\n",
    "violations_df = run_policy_scan(assets_df, policies_df, POLICY_FUNCTIONS)\n",
    "\n",
    "print(f\"Total violations found: {len(violations_df)}\")\n",
    "print(f\"Assets with at least one violation: {violations_df['asset_id'].nunique()} / {len(assets_df)}\")\n",
    "print(f\"\\nViolations by severity:\")\n",
    "print(violations_df['severity'].value_counts())\n",
    "print(f\"\\nViolations by policy:\")\n",
    "print(violations_df['policy_name'].value_counts())\n",
    "print(f\"\\nViolations by category:\")\n",
    "print(violations_df['category'].value_counts())\n",
    "print(f\"\\nSample violations:\")\n",
    "violations_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Data Lineage Tracking\n",
    "\n",
    "Build a directed graph with **NetworkX** to represent data lineage. Visualise the graph\n",
    "and identify the longest lineage paths and most-connected nodes.\n",
    "\n",
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "G = nx.DiGraph()\n",
    "\n",
    "source_nodes = ['CRM_Database', 'ERP_System', 'HR_Portal', 'Web_Analytics',\n",
    "    'IoT_Sensors', 'Email_Server', 'Payment_Gateway', 'Social_API',\n",
    "    'Survey_Platform', 'Legacy_Mainframe', 'Cloud_Storage', 'Partner_Feed']\n",
    "transform_nodes = ['ETL_Pipeline_A', 'ETL_Pipeline_B', 'Spark_Job_Clean',\n",
    "    'Spark_Job_Enrich', 'Python_Transform', 'dbt_Model_Stg',\n",
    "    'dbt_Model_Int', 'dbt_Model_Mart', 'ML_Feature_Eng',\n",
    "    'Anonymiser', 'Data_Quality_Check', 'Aggregator',\n",
    "    'Dedup_Service', 'Schema_Validator', 'Encryption_Layer']\n",
    "output_nodes = ['DW_Finance_Mart', 'DW_HR_Mart', 'DW_Marketing_Mart',\n",
    "    'DW_Engineering_Mart', 'DW_Legal_Mart', 'BI_Dashboard',\n",
    "    'ML_Training_Set', 'Customer_360', 'Compliance_Report',\n",
    "    'Executive_Summary', 'Data_Catalogue', 'Archive_S3']\n",
    "\n",
    "for s in source_nodes: G.add_node(s, node_type='source')\n",
    "for t in transform_nodes: G.add_node(t, node_type='transform')\n",
    "for o in output_nodes: G.add_node(o, node_type='output')\n",
    "\n",
    "for _ in range(50):\n",
    "    src = np.random.choice(source_nodes)\n",
    "    chain = list(np.random.choice(transform_nodes, size=np.random.randint(1,4), replace=False))\n",
    "    out = np.random.choice(output_nodes)\n",
    "    G.add_edge(src, chain[0])\n",
    "    for j in range(len(chain)-1): G.add_edge(chain[j], chain[j+1])\n",
    "    G.add_edge(chain[-1], out)\n",
    "\n",
    "print(f\"Lineage graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "\n",
    "color_map = {'source': '#3b82f6', 'transform': '#f59e0b', 'output': '#10b981'}\n",
    "node_colors = [color_map[G.nodes[n].get('node_type','transform')] for n in G.nodes]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "pos = nx.spring_layout(G, seed=42, k=1.8)\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=600, alpha=0.9, ax=ax)\n",
    "nx.draw_networkx_labels(G, pos, font_size=6, font_weight='bold', ax=ax)\n",
    "nx.draw_networkx_edges(G, pos, edge_color='#94a3b8', arrows=True, arrowsize=15, width=1.2, alpha=0.7, ax=ax)\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0],[0], marker='o', color='w', markerfacecolor='#3b82f6', markersize=12, label='Source'),\n",
    "    Line2D([0],[0], marker='o', color='w', markerfacecolor='#f59e0b', markersize=12, label='Transform'),\n",
    "    Line2D([0],[0], marker='o', color='w', markerfacecolor='#10b981', markersize=12, label='Output')]\n",
    "ax.legend(handles=legend_elements, loc='upper left', fontsize=11)\n",
    "ax.set_title('Data Lineage Graph', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"\\n--- Lineage Analysis ---\")\n",
    "try:\n",
    "    longest_path = nx.dag_longest_path(G)\n",
    "    print(f\"Longest lineage path ({len(longest_path)} nodes):\")\n",
    "    print(f\"  {' -> '.join(longest_path)}\")\n",
    "except nx.NetworkXUnfeasible:\n",
    "    print(\"Graph contains cycles. Finding longest simple path...\")\n",
    "    max_path = []\n",
    "    for s in source_nodes:\n",
    "        for o in output_nodes:\n",
    "            try:\n",
    "                for p in nx.all_simple_paths(G, s, o):\n",
    "                    if len(p) > len(max_path): max_path = p\n",
    "            except nx.NetworkXError: pass\n",
    "    if max_path: print(f\"Longest path ({len(max_path)} nodes): {' -> '.join(max_path)}\")\n",
    "\n",
    "degree_sorted = sorted(G.degree(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(f\"\\nTop 10 most-connected nodes:\")\n",
    "for node, deg in degree_sorted:\n",
    "    print(f\"  {node:30s}  type={G.nodes[node].get('node_type','?'):10s}  degree={deg}\")\n",
    "\n",
    "betweenness = nx.betweenness_centrality(G)\n",
    "top_b = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(f\"\\nTop 5 bottleneck nodes (betweenness centrality):\")\n",
    "for node, bc in top_b:\n",
    "    print(f\"  {node:30s}  type={G.nodes[node].get('node_type','?'):10s}  centrality={bc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Access Control Simulation\n",
    "\n",
    "Define an RBAC model with four roles. Simulate 2000 access requests, check against rules,\n",
    "and visualise denial rates as a heatmap.\n",
    "\n",
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(99)\n",
    "roles = {\n",
    "    'admin':    {'allowed': ['Public', 'Internal', 'Confidential', 'Restricted']},\n",
    "    'analyst':  {'allowed': ['Public', 'Internal', 'Confidential']},\n",
    "    'engineer': {'allowed': ['Public', 'Internal']},\n",
    "    'viewer':   {'allowed': ['Public']},\n",
    "}\n",
    "user_roles = {}\n",
    "for user in all_users:\n",
    "    user_roles[user] = np.random.choice(list(roles.keys()), p=[0.10, 0.30, 0.35, 0.25])\n",
    "\n",
    "print(\"User role assignments:\")\n",
    "for user, role in sorted(user_roles.items()):\n",
    "    print(f\"  {user:10s} -> {role}\")\n",
    "\n",
    "access_log = []\n",
    "for _ in range(2000):\n",
    "    user = np.random.choice(all_users)\n",
    "    asset = assets_df.iloc[np.random.randint(0, len(assets_df))]\n",
    "    role = user_roles[user]\n",
    "    granted = asset['sensitivity'] in roles[role]['allowed']\n",
    "    access_log.append({'user': user, 'role': role, 'asset_id': asset['asset_id'],\n",
    "        'sensitivity': asset['sensitivity'], 'category': asset['category'], 'granted': granted})\n",
    "\n",
    "access_df = pd.DataFrame(access_log)\n",
    "print(f\"\\nSimulated {len(access_df)} access requests\")\n",
    "print(f\"Granted: {access_df['granted'].sum()}  |  Denied: {(~access_df['granted']).sum()}\")\n",
    "print(f\"Overall denial rate: {(~access_df['granted']).mean():.1%}\")\n",
    "\n",
    "denial_matrix = access_df.groupby(['role','sensitivity'])['granted'].apply(\n",
    "    lambda x: (~x).mean()).unstack(fill_value=0)\n",
    "role_order = ['admin', 'analyst', 'engineer', 'viewer']\n",
    "sens_order = ['Public', 'Internal', 'Confidential', 'Restricted']\n",
    "denial_matrix = denial_matrix.reindex(index=role_order, columns=sens_order, fill_value=0)\n",
    "print(f\"\\nDenial rate matrix:\")\n",
    "print(denial_matrix.round(3))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "im = ax.imshow(denial_matrix.values, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(4)); ax.set_xticklabels(sens_order, fontsize=12)\n",
    "ax.set_yticks(range(4)); ax.set_yticklabels(role_order, fontsize=12)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        val = denial_matrix.values[i, j]\n",
    "        ax.text(j, i, f'{val:.0%}', ha='center', va='center',\n",
    "                fontsize=14, fontweight='bold', color='white' if val > 0.5 else 'black')\n",
    "plt.colorbar(im, ax=ax, label='Denial Rate')\n",
    "ax.set_title('Access Denial Rate: Role vs Sensitivity Level', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Asset Sensitivity', fontsize=12); ax.set_ylabel('User Role', fontsize=12)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"\\nPer-role access summary:\")\n",
    "print(access_df.groupby('role').agg(\n",
    "    total=('granted','count'), granted=('granted','sum'),\n",
    "    denied=('granted', lambda x: (~x).sum()),\n",
    "    denial_rate=('granted', lambda x: f\"{(~x).mean():.1%}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1: Governance Dashboard\n",
    "\n",
    "Build a 2x3 matplotlib dashboard with six visualisations.\n",
    "\n",
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violating_ids = violations_df['asset_id'].unique()\n",
    "n_compliant = len(assets_df) - len(violating_ids)\n",
    "compliance_pct = n_compliant / len(assets_df) * 100\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Violations by severity\n",
    "ax = axes[0, 0]\n",
    "sev_order = ['Critical', 'High', 'Medium', 'Low']\n",
    "sev_colors = {'Critical': '#ef4444', 'High': '#f59e0b', 'Medium': '#3b82f6', 'Low': '#10b981'}\n",
    "sev_counts = violations_df['severity'].value_counts().reindex(sev_order, fill_value=0)\n",
    "bars = ax.bar(sev_counts.index, sev_counts.values, color=[sev_colors[s] for s in sev_counts.index])\n",
    "for bar, val in zip(bars, sev_counts.values):\n",
    "    ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+1, str(val), ha='center', fontweight='bold')\n",
    "ax.set_title('Violations by Severity', fontsize=12, fontweight='bold'); ax.set_ylabel('Count')\n",
    "\n",
    "# 2. Violations by category (stacked)\n",
    "ax = axes[0, 1]\n",
    "cat_sev = violations_df.groupby(['category','severity']).size().unstack(fill_value=0).reindex(columns=sev_order, fill_value=0)\n",
    "cat_sev.plot(kind='bar', stacked=True, ax=ax, color=[sev_colors[s] for s in sev_order])\n",
    "ax.set_title('Violations by Category & Severity', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Count'); ax.legend(title='Severity', fontsize=8); ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Compliance trend\n",
    "ax = axes[0, 2]\n",
    "months = pd.date_range('2024-01-01', periods=12, freq='MS')\n",
    "np.random.seed(55)\n",
    "trend = [min(60 + i*2.5 + np.random.normal(0, 1.5), 98) for i in range(12)]\n",
    "ax.plot(months, trend, 'o-', color='#3b82f6', linewidth=2, markersize=6)\n",
    "ax.fill_between(months, trend, alpha=0.15, color='#3b82f6')\n",
    "ax.axhline(y=85, color='#ef4444', linestyle='--', alpha=0.7, label='Target (85%)')\n",
    "ax.set_title('Compliance Rate Trend (Simulated)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Compliance %'); ax.set_ylim(50, 100); ax.legend(fontsize=9)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Top violated policies\n",
    "ax = axes[1, 0]\n",
    "pc = violations_df['policy_name'].value_counts().head(10)\n",
    "psev = violations_df.drop_duplicates('policy_name').set_index('policy_name')['severity']\n",
    "bc = [sev_colors.get(psev.get(p, 'Medium'), '#3b82f6') for p in pc.index]\n",
    "ax.barh(pc.index[::-1], pc.values[::-1], color=bc[::-1])\n",
    "ax.set_title('Top Violated Policies', fontsize=12, fontweight='bold'); ax.set_xlabel('Violations')\n",
    "\n",
    "# 5. Sensitivity pie\n",
    "ax = axes[1, 1]\n",
    "sd = violations_df.drop_duplicates('asset_id')['sensitivity'].value_counts()\n",
    "scm = {'Public':'#10b981','Internal':'#3b82f6','Confidential':'#f59e0b','Restricted':'#ef4444'}\n",
    "ax.pie(sd.values, labels=sd.index, colors=[scm.get(s,'#94a3b8') for s in sd.index],\n",
    "       autopct='%1.1f%%', startangle=90, textprops={'fontsize': 10})\n",
    "ax.set_title('Sensitivity of Violating Assets', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 6. Gauge\n",
    "ax = axes[1, 2]\n",
    "theta = np.linspace(np.pi, 0, 100)\n",
    "ax.plot(np.cos(theta), np.sin(theta), color='#e2e8f0', linewidth=25, solid_capstyle='round')\n",
    "score = compliance_pct\n",
    "tf = np.linspace(np.pi, np.pi-(np.pi*score/100), max(int(score), 2))\n",
    "gc = '#10b981' if score >= 80 else '#f59e0b' if score >= 60 else '#ef4444'\n",
    "ax.plot(np.cos(tf), np.sin(tf), color=gc, linewidth=25, solid_capstyle='round')\n",
    "ax.text(0, 0.15, f'{score:.1f}%', ha='center', va='center', fontsize=36, fontweight='bold', color=gc)\n",
    "ax.text(0, -0.15, 'Overall Compliance', ha='center', va='center', fontsize=12, color='#64748b')\n",
    "ax.text(0, -0.35, f'{n_compliant} of {len(assets_df)} assets compliant', ha='center', va='center', fontsize=10, color='#94a3b8')\n",
    "ax.set_xlim(-1.3, 1.3); ax.set_ylim(-0.5, 1.3); ax.set_aspect('equal'); ax.axis('off')\n",
    "ax.set_title('Compliance Score', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Data Governance Dashboard', fontsize=18, fontweight='bold', y=1.01)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2: Compliance Report Generator\n",
    "\n",
    "Generate a text-based compliance report with executive summary, critical findings,\n",
    "recommendations, and per-department breakdown.\n",
    "\n",
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_compliance_report(assets_df, violations_df, policies_df):\n",
    "    \"\"\"Generate a formatted text compliance report.\"\"\"\n",
    "    total = len(assets_df)\n",
    "    vids = violations_df['asset_id'].unique()\n",
    "    n_v = len(vids); n_c = total - n_v\n",
    "    score = n_c / total * 100\n",
    "    sev_counts = violations_df['severity'].value_counts()\n",
    "\n",
    "    L = []\n",
    "    L.append('=' * 72)\n",
    "    L.append('        DATA GOVERNANCE COMPLIANCE REPORT')\n",
    "    L.append(f'        Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "    L.append('=' * 72); L.append('')\n",
    "\n",
    "    L.append('EXECUTIVE SUMMARY'); L.append('-' * 40)\n",
    "    L.append(f'  Overall Compliance Score:  {score:.1f}%')\n",
    "    L.append(f'  Total Assets Scanned:      {total}')\n",
    "    L.append(f'  Compliant Assets:          {n_c}')\n",
    "    L.append(f'  Non-Compliant Assets:      {n_v}')\n",
    "    L.append(f'  Total Violations Found:    {len(violations_df)}')\n",
    "    L.append(f'  Policies Evaluated:        {len(policies_df)}'); L.append('')\n",
    "\n",
    "    if score >= 90: rating = 'EXCELLENT - Governance posture is strong.'\n",
    "    elif score >= 75: rating = 'GOOD - Minor remediation needed.'\n",
    "    elif score >= 60: rating = 'FAIR - Significant gaps require attention.'\n",
    "    else: rating = 'POOR - Urgent remediation required.'\n",
    "    L.append(f'  Rating: {rating}'); L.append('')\n",
    "\n",
    "    L.append('VIOLATIONS BY SEVERITY'); L.append('-' * 40)\n",
    "    for sev in ['Critical', 'High', 'Medium', 'Low']:\n",
    "        cnt = sev_counts.get(sev, 0)\n",
    "        L.append(f'  {sev:10s}  {cnt:4d}  {\"#\" * (cnt // 3)}')\n",
    "    L.append('')\n",
    "\n",
    "    L.append('CRITICAL FINDINGS'); L.append('-' * 40)\n",
    "    crit = violations_df[violations_df['severity'] == 'Critical']\n",
    "    if len(crit) > 0:\n",
    "        for i, (pol, cnt) in enumerate(crit['policy_name'].value_counts().items(), 1):\n",
    "            L.append(f'  {i}. {pol}: {cnt} violations')\n",
    "            for _, r in crit[crit['policy_name']==pol].head(3).iterrows():\n",
    "                L.append(f'     - {r[\"asset_id\"]} ({r[\"category\"]}): {r[\"detail\"]}')\n",
    "    else:\n",
    "        L.append('  No critical violations found.')\n",
    "    L.append('')\n",
    "\n",
    "    L.append('RECOMMENDATIONS'); L.append('-' * 40)\n",
    "    recs = []\n",
    "    cc = sev_counts.get('Critical', 0); hc = sev_counts.get('High', 0)\n",
    "    if cc > 0:\n",
    "        recs.append(f'[URGENT] Address {cc} critical violations immediately, '\n",
    "                    f'focusing on PII encryption and access control gaps.')\n",
    "    if hc > 0:\n",
    "        recs.append(f'[HIGH] Remediate {hc} high-severity violations within 30 days, '\n",
    "                    f'including retention compliance and owner assignment.')\n",
    "    ov = violations_df[violations_df['policy_name']=='Owner Assigned']\n",
    "    if len(ov) > 0:\n",
    "        recs.append(f'[PROCESS] Assign owners to {len(ov)} orphaned assets. '\n",
    "                    f'Implement mandatory owner field in asset registration.')\n",
    "    sv = violations_df[violations_df['policy_name']=='Stale Data']\n",
    "    if len(sv) > 0:\n",
    "        recs.append(f'[HYGIENE] Review {len(sv)} stale assets for archival or deletion. '\n",
    "                    f'Consider automated lifecycle management.')\n",
    "    recs.append('[ONGOING] Schedule quarterly governance scans and track compliance trends.')\n",
    "    for i, rec in enumerate(recs, 1):\n",
    "        L.append(f'  {i}. {rec}')\n",
    "    L.append('')\n",
    "\n",
    "    L.append('PER-DEPARTMENT BREAKDOWN'); L.append('-' * 40)\n",
    "    for cat in sorted(assets_df['category'].unique()):\n",
    "        ca = assets_df[assets_df['category']==cat]\n",
    "        cv = violations_df[violations_df['category']==cat]\n",
    "        cv_n = cv['asset_id'].nunique(); ct = len(ca)\n",
    "        cc_ = ct - cv_n; cs = cc_/ct*100 if ct > 0 else 100\n",
    "        L.append(f'  {cat}')\n",
    "        L.append(f'    Assets:     {ct}')\n",
    "        L.append(f'    Compliant:  {cc_} ({cs:.1f}%)')\n",
    "        L.append(f'    Violations: {len(cv)}')\n",
    "        csev = cv['severity'].value_counts()\n",
    "        parts = [f'{s}: {csev.get(s,0)}' for s in ['Critical','High','Medium','Low'] if csev.get(s,0)>0]\n",
    "        if parts: L.append(f'    Breakdown:  {\", \".join(parts)}')\n",
    "        L.append('')\n",
    "\n",
    "    L.append('=' * 72)\n",
    "    L.append('  End of Report')\n",
    "    L.append('  Data Discovery: Harnessing AI, AGI & Vector Databases | AI Elevate')\n",
    "    L.append('=' * 72)\n",
    "    return '\\n'.join(L)\n",
    "\n",
    "report = generate_compliance_report(assets_df, violations_df, policies_df)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Define** governance policies as composable check functions that can be applied to any data asset\n",
    "2. **Automate** policy scanning across hundreds of assets to detect violations at scale\n",
    "3. **Model** data lineage as a directed graph using NetworkX, identifying bottlenecks and long dependency chains\n",
    "4. **Simulate** role-based access control and measure denial rates across roles and sensitivity levels\n",
    "5. **Visualise** governance posture through a multi-panel dashboard with compliance gauges, trend lines, and breakdowns\n",
    "6. **Generate** structured compliance reports with executive summaries, critical findings, and actionable recommendations\n",
    "\n",
    "---\n",
    "\n",
    "*Data Discovery: Harnessing AI, AGI & Vector Databases | AI Elevate*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}