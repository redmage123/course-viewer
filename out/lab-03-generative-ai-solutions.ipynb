{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 3: Generative AI with Ollama - SOLUTIONS\n",
    "\n",
    "**Duration:** 90-120 minutes | **Difficulty:** Intermediate to Advanced\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "1. Connect to and use Ollama for local LLM inference\n",
    "2. Generate text using the Llama model\n",
    "3. Apply prompt engineering techniques for better results\n",
    "4. Control generation with temperature and other parameters\n",
    "5. Build multi-turn conversations with chat history\n",
    "6. Implement Retrieval-Augmented Generation (RAG)\n",
    "7. Understand fine-tuning concepts with LoRA and QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# Check connection to Ollama\n",
    "try:\n",
    "    models = ollama.list()\n",
    "    print(\"Connected to Ollama!\")\n",
    "    print(\"\\nAvailable models:\")\n",
    "    for model in models.get('models', []):\n",
    "        print(f\"  - {model['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama: {e}\")\n",
    "    print(\"\\nMake sure Ollama is running: ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Basic Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text generation\n",
    "response = ollama.generate(\n",
    "    model='llama3.1:8b-instruct-q4_K_M',\n",
    "    prompt='What is machine learning in one sentence?'\n",
    ")\n",
    "print(\"Response:\")\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Generate Your First Response - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 1.1 Solution: Basic Text Generation\n\nThis solution demonstrates the simplest form of LLM interaction:\nsending a prompt and receiving a generated response.\n\"\"\"\n\n# Generate a response using ollama.generate()\n# model: specifies which LLM to use (must be installed via 'ollama pull')\n# prompt: the text input that the model will respond to\nmy_response = ollama.generate(\n    model='llama3.1:8b-instruct-q4_K_M',\n    prompt='What is an API?'\n)\n\n# Extract the generated text from the response dictionary\n# The response contains metadata (timing, tokens) and the actual text in 'response' key\nanswer = my_response['response']\n\nprint(\"Answer:\", answer)"
  },
  {
   "cell_type": "markdown",
   "id": "zfi07rw2w2",
   "source": "### Code Explanation: Exercise 1.1\n\n| Line | Code | Explanation |\n|------|------|-------------|\n| 1 | `ollama.generate(model='llama3.1:8b-instruct-q4_K_M', prompt='...')` | **Sends prompt to local LLM.** Returns a dictionary with the response and metadata. |\n| 2 | `my_response['response']` | **Extracts generated text.** The response dict also contains `model`, `total_duration`, `eval_count`, etc. |\n\n**Response Dictionary Structure:**\n```python\n{\n    'model': 'llama3.1:8b-instruct-q4_K_M',           # Model used\n    'response': 'An API is...',    # The generated text\n    'total_duration': 2500000000,  # Time in nanoseconds\n    'eval_count': 45,              # Tokens generated\n    'prompt_eval_count': 8         # Tokens in prompt\n}\n```\n\n**Why local LLMs matter:**\n- No API costs or rate limits\n- Data stays on your machine (privacy)\n- Works offline\n- Full control over model parameters",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Write a Role-Based Prompt - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 2.1 Solution: Role-Based Prompting\n\nAssigning a role to the LLM improves response quality and consistency.\nThe model adopts the persona's expertise and communication style.\n\"\"\"\n\n# Create a prompt that assigns a specific role/persona\n# The role provides context that shapes the response style and content\nchef_prompt = \"\"\"You are a professional chef who specializes in quick, easy meals \nthat anyone can make at home with common ingredients.\n\nSuggest a simple dinner recipe that can be made in under 30 minutes.\nInclude a list of ingredients and brief cooking instructions.\"\"\"\n\n# Generate response - the model will respond as a professional chef\nrecipe_response = ollama.generate(model='llama3.1:8b-instruct-q4_K_M', prompt=chef_prompt)\n\nif recipe_response:\n    print(recipe_response['response'])"
  },
  {
   "cell_type": "markdown",
   "id": "x5ye6e1l8f",
   "source": "### Code Explanation: Exercise 2.1\n\n| Component | Purpose |\n|-----------|---------|\n| `\"You are a professional chef...\"` | **Sets the persona.** The model adopts this role's expertise, vocabulary, and perspective. |\n| `\"who specializes in...\"` | **Narrows the expertise.** Focuses responses on quick, accessible cooking. |\n| `\"Suggest a simple dinner...\"` | **The actual task.** Clear instruction for what output is needed. |\n| `\"Include a list...\"` | **Output structure.** Specifies what format the response should take. |\n\n**Anatomy of a Good Role Prompt:**\n```\n1. Role Assignment:  \"You are a [profession/expert]...\"\n2. Specialization:   \"...who specializes in [specific area]\"\n3. Task:             \"Please [specific action]\"\n4. Format:           \"Include/Format as [structure]\"\n```\n\n**Why role-based prompting works:**\n- LLMs have learned from text written by various professionals\n- Assigning a role activates relevant knowledge patterns\n- Responses become more focused and authoritative\n- Communication style matches the role's typical tone",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Generate JSON Output - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 2.2 Solution: Structured JSON Output\n\nThis solution demonstrates how to get parseable, structured data from an LLM.\nKey techniques: explicit format specification, \"ONLY\" instruction, try/except parsing.\n\"\"\"\n\n# Craft a prompt that forces structured JSON output\n# Key elements:\n# 1. Clear task description\n# 2. Exact JSON format with field names and types\n# 3. Explicit instruction to output ONLY JSON\nmovie_prompt = \"\"\"Generate information about a famous movie.\n\nRespond with ONLY valid JSON in this exact format:\n{\"title\": \"...\", \"director\": \"...\", \"year\": YYYY, \"rating\": X.X}\n\nThe rating should be out of 10. Do not include any other text, just the JSON.\"\"\"\n\n# Send to LLM\nmovie_response = ollama.generate(model='llama3.1:8b-instruct-q4_K_M', prompt=movie_prompt)\n\n# Parse JSON response with error handling\n# LLMs can sometimes add extra text, so we strip whitespace\n# try/except catches malformed JSON gracefully\ntry:\n    movie_data = json.loads(movie_response['response'].strip())\nexcept:\n    movie_data = None\n\n# Display parsed data if successful\nif movie_data:\n    print(\"Movie data:\")\n    for key, value in movie_data.items():\n        print(f\"  {key}: {value}\")"
  },
  {
   "cell_type": "markdown",
   "id": "qs04bb96s7q",
   "source": "### Code Explanation: Exercise 2.2\n\n| Line | Code | Explanation |\n|------|------|-------------|\n| 1 | `\"\"\"Respond with ONLY valid JSON...\"\"\"` | **Format enforcement.** \"ONLY\" reduces extra text. Showing exact format guides structure. |\n| 2 | `{\"title\": \"...\", \"director\": \"...\"}` | **Template example.** LLMs follow patterns - provide the exact structure you want. |\n| 3 | `json.loads(response.strip())` | **Parse JSON.** `.strip()` removes leading/trailing whitespace that would break parsing. |\n| 4 | `try: ... except: movie_data = None` | **Graceful failure.** LLMs can produce malformed JSON; handle it without crashing. |\n\n**Prompt Engineering for JSON:**\n```\n❌ Bad:  \"Tell me about a movie\"\n✓ Good: \"Respond with ONLY valid JSON in this exact format: {...}\"\n```\n\n**Common JSON Pitfalls:**\n1. LLM adds \"Here's the JSON:\" before the actual JSON\n2. Markdown code fences: ` ```json ... ``` `\n3. Trailing commas in arrays/objects\n4. Single quotes instead of double quotes\n\n**Solutions:**\n- Use \"ONLY\" and \"Do not include any other text\"\n- Use `.strip()` to remove whitespace\n- Consider regex extraction as fallback\n- Use low temperature for more reliable formatting",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Generation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Exercise 3.1: Experiment with Temperature - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 3.1 Solution: Temperature Parameter Experimentation\n\nTemperature controls the randomness/creativity of LLM outputs.\n- Low temp (0.0-0.3): Deterministic, focused, consistent\n- High temp (0.7-1.0): Creative, varied, unpredictable\n\"\"\"\n\n# Task that benefits from creative responses\ncreative_prompt = \"Suggest a creative and unique name for a coffee shop.\"\n\n# LOW TEMPERATURE (0.2): Deterministic, predictable output\n# The model chooses high-probability tokens more often\n# Good for: factual answers, code, structured data\nlow_temp_response = ollama.generate(\n    model='llama3.1:8b-instruct-q4_K_M',\n    prompt=creative_prompt,\n    options={'temperature': 0.2}  # Conservative, focused\n)\n\n# HIGH TEMPERATURE (0.9): Creative, diverse output\n# The model samples from a wider distribution of tokens\n# Good for: creative writing, brainstorming, variety\nhigh_temp_response = ollama.generate(\n    model='llama3.1:8b-instruct-q4_K_M',\n    prompt=creative_prompt,\n    options={'temperature': 0.9}  # Creative, varied\n)\n\n# Compare outputs - run multiple times to see temperature effect\nprint(\"Low temp (0.2):\")\nprint(low_temp_response['response'])\nprint(\"\\nHigh temp (0.9):\")\nprint(high_temp_response['response'])"
  },
  {
   "cell_type": "markdown",
   "id": "yup8vhtddo",
   "source": "### Code Explanation: Exercise 3.1\n\n| Parameter | Value | Effect |\n|-----------|-------|--------|\n| `temperature: 0.2` | Low | Model picks most likely tokens. Consistent, focused output. |\n| `temperature: 0.9` | High | Model samples broadly. Creative, varied, surprising output. |\n\n**How Temperature Works (Softmax Scaling):**\n```\nLow temp:  [0.9, 0.05, 0.05]  → Almost always picks first token\nHigh temp: [0.4, 0.35, 0.25] → Distributes probability more evenly\n```\n\n**Temperature Guidelines:**\n\n| Use Case | Recommended Temp |\n|----------|------------------|\n| Code generation | 0.0 - 0.2 |\n| Factual Q&A | 0.1 - 0.3 |\n| JSON/structured output | 0.1 - 0.2 |\n| General assistant | 0.5 - 0.7 |\n| Creative writing | 0.7 - 0.9 |\n| Brainstorming | 0.8 - 1.0 |\n\n**Other Generation Parameters:**\n- `top_p` (nucleus sampling): Limits token pool by cumulative probability\n- `top_k`: Limits to top K most likely tokens\n- `repeat_penalty`: Reduces repetition in output\n\n**Pro Tip:** Run the same prompt multiple times with high temperature to see variation, and with low temperature to see consistency.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Chat Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Create a Multi-Turn Conversation - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 4.1 Solution: Multi-Turn Conversation with Memory\n\nThis demonstrates how to maintain conversation context across multiple exchanges.\nThe chat API tracks messages, allowing the model to reference earlier information.\n\"\"\"\n\n# Initialize conversation with first user message\n# The messages list acts as the conversation memory\nmessages = [\n    {'role': 'user', 'content': 'My favorite color is blue.'}\n]\n\n# TURN 1: Send initial message\n# ollama.chat() differs from generate() - it takes structured messages\nresponse1 = ollama.chat(model='llama3.1:8b-instruct-q4_K_M', messages=messages)\n\nif response1:\n    print(\"Assistant:\", response1['message']['content'])\n    \n    # CRITICAL: Add assistant's response to message history\n    # This is what gives the model \"memory\" of the conversation\n    messages.append(response1['message'])\n    \n    # Add follow-up question that requires memory of earlier info\n    messages.append({'role': 'user', 'content': 'What is my favorite color?'})\n    \n    # TURN 2: Model should remember the color from Turn 1\n    response2 = ollama.chat(model='llama3.1:8b-instruct-q4_K_M', messages=messages)\n    \n    if response2:\n        print(\"\\nAssistant:\", response2['message']['content'])"
  },
  {
   "cell_type": "markdown",
   "id": "m9hzwcntl0s",
   "source": "### Code Explanation: Exercise 4.1\n\n| Step | Code | Purpose |\n|------|------|---------|\n| 1 | `messages = [{'role': 'user', 'content': '...'}]` | Initialize conversation history list |\n| 2 | `ollama.chat(model='...', messages=messages)` | Send entire history to model |\n| 3 | `messages.append(response1['message'])` | **Critical:** Add assistant reply to history |\n| 4 | `messages.append({'role': 'user', '...'})` | Add next user message |\n| 5 | `ollama.chat(...)` again | Model sees full conversation, has \"memory\" |\n\n**Message Structure:**\n```python\nmessages = [\n    {'role': 'user', 'content': 'My favorite color is blue.'},\n    {'role': 'assistant', 'content': 'That\\'s nice! Blue is...'},\n    {'role': 'user', 'content': 'What is my favorite color?'},\n    # Model receives ALL messages and can reference earlier context\n]\n```\n\n**Roles Explained:**\n- `user`: Human messages/questions\n- `assistant`: Model's previous responses\n- `system`: Instructions that shape behavior (used in next exercise)\n\n**Why append both user AND assistant messages?**\nWithout the assistant's response in history, the model doesn't know what it said. It would lose context and might contradict itself.\n\n**generate() vs chat():**\n| `ollama.generate()` | `ollama.chat()` |\n|---------------------|-----------------|\n| Single prompt in, text out | Message list in, message out |\n| No conversation structure | Supports roles and turns |\n| Good for one-shot tasks | Good for multi-turn dialogue |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Create a Specialized Chatbot - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 4.2 Solution: Specialized Chatbot with System Prompt\n\nSystem prompts define the chatbot's personality, expertise, and behavior rules.\nThey're sent at the start of every conversation but aren't visible to users.\n\"\"\"\n\n# SYSTEM PROMPT: Defines the chatbot's persona and behavior\n# This shapes ALL responses in the conversation\n# Key elements:\n# 1. Role definition (\"You are a...\")\n# 2. Expertise area (\"Python programming tutor\")\n# 3. Behavior rules (\"explain clearly\", \"provide examples\")\n# 4. Tone/personality (\"helpful\", \"patient\", \"encouraging\")\nsystem_prompt = \"\"\"You are a helpful and patient Python programming tutor. \nYou explain concepts clearly using simple language and always provide code examples.\nWhen answering questions, you break down complex topics into easy-to-understand steps.\nYou encourage learning and provide helpful tips.\"\"\"\n\n# Initialize messages with system prompt FIRST\n# System message sets context before any user interaction\nmessages = [\n    {'role': 'system', 'content': system_prompt},  # Always first!\n    {'role': 'user', 'content': 'What is a list comprehension in Python?'}\n]\n\n# Get response - model will behave as defined in system prompt\ntutor_response = ollama.chat(model='llama3.1:8b-instruct-q4_K_M', messages=messages)\n\nif tutor_response:\n    print(\"Python Tutor:\")\n    print(tutor_response['message']['content'])"
  },
  {
   "cell_type": "markdown",
   "id": "l3d90r89l",
   "source": "### Code Explanation: Exercise 4.2\n\n| Component | Purpose |\n|-----------|---------|\n| `'role': 'system'` | Special role that defines chatbot behavior (invisible to users) |\n| `\"You are a...tutor\"` | Sets expertise and identity |\n| `\"explain clearly\"` | Defines communication style |\n| `\"provide code examples\"` | Specifies required output format |\n| `\"patient\", \"encouraging\"` | Sets emotional tone |\n\n**System Prompt Anatomy:**\n```python\nsystem_prompt = \"\"\"\nYou are a [ROLE/IDENTITY].           # Who the bot is\nYou specialize in [EXPERTISE].       # What it knows\nYou always [BEHAVIOR RULES].         # How it acts\nYour tone is [PERSONALITY].          # How it sounds\nYou never [RESTRICTIONS].            # What it won't do\n\"\"\"\n```\n\n**System vs User Messages:**\n| System Message | User Message |\n|----------------|--------------|\n| Sets behavior/rules | Actual questions/requests |\n| Sent once at start | Sent each turn |\n| Not shown in UI | Visible in chat |\n| Persistent context | Turn-by-turn content |\n\n**Best Practices for System Prompts:**\n1. Be specific about expertise (\"Python tutor\" not just \"tutor\")\n2. Define output format (\"always provide code examples\")\n3. Set guardrails (\"never write malicious code\")\n4. Include personality traits for consistent tone\n5. Keep it focused - one role per chatbot",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Building a Simple Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Exercise 5.1: Build a Sentiment Analyzer - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 5.1 Solution: Sentiment Analysis Application\n\nThis function wraps an LLM to perform sentiment analysis on text.\nIt demonstrates how to build reusable AI-powered utilities.\n\"\"\"\n\ndef analyze_sentiment(text):\n    \"\"\"\n    Analyze the sentiment of input text using an LLM.\n    \n    Args:\n        text: The text to analyze\n        \n    Returns:\n        dict with 'sentiment' (positive/negative/neutral) and 'confidence' (high/medium/low)\n        or None if parsing fails\n    \"\"\"\n    # Construct a prompt that:\n    # 1. Clearly states the task\n    # 2. Provides the input text\n    # 3. Specifies exact JSON output format\n    # 4. Instructs to only output JSON (no extra text)\n    prompt = f\"\"\"Analyze the sentiment of the following text.\n\nText: {text}\n\nRespond with ONLY valid JSON in this exact format:\n{{\"sentiment\": \"positive/negative/neutral\", \"confidence\": \"high/medium/low\"}}\n\nDo not include any other text.\"\"\"\n    \n    # Use low temperature for consistent, deterministic output\n    response = ollama.generate(\n        model='llama3.1:8b-instruct-q4_K_M',\n        prompt=prompt,\n        options={'temperature': 0.1}  # Low temp = more predictable\n    )\n    \n    # Parse JSON response, handle potential errors\n    try:\n        return json.loads(response['response'].strip())\n    except:\n        return None\n\n# Test with examples covering all sentiment types\ntest_texts = [\n    \"I absolutely love this product! Best purchase ever!\",  # Positive\n    \"This is the worst experience I've ever had.\",          # Negative\n    \"The weather today is cloudy.\"                          # Neutral\n]\n\nfor text in test_texts:\n    result = analyze_sentiment(text)\n    if result:\n        print(f\"Text: {text[:50]}...\")\n        print(f\"  Sentiment: {result.get('sentiment')}\")\n        print(f\"  Confidence: {result.get('confidence')}\")\n        print()"
  },
  {
   "cell_type": "markdown",
   "id": "ajfb0xmiy7f",
   "source": "### Code Explanation: Exercise 5.1\n\n| Component | Code | Purpose |\n|-----------|------|---------|\n| Function wrapper | `def analyze_sentiment(text):` | Creates reusable utility for sentiment analysis |\n| Prompt template | `f\"\"\"Analyze...Text: {text}...\"\"\"` | Injects input text into structured prompt |\n| JSON format spec | `{{\"sentiment\": \"...\", \"confidence\": \"...\"}}` | Ensures parseable output format |\n| Low temperature | `options={'temperature': 0.1}` | Reduces randomness for consistent output |\n| Error handling | `try: json.loads(...) except: None` | Gracefully handles malformed responses |\n\n**Pattern: LLM as a Function:**\n```python\ndef llm_utility(input_data):\n    prompt = f\"[Task description]\\n\\nInput: {input_data}\\n\\nOutput format: [JSON spec]\"\n    response = ollama.generate(model='...', prompt=prompt, options={'temperature': 0.1})\n    return json.loads(response['response'])\n```\n\n**Best Practices for Structured Output:**\n1. Specify exact JSON format with example\n2. Say \"ONLY valid JSON\" to prevent extra text\n3. Use low temperature (0.1-0.3) for consistency\n4. Always wrap `json.loads()` in try/except\n5. Use `.strip()` to remove whitespace",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Exercise 5.2: Build a Q&A Bot - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 5.2 Solution: Context-Grounded Q&A Bot\n\nThis function answers questions using ONLY provided context (not model knowledge).\nThis is the foundation of RAG - ensuring answers are grounded in your data.\n\"\"\"\n\ndef answer_question(context, question):\n    \"\"\"\n    Answer a question based strictly on provided context.\n    \n    Args:\n        context: Text containing information to answer from\n        question: The question to answer\n        \n    Returns:\n        str: The answer, or acknowledgment if not found in context\n    \"\"\"\n    # Prompt structure for grounded Q&A:\n    # 1. Explicit instruction to use ONLY context\n    # 2. Fallback behavior when answer isn't present\n    # 3. Context block clearly labeled\n    # 4. Question clearly separated\n    prompt = f\"\"\"Answer the question based ONLY on the provided context.\nIf the answer is not in the context, say \"I don't know based on the provided context.\"\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n    \n    # Low temperature for factual, consistent answers\n    response = ollama.generate(\n        model='llama3.1:8b-instruct-q4_K_M',\n        prompt=prompt,\n        options={'temperature': 0.2}\n    )\n    \n    return response['response'].strip()\n\n# Test context - structured information about Python\ncontext = \"\"\"\nPython was created by Guido van Rossum and first released in 1991. \nIt emphasizes code readability and uses significant indentation. \nPython supports multiple programming paradigms including procedural, \nobject-oriented, and functional programming. The language is named \nafter the British comedy group Monty Python.\n\"\"\"\n\n# Questions - some answerable from context, one not\nquestions = [\n    \"Who created Python?\",           # In context\n    \"When was Python first released?\", # In context  \n    \"What is Python's mascot?\"       # NOT in context - tests fallback\n]\n\nfor q in questions:\n    answer = answer_question(context, q)\n    print(f\"Q: {q}\")\n    print(f\"A: {answer}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "id": "ih975zbl03i",
   "source": "### Code Explanation: Exercise 5.2\n\n| Component | Code | Purpose |\n|-----------|------|---------|\n| Grounding instruction | `\"based ONLY on the provided context\"` | Prevents hallucination from model knowledge |\n| Fallback instruction | `\"If the answer is not in the context...\"` | Defines behavior when info is missing |\n| Context injection | `f\"\"\"...Context:\\n{context}...\"\"\"` | Inserts your data into the prompt |\n| Low temperature | `options={'temperature': 0.2}` | Ensures factual, consistent answers |\n\n**Why \"ONLY on the provided context\" matters:**\nWithout this constraint, LLMs will answer from their training data, even if wrong or outdated. For example, asking \"What is Python?\" without context might get general info instead of your specific documentation.\n\n**Q&A Prompt Pattern:**\n```\n[Instruction: Answer based ONLY on context]\n[Fallback: What to say if answer not found]\n\nContext:\n[Your data here]\n\nQuestion: [User's question]\n\nAnswer:\n```\n\n**Testing Grounding:**\nThe third question (\"What is Python's mascot?\") isn't in the context. A well-grounded model should respond \"I don't know\" rather than saying \"snake\" from general knowledge.\n\n**This is the core of RAG:**\nInstead of relying on model training data:\n1. Retrieve relevant documents (next section)\n2. Inject as context\n3. Ground answer in that context",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for RAG\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = ollama.embed(model='llama3.1:8b-instruct-q4_K_M', input=text)\n",
    "    return response['embeddings'][0]\n",
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self, model='llama3.1:8b-instruct-q4_K_M'):\n",
    "        self.model = model\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def add_documents(self, docs: List[str]):\n",
    "        for doc in docs:\n",
    "            embedding = get_embedding(doc)\n",
    "            self.documents.append(doc)\n",
    "            self.embeddings.append(embedding)\n",
    "        print(f\"Added {len(docs)} documents. Total: {len(self.documents)}\")\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 2) -> List[str]:\n",
    "        query_embedding = get_embedding(query)\n",
    "        similarities = []\n",
    "        for i, doc_embedding in enumerate(self.embeddings):\n",
    "            sim = cosine_similarity(query_embedding, doc_embedding)\n",
    "            similarities.append((sim, i))\n",
    "        similarities.sort(reverse=True)\n",
    "        top_indices = [idx for _, idx in similarities[:top_k]]\n",
    "        return [self.documents[i] for i in top_indices]\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 2) -> str:\n",
    "        relevant_docs = self.retrieve(question, top_k)\n",
    "        context = \"\\n\\n\".join(relevant_docs)\n",
    "        prompt = f\"\"\"Use the following context to answer the question. \n",
    "If the answer is not in the context, say \"I don't have information about that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        response = ollama.generate(model=self.model, prompt=prompt, options={'temperature': 0.3})\n",
    "        return response['response'].strip()\n",
    "\n",
    "# Create and populate RAG system\n",
    "rag = SimpleRAG()\n",
    "knowledge_base = [\n",
    "    \"The Eiffel Tower is located in Paris, France. It was built in 1889 and stands 330 meters tall.\",\n",
    "    \"The Great Wall of China is over 21,000 kilometers long and was built over many centuries.\",\n",
    "    \"Python programming language was created by Guido van Rossum and released in 1991.\",\n",
    "    \"Machine learning is a subset of AI that enables computers to learn from data.\",\n",
    "    \"The Amazon rainforest produces about 20% of the world's oxygen.\"\n",
    "]\n",
    "rag.add_documents(knowledge_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Exercise 6.1: Extend the RAG Knowledge Base - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 6.1 Solution: Extending RAG Knowledge Base\n\nThis demonstrates adding domain-specific documents to an existing RAG system.\nEach document is embedded and added to the vector store for semantic search.\n\"\"\"\n\n# Define new domain-specific documents (sports knowledge)\n# Each document should be a self-contained fact or concept\n# Keep documents focused - one main idea per document works best\nmy_documents = [\n    \"The Olympic Games originated in ancient Greece around 776 BC.\",\n    \"Basketball was invented by James Naismith in 1891 in Springfield, Massachusetts.\",\n    \"The FIFA World Cup is held every four years and is the most watched sporting event.\",\n    \"Tennis uses a scoring system of 15, 30, 40, and game points.\",\n    \"The marathon race is 26.2 miles long, commemorating the legend of Pheidippides.\"\n]\n\n# Add documents to the RAG system\n# Internally this:\n# 1. Generates embeddings for each document (vector representation)\n# 2. Stores documents alongside their embeddings\n# 3. Enables semantic search across all documents\nrag.add_documents(my_documents)\n\n# Test retrieval with questions\n# The RAG system will:\n# 1. Embed the question\n# 2. Find most similar documents (by cosine similarity)\n# 3. Generate answer grounded in retrieved context\nmy_questions = [\n    \"Who invented basketball?\",      # Should find Naismith document\n    \"How long is a marathon?\",       # Should find marathon document\n    \"When did the Olympic Games start?\"  # Should find Olympics document\n]\n\nfor q in my_questions:\n    print(f\"Q: {q}\")\n    answer = rag.query(q)\n    print(f\"A: {answer}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "id": "j52scmji58b",
   "source": "### Code Explanation: Exercise 6.1\n\n| Step | What Happens | Why It Matters |\n|------|--------------|----------------|\n| `rag.add_documents(my_documents)` | Each doc gets embedded (converted to vector) | Vectors enable semantic similarity search |\n| Internal: `get_embedding(doc)` | LLM creates ~4096-dimensional vector | Similar meanings → similar vectors |\n| `rag.query(question)` | Question embedded, similar docs retrieved | Finds relevant info without keyword matching |\n| `ollama.generate(context + question)` | LLM answers using retrieved docs | Grounds response in your specific data |\n\n**RAG Pipeline Flow:**\n```\n1. Document Ingestion:\n   \"Basketball was invented...\" → [0.12, -0.34, 0.78, ...] → Store\n\n2. Query Time:\n   \"Who invented basketball?\" \n   → [0.11, -0.32, 0.76, ...]  (similar vector!)\n   → Find nearest documents\n   → Generate answer from retrieved context\n```\n\n**Document Design Best Practices:**\n- One fact/concept per document for precise retrieval\n- Include key terms naturally (helps embedding quality)\n- Keep documents similar length (~1-3 sentences)\n- Avoid pronouns without antecedents (\"It was invented...\" - what's \"it\"?)\n\n**Why Semantic Search > Keyword Search:**\n| Query | Keyword Search | Semantic Search |\n|-------|----------------|-----------------|\n| \"hoops game origin\" | ❌ No match for \"basketball\" | ✓ Finds basketball doc |\n| \"foot race distance\" | ❌ No match for \"marathon\" | ✓ Finds marathon doc |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Exercise 6.2: Implement Document Chunking - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 6.2 Solution: Document Chunking for RAG\n\nLong documents must be split into chunks for effective RAG retrieval.\nOverlapping chunks ensure important information at boundaries isn't lost.\n\"\"\"\n\ndef chunk_text(text: str, chunk_size: int = 100, overlap: int = 20) -> List[str]:\n    \"\"\"\n    Split text into overlapping chunks for RAG ingestion.\n    \n    Args:\n        text: The long document to split\n        chunk_size: Number of words per chunk\n        overlap: Number of words to overlap between consecutive chunks\n        \n    Returns:\n        List of text chunks with specified overlap\n    \"\"\"\n    # Split text into words\n    words = text.split()\n    chunks = []\n    \n    # Handle short texts that don't need chunking\n    if len(words) <= chunk_size:\n        return [text.strip()]\n    \n    # Sliding window approach with overlap\n    start = 0\n    while start < len(words):\n        # Get chunk_size words starting from current position\n        end = min(start + chunk_size, len(words))\n        chunk = ' '.join(words[start:end])\n        chunks.append(chunk.strip())\n        \n        # Stop if we've reached the end\n        if end >= len(words):\n            break\n        \n        # Move start forward, but keep 'overlap' words for context\n        # This creates overlapping windows to preserve boundary context\n        start = end - overlap\n    \n    return chunks\n\n# Test with a longer document\nlong_document = \"\"\"\nArtificial intelligence has transformed the technology landscape dramatically over the past decade. \nMachine learning algorithms now power everything from recommendation systems to autonomous vehicles.\nDeep learning, a subset of machine learning, uses neural networks with many layers to learn complex patterns.\nNatural language processing enables computers to understand and generate human language.\nComputer vision allows machines to interpret and analyze visual information from the world.\nReinforcement learning teaches agents to make decisions through trial and error.\nThe field continues to advance rapidly, with new breakthroughs announced regularly.\nEthical considerations around AI bias and fairness have become increasingly important.\nResearchers are working on making AI systems more transparent and explainable.\nThe future of AI holds both tremendous promise and significant challenges for society.\n\"\"\"\n\n# Create chunks with 50 words each, 10 word overlap\nchunks = chunk_text(long_document, chunk_size=50, overlap=10)\nprint(f\"Created {len(chunks)} chunks:\\n\")\nfor i, chunk in enumerate(chunks):\n    print(f\"Chunk {i+1}: {chunk[:80]}...\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "id": "3rd6s8han5s",
   "source": "### Code Explanation: Exercise 6.2\n\n| Line | Code | Purpose |\n|------|------|---------|\n| `words = text.split()` | Tokenize by whitespace | Simple word-based splitting |\n| `if len(words) <= chunk_size:` | Short-circuit | Don't chunk if already small enough |\n| `end = min(start + chunk_size, len(words))` | Boundary check | Prevent index out of bounds |\n| `start = end - overlap` | Sliding window | Creates overlapping chunks |\n\n**Why Overlap Matters:**\n```\nWithout overlap:\n[Chunk 1: \"...uses neural\"] [Chunk 2: \"networks with many...\"]\n→ Sentence split! \"Neural networks\" context lost\n\nWith overlap:\n[Chunk 1: \"...uses neural networks\"] [Chunk 2: \"neural networks with many...\"]\n→ Important phrase preserved in both chunks\n```\n\n**Chunking Strategy Visualization:**\n```\nDocument: |-----100 words-----|-----100 words-----|-----100 words-----|\nChunk 1:  |===================|\nChunk 2:                  |===================|   (20 word overlap)\nChunk 3:                                    |===================|\n```\n\n**Choosing Chunk Parameters:**\n\n| Parameter | Small Value | Large Value |\n|-----------|-------------|-------------|\n| `chunk_size` | More precise retrieval | More context per chunk |\n| `overlap` | Less redundancy | Better boundary preservation |\n\n**Common Settings:**\n- `chunk_size=512`, `overlap=50` for long documents\n- `chunk_size=100`, `overlap=20` for precise retrieval\n- Consider semantic chunking (by paragraph/section) for structured docs",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Fine-tuning Concepts with LoRA and QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data formats demonstration\n",
    "instruction_format = {\n",
    "    \"instruction\": \"Summarize the following text in one sentence.\",\n",
    "    \"input\": \"The quick brown fox jumps over the lazy dog. This sentence contains every letter.\",\n",
    "    \"output\": \"A pangram sentence featuring a fox demonstrates all 26 alphabet letters.\"\n",
    "}\n",
    "\n",
    "chat_format = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How do I reverse a string in Python?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Use slicing: `reversed_string = original_string[::-1]`\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Instruction Format:\")\n",
    "print(json.dumps(instruction_format, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Exercise 7.1: Create Training Data - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 7.1 Solution: Creating Fine-Tuning Training Data\n\nThis demonstrates creating instruction-format training data for LoRA fine-tuning.\nQuality training data is the most important factor in fine-tuning success.\n\"\"\"\n\n# Training data in instruction format for a Fitness Coach persona\n# Each example has three parts:\n# - instruction: The task type/category\n# - input: The user's actual question\n# - output: The ideal response (what we want the model to learn)\nmy_training_data = [\n    {\n        \"instruction\": \"Provide fitness advice for a beginner.\",\n        \"input\": \"I want to start exercising but I've never worked out before. Where do I begin?\",\n        \"output\": \"Start with 20-30 minutes of walking 3-4 times per week. Once comfortable, add bodyweight exercises like squats, push-ups, and planks. Focus on consistency over intensity - it's better to do a little regularly than to overdo it and quit.\"\n    },\n    {\n        \"instruction\": \"Suggest a workout modification.\",\n        \"input\": \"I have bad knees and can't do regular squats. What can I do instead?\",\n        \"output\": \"Try wall sits or chair-assisted squats to reduce knee strain. You can also do glute bridges, which work similar muscles without stressing your knees. Swimming and cycling are excellent low-impact alternatives for leg strength.\"\n    },\n    {\n        \"instruction\": \"Provide nutrition advice for fitness.\",\n        \"input\": \"What should I eat before a morning workout?\",\n        \"output\": \"Have a light snack 30-60 minutes before: a banana, toast with peanut butter, or a small bowl of oatmeal. Avoid heavy meals. If exercising within 30 minutes of waking, just have water - your body can use stored energy for moderate workouts.\"\n    },\n    {\n        \"instruction\": \"Create a simple workout plan.\",\n        \"input\": \"Can you give me a quick 15-minute home workout?\",\n        \"output\": \"Try this circuit: 1 minute jumping jacks, 10 squats, 10 push-ups (or knee push-ups), 30-second plank, 10 lunges each leg, rest 30 seconds. Repeat twice. This hits all major muscle groups and gets your heart rate up!\"\n    },\n    {\n        \"instruction\": \"Motivate someone who wants to give up.\",\n        \"input\": \"I've been working out for 2 weeks and haven't lost any weight. Should I quit?\",\n        \"output\": \"Don't quit! Two weeks is too early to see major results - your body is adapting. Focus on non-scale victories: more energy, better sleep, improved mood. Muscle weighs more than fat, so you might be recomposing. Take progress photos and measurements instead.\"\n    }\n]\n\ndef validate_training_data(data):\n    \"\"\"\n    Validate that all training examples have required fields.\n    Returns True if valid, False otherwise with error message.\n    \"\"\"\n    required_keys = ['instruction', 'input', 'output']\n    for i, example in enumerate(data):\n        for key in required_keys:\n            if key not in example or example[key] is None:\n                print(f\"Example {i+1} missing '{key}'\")\n                return False\n    print(f\"All {len(data)} examples are valid!\")\n    return True\n\n# Validate our training data\nvalidate_training_data(my_training_data)"
  },
  {
   "cell_type": "markdown",
   "id": "32hhi8zqhdf",
   "source": "### Code Explanation: Exercise 7.1\n\n| Field | Purpose | Example |\n|-------|---------|---------|\n| `instruction` | Task category/type | \"Provide fitness advice for a beginner\" |\n| `input` | User's actual question | \"I want to start exercising but...\" |\n| `output` | Model's ideal response | \"Start with 20-30 minutes of walking...\" |\n\n**Instruction Format Structure:**\n```json\n{\n    \"instruction\": \"What type of task is this?\",\n    \"input\": \"What is the user asking?\",\n    \"output\": \"What should the model say?\"\n}\n```\n\n**Training Data Quality Checklist:**\n- ✓ Diverse examples covering different scenarios\n- ✓ Consistent tone/personality across outputs\n- ✓ Responses match desired length and style\n- ✓ Instruction categories are meaningful\n- ✓ No contradictory information between examples\n\n**Why 5 Examples Isn't Enough:**\nFor real fine-tuning, you typically need:\n- **Minimum:** 50-100 examples for basic adaptation\n- **Good:** 500-1000 examples for reliable behavior\n- **Production:** 10,000+ for complex tasks\n\n**Data Quality > Quantity:**\n5 high-quality examples that perfectly demonstrate desired behavior are better than 100 sloppy ones. Each example teaches the model a pattern.\n\n**Alternative Format (Chat Format):**\n```json\n{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a fitness coach.\"},\n        {\"role\": \"user\", \"content\": \"How do I start exercising?\"},\n        {\"role\": \"assistant\", \"content\": \"Start with...\"}\n    ]\n}\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Exercise 7.2: Design a LoRA Configuration - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 7.2 Solution: LoRA Configuration Design\n\nLoRA (Low-Rank Adaptation) enables efficient fine-tuning by training only\nsmall adapter matrices instead of the full model weights.\n\"\"\"\n\n# LoRA CONFIGURATION for fitness coach fine-tuning\n# These parameters control how the adapter matrices are structured\nmy_lora_config = {\n    # Rank (r): Dimensionality of the low-rank matrices\n    # Higher = more capacity, more memory, potentially better fit\n    # Typical values: 4, 8, 16, 32, 64\n    \"r\": 16,                # Medium rank - good balance for instruction following\n    \n    # Alpha: Scaling factor, typically 2x the rank\n    # Affects how much the LoRA updates influence the model\n    \"lora_alpha\": 32,       # 2x the rank as recommended\n    \n    # Target modules: Which transformer layers to adapt\n    # Attention layers are most impactful for behavior changes\n    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    \n    # Dropout: Regularization to prevent overfitting\n    # Lower for small datasets, higher for large datasets\n    \"lora_dropout\": 0.05,   # Light dropout for regularization\n}\n\n# TRAINING CONFIGURATION\n# Controls the optimization process\nmy_training_config = {\n    # Epochs: How many times to iterate through the training data\n    # More epochs = more learning, but risk of overfitting\n    \"num_epochs\": 3,        # Enough for small dataset to converge\n    \n    # Batch size: Examples processed together before weight update\n    # Smaller = less memory, noisier gradients\n    \"batch_size\": 4,        # Small batch for limited GPU memory\n    \n    # Learning rate: Step size for weight updates\n    # Too high = unstable, too low = slow/stuck\n    \"learning_rate\": 2e-4,  # Standard LoRA learning rate\n    \n    # Warmup: Gradually increase LR at start to stabilize training\n    \"warmup_ratio\": 0.05,   # 5% warmup\n}\n\nprint(\"Fitness Coach LoRA Config:\")\nprint(json.dumps(my_lora_config, indent=2))\nprint(\"\\nTraining Config:\")\nprint(json.dumps(my_training_config, indent=2))"
  },
  {
   "cell_type": "markdown",
   "id": "3c8j7xccglx",
   "source": "### Code Explanation: Exercise 7.2\n\n**LoRA Parameters:**\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `r` (rank) | 16 | Adapter matrix dimensions. Higher = more capacity |\n| `lora_alpha` | 32 | Scaling factor (usually 2x rank) |\n| `target_modules` | `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]` | Which layers to adapt |\n| `lora_dropout` | 0.05 | Regularization to prevent overfitting |\n\n**How LoRA Works:**\n```\nOriginal: W (large matrix, e.g., 4096×4096)\nLoRA:     W + A×B where A is 4096×16, B is 16×4096\n\nInstead of training 16M parameters in W,\nwe train only 131K parameters in A and B!\n```\n\n**Rank Selection Guide:**\n\n| Task Complexity | Recommended Rank |\n|-----------------|------------------|\n| Simple style change | 4-8 |\n| Instruction following | 8-16 |\n| Domain knowledge | 16-32 |\n| Complex reasoning | 32-64 |\n\n**Training Parameters:**\n\n| Parameter | Value | Effect |\n|-----------|-------|--------|\n| `num_epochs` | 3 | More = better fit, risk overfitting |\n| `batch_size` | 4 | Smaller = less GPU memory needed |\n| `learning_rate` | 2e-4 | Standard for LoRA (10x higher than full fine-tune) |\n| `warmup_ratio` | 0.05 | Stabilizes early training |\n\n**Memory Comparison:**\n- Full fine-tuning 7B model: ~28GB VRAM\n- LoRA fine-tuning 7B model: ~8GB VRAM\n- QLoRA (4-bit + LoRA): ~4GB VRAM\n\n**Why target attention layers?**\nAttention (q, k, v, o projections) controls how the model processes and relates information. Adapting these has the highest impact on behavior with minimal parameters.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "---\n",
    "# Lab Complete!\n",
    "\n",
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- **Basic Generation**: Use `ollama.generate()` for text completion\n",
    "- **Prompt Engineering**: Role-based prompts, structured output, JSON responses\n",
    "- **Parameters**: Control creativity with temperature\n",
    "- **Chat API**: Multi-turn conversations with `ollama.chat()`\n",
    "- **Applications**: Build summarizers, sentiment analyzers, and Q&A bots\n",
    "- **RAG**: Implement retrieval-augmented generation with embeddings\n",
    "- **Fine-tuning**: Understand LoRA/QLoRA for efficient model adaptation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}