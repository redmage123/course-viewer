{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Semantic Search & Data Catalogue RAG (Solutions)\n",
    "\n",
    "**Data Discovery: Harnessing AI, AGI & Vector Databases - Day 2**\n",
    "\n",
    "| Duration | Difficulty | Framework | Exercises |\n",
    "|---|---|---|---|\n",
    "| 90 min | Intermediate | pandas, numpy, sentence-transformers, chromadb, scikit-learn, rank_bm25, matplotlib | 6 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aysroko4gd",
   "source": "## Student Notes & Background\n\n### Why Semantic Search Matters for Data Discovery\n\nTraditional data catalogues rely on **keyword matching** — if you search for \"salary data,\" you only find assets whose metadata literally contains the word \"salary.\" But what about assets described as \"compensation benchmarking\" or \"payroll deductions\"? These are clearly relevant, yet a keyword search misses them entirely.\n\n**Semantic search** solves this by converting text into dense numerical vectors (embeddings) that capture *meaning*, not just surface words. Two descriptions that are conceptually similar will have vectors that are close together in embedding space, even if they share no words in common.\n\n### Key Concepts\n\n#### 1. Embedding Models\nAn **embedding model** (e.g., `all-MiniLM-L6-v2` from Sentence-Transformers) takes a piece of text and maps it to a fixed-length vector, typically 384 or 768 dimensions. These models are pre-trained on large text corpora and fine-tuned so that semantically similar texts produce similar vectors. Different models have different strengths — some excel at short queries, others at long documents, and some are tuned for specific domains.\n\n**Cosine similarity** is the standard metric for comparing embeddings. It measures the angle between two vectors:\n- **1.0** = identical direction (maximum similarity)\n- **0.0** = orthogonal (no similarity)\n- **-1.0** = opposite direction (though rare with modern embeddings)\n\n#### 2. BM25 Keyword Search\n**BM25 (Best Matching 25)** is a classical information retrieval algorithm that improves on simple TF-IDF. It scores documents by:\n- **Term Frequency (TF):** How often the query term appears in the document (with diminishing returns)\n- **Inverse Document Frequency (IDF):** Terms that appear in fewer documents are weighted higher\n- **Document length normalisation:** Longer documents are penalised slightly to avoid bias\n\nBM25 excels at finding exact keyword matches and is extremely fast, but it cannot understand synonyms, paraphrases, or conceptual relationships.\n\n#### 3. Hybrid Search (Fusion)\nNeither keyword nor semantic search is universally superior. **Hybrid search** combines both using weighted fusion:\n\n```\nfinal_score = α × BM25_normalised + (1 - α) × cosine_similarity\n```\n\n- **α = 0.0:** Pure semantic search\n- **α = 0.5:** Equal weight (good default)\n- **α = 1.0:** Pure keyword search\n\nThe optimal α depends on your data and use case. In practice, hybrid search consistently outperforms either strategy alone.\n\n#### 4. Precision@k\n**Precision@k** measures what fraction of the top-k results are truly relevant:\n\n```\nPrecision@k = (number of relevant results in top k) / k\n```\n\nFor example, if you search for \"employee data\" and 3 of your top-5 results are from the HR category, your Precision@5 = 3/5 = 0.60.\n\n#### 5. Retrieval-Augmented Generation (RAG)\n**RAG** is a pattern that combines retrieval with language model generation:\n1. **Retrieve** relevant documents using search (keyword, semantic, or hybrid)\n2. **Augment** a prompt with the retrieved context\n3. **Generate** an answer grounded in the retrieved facts\n\nIn this lab, we simulate the generation step with structured extraction since we don't have a live LLM, but the retrieval pipeline is identical to what you'd use in production.\n\n#### 6. Query Expansion\n**Query expansion** improves recall by adding semantically related terms to the original query before searching. For example, expanding \"salary\" might add \"compensation,\" \"payroll,\" \"remuneration,\" and \"wages.\" This helps bridge vocabulary gaps between the user's query and the catalogue descriptions.\n\n### What You'll Build\n\nIn this lab, you will:\n1. **Compare** two embedding models (`all-MiniLM-L6-v2` vs `all-mpnet-base-v2`) by measuring how well each clusters descriptions from the same department\n2. **Build** a BM25 keyword search index and test it with domain queries\n3. **Implement** hybrid search that fuses BM25 and semantic scores with a tuneable α parameter\n4. **Evaluate** search quality using Precision@k against known ground-truth category labels\n5. **Build** a RAG-style pipeline that retrieves relevant catalogue entries and produces structured answers\n6. **Implement** query expansion using embedding similarity over the corpus vocabulary\n\n### Prerequisites\n- Familiarity with pandas DataFrames and numpy arrays\n- Basic understanding of cosine similarity (from Lab 1)\n- Concepts from Lab 1: TF-IDF vectorisation, ChromaDB basics\n\n### Tips\n- The synthetic data uses a fixed random seed (`np.random.seed(42)`), so your results should be reproducible\n- When comparing search strategies, pay attention to which categories each strategy retrieves — this reveals their strengths and weaknesses\n- The α parameter in hybrid search is powerful — experiment with different values to build intuition\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Synthetic Data Catalogue\n",
    "\n",
    "We reuse the **exact same 500-asset catalogue** from Lab 1 so that results are\n",
    "directly comparable.  The random seed, categories, description pools, and\n",
    "generation logic are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "categories = ['HR', 'Finance', 'Marketing', 'Engineering', 'Legal']\n",
    "sources = ['PostgreSQL', 'S3 Bucket', 'SharePoint', 'Salesforce', 'MongoDB']\n",
    "data_types = ['Table', 'Document', 'Spreadsheet', 'Log File', 'Report']\n",
    "sensitivity_levels = ['Public', 'Internal', 'Confidential', 'Restricted']\n",
    "\n",
    "descriptions_pool = {\n",
    "    'HR': [\n",
    "        'Employee personal records including name address and date of birth',\n",
    "        'Annual performance review scores and manager feedback',\n",
    "        'Payroll data with salary deductions and tax withholdings',\n",
    "        'Recruitment pipeline tracking applicant status and interview notes',\n",
    "        'Benefits enrollment records for health dental and vision plans',\n",
    "        'Employee onboarding documentation and training completion',\n",
    "        'Workforce diversity and inclusion metrics by department',\n",
    "        'Time and attendance records with overtime calculations',\n",
    "        'Employee termination records and exit interview summaries',\n",
    "        'Compensation benchmarking data across industry roles',\n",
    "    ],\n",
    "    'Finance': [\n",
    "        'Quarterly revenue reports broken down by business unit',\n",
    "        'Accounts payable invoices and payment processing records',\n",
    "        'Annual budget forecasts with departmental allocations',\n",
    "        'Customer billing records including credit card transactions',\n",
    "        'Expense reimbursement claims with receipt attachments',\n",
    "        'General ledger entries and journal adjustments',\n",
    "        'Tax filing documents and regulatory compliance records',\n",
    "        'Cash flow projections and working capital analysis',\n",
    "        'Vendor payment terms and contract financial summaries',\n",
    "        'Audit trail logs for financial transaction approvals',\n",
    "    ],\n",
    "    'Marketing': [\n",
    "        'Campaign performance metrics including click rates and conversions',\n",
    "        'Customer segmentation profiles based on purchase behaviour',\n",
    "        'Social media analytics with engagement and reach data',\n",
    "        'Email marketing subscriber lists with opt-in preferences',\n",
    "        'Brand sentiment analysis from customer reviews and surveys',\n",
    "        'Website traffic analytics and user journey tracking',\n",
    "        'Lead scoring models and marketing qualified lead reports',\n",
    "        'Content calendar and editorial planning documents',\n",
    "        'Competitive intelligence reports and market research data',\n",
    "        'Event registration lists with attendee contact information',\n",
    "    ],\n",
    "    'Engineering': [\n",
    "        'Application server logs with error traces and stack dumps',\n",
    "        'CI/CD pipeline metrics including build times and failure rates',\n",
    "        'Infrastructure monitoring data from cloud resources',\n",
    "        'API usage statistics and rate limiting configurations',\n",
    "        'Database schema documentation and migration scripts',\n",
    "        'Code repository commit history and pull request reviews',\n",
    "        'Load testing results and performance benchmarks',\n",
    "        'Security vulnerability scan reports and remediation tracking',\n",
    "        'Microservice dependency maps and architecture diagrams',\n",
    "        'Incident response logs and post-mortem analysis documents',\n",
    "    ],\n",
    "    'Legal': [\n",
    "        'Active contract repository with vendor agreements and SLAs',\n",
    "        'Intellectual property filings including patents and trademarks',\n",
    "        'Regulatory compliance audit findings and remediation plans',\n",
    "        'Data processing agreements under GDPR Article 28',\n",
    "        'Litigation case files and legal correspondence records',\n",
    "        'Corporate governance meeting minutes and board resolutions',\n",
    "        'Privacy impact assessments for new data processing activities',\n",
    "        'Non-disclosure agreement tracking and expiration dates',\n",
    "        'Employment law compliance documentation by jurisdiction',\n",
    "        'Insurance policy records and claims history',\n",
    "    ],\n",
    "}\n",
    "\n",
    "n_assets = 500\n",
    "records = []\n",
    "\n",
    "for i in range(n_assets):\n",
    "    cat = np.random.choice(categories)\n",
    "    desc = np.random.choice(descriptions_pool[cat])\n",
    "    if np.random.random() < 0.3:\n",
    "        desc += ' updated ' + np.random.choice(['weekly', 'monthly', 'quarterly', 'annually'])\n",
    "    records.append({\n",
    "        'asset_id': f'ASSET-{i+1:04d}',\n",
    "        'name': f'{cat.lower()}_{np.random.choice([\"report\", \"dataset\", \"log\", \"file\", \"table\"])}_{i+1:04d}',\n",
    "        'description': desc,\n",
    "        'category': cat,\n",
    "        'source': np.random.choice(sources),\n",
    "        'data_type': np.random.choice(data_types),\n",
    "        'sensitivity': np.random.choice(sensitivity_levels, p=[0.15, 0.35, 0.30, 0.20]),\n",
    "        'owner': np.random.choice(['alice', 'bob', 'carol', 'dave', 'eve', None], p=[0.2, 0.2, 0.2, 0.2, 0.15, 0.05]),\n",
    "        'row_count': np.random.randint(100, 1_000_000) if np.random.random() > 0.2 else None,\n",
    "        'last_updated': pd.Timestamp('2023-01-01') + pd.Timedelta(days=int(np.random.randint(0, 730))),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(f\"Generated {len(df)} data asset records\")\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Multi-Strategy Embeddings - SOLUTION\n",
    "\n",
    "Compare two sentence-transformer models on our catalogue descriptions:\n",
    "- **all-MiniLM-L6-v2** (lightweight, 384-dim)\n",
    "- **all-mpnet-base-v2** (higher quality, 768-dim)\n",
    "\n",
    "We measure **intra-class cosine similarity** (how close descriptions within the\n",
    "same category are to each other) to see which model produces tighter clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both embedding models\n",
    "model_mini = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model_mpnet = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "descriptions = df['description'].tolist()\n",
    "\n",
    "print(\"Encoding descriptions with all-MiniLM-L6-v2...\")\n",
    "emb_mini = model_mini.encode(descriptions, show_progress_bar=True)\n",
    "\n",
    "print(\"Encoding descriptions with all-mpnet-base-v2...\")\n",
    "emb_mpnet = model_mpnet.encode(descriptions, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nMiniLM embeddings shape:  {emb_mini.shape}\")\n",
    "print(f\"MPNet embeddings shape:   {emb_mpnet.shape}\")\n",
    "\n",
    "# Compute intra-class similarity for each model\n",
    "def compute_intra_class_similarity(embeddings, labels):\n",
    "    \"\"\"Compute average cosine similarity among items in the same class.\"\"\"\n",
    "    unique_labels = sorted(set(labels))\n",
    "    results = {}\n",
    "    for label in unique_labels:\n",
    "        mask = [i for i, l in enumerate(labels) if l == label]\n",
    "        class_emb = embeddings[mask]\n",
    "        sim_matrix = cosine_similarity(class_emb)\n",
    "        # Take upper triangle (exclude diagonal)\n",
    "        n = len(mask)\n",
    "        upper_tri = sim_matrix[np.triu_indices(n, k=1)]\n",
    "        results[label] = float(np.mean(upper_tri))\n",
    "    return results\n",
    "\n",
    "labels = df['category'].tolist()\n",
    "sim_mini = compute_intra_class_similarity(emb_mini, labels)\n",
    "sim_mpnet = compute_intra_class_similarity(emb_mpnet, labels)\n",
    "\n",
    "print(\"\\nIntra-class cosine similarity (higher = tighter clusters):\")\n",
    "print(f\"{'Category':<15} {'MiniLM':>10} {'MPNet':>10}\")\n",
    "print(\"-\" * 37)\n",
    "for cat in categories:\n",
    "    print(f\"{cat:<15} {sim_mini[cat]:>10.4f} {sim_mpnet[cat]:>10.4f}\")\n",
    "print(f\"{'Average':<15} {np.mean(list(sim_mini.values())):>10.4f} {np.mean(list(sim_mpnet.values())):>10.4f}\")\n",
    "\n",
    "# Bar chart comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "bars1 = ax.bar(x - width/2, [sim_mini[c] for c in categories], width,\n",
    "               label='all-MiniLM-L6-v2', color='#3b82f6')\n",
    "bars2 = ax.bar(x + width/2, [sim_mpnet[c] for c in categories], width,\n",
    "               label='all-mpnet-base-v2', color='#10b981')\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Intra-class Cosine Similarity')\n",
    "ax.set_title('Embedding Model Comparison: Intra-class Similarity')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2: BM25 Keyword Search - SOLUTION\n",
    "\n",
    "Build a **BM25Okapi** index over the catalogue descriptions.  BM25 is a\n",
    "classic term-frequency-based ranking function that excels at exact keyword\n",
    "matches and is complementary to dense semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize descriptions for BM25\n",
    "tokenized_corpus = [desc.lower().split() for desc in descriptions]\n",
    "\n",
    "# Build BM25 index\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "print(f\"BM25 index built over {len(tokenized_corpus)} documents\")\n",
    "print(f\"Average document length: {np.mean([len(d) for d in tokenized_corpus]):.1f} tokens\")\n",
    "\n",
    "def bm25_search(query, top_k=10):\n",
    "    \"\"\"Search the catalogue using BM25 keyword matching.\"\"\"\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'rank': len(results) + 1,\n",
    "            'asset_id': df.iloc[idx]['asset_id'],\n",
    "            'category': df.iloc[idx]['category'],\n",
    "            'description': df.iloc[idx]['description'],\n",
    "            'score': scores[idx],\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"employee salary payroll\",\n",
    "    \"revenue budget financial\",\n",
    "    \"server logs monitoring\",\n",
    "    \"contract compliance GDPR\",\n",
    "    \"campaign click conversion\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = bm25_search(query, top_k=5)\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 70)\n",
    "    for r in results:\n",
    "        print(f\"  {r['rank']}. [{r['category']:<12}] {r['description'][:65]}... (score: {r['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Hybrid Search - SOLUTION\n",
    "\n",
    "Combine **BM25 scores** (keyword relevance) with **cosine similarity** (semantic\n",
    "relevance) using a weighted fusion parameter `alpha`:\n",
    "\n",
    "$$\\text{hybrid\\_score} = \\alpha \\cdot \\text{norm\\_semantic} + (1 - \\alpha) \\cdot \\text{norm\\_bm25}$$\n",
    "\n",
    "where `alpha=1.0` is pure semantic and `alpha=0.0` is pure BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MiniLM embeddings for the hybrid search (faster, good quality)\n",
    "catalogue_embeddings = emb_mini\n",
    "\n",
    "def hybrid_search(query, alpha=0.5, top_k=10):\n",
    "    \"\"\"Combine BM25 keyword scores with semantic cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        query: Natural language search query\n",
    "        alpha: Weight for semantic score (0=pure BM25, 1=pure semantic)\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of result dicts with hybrid scores\n",
    "    \"\"\"\n",
    "    # BM25 scores\n",
    "    tokenized_query = query.lower().split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Normalise BM25 scores to [0, 1]\n",
    "    bm25_max = bm25_scores.max()\n",
    "    if bm25_max > 0:\n",
    "        bm25_norm = bm25_scores / bm25_max\n",
    "    else:\n",
    "        bm25_norm = bm25_scores\n",
    "    \n",
    "    # Semantic cosine similarity scores\n",
    "    query_emb = model_mini.encode([query])\n",
    "    semantic_scores = cosine_similarity(query_emb, catalogue_embeddings)[0]\n",
    "    \n",
    "    # Normalise semantic scores to [0, 1]\n",
    "    sem_min = semantic_scores.min()\n",
    "    sem_max = semantic_scores.max()\n",
    "    if sem_max > sem_min:\n",
    "        semantic_norm = (semantic_scores - sem_min) / (sem_max - sem_min)\n",
    "    else:\n",
    "        semantic_norm = semantic_scores\n",
    "    \n",
    "    # Weighted fusion\n",
    "    hybrid_scores = alpha * semantic_norm + (1 - alpha) * bm25_norm\n",
    "    \n",
    "    # Rank by hybrid score\n",
    "    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'rank': len(results) + 1,\n",
    "            'asset_id': df.iloc[idx]['asset_id'],\n",
    "            'category': df.iloc[idx]['category'],\n",
    "            'description': df.iloc[idx]['description'],\n",
    "            'hybrid_score': hybrid_scores[idx],\n",
    "            'semantic_score': semantic_scores[idx],\n",
    "            'bm25_score': bm25_scores[idx],\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Demonstrate hybrid search with different alpha values\n",
    "demo_query = \"employee salary compensation data\"\n",
    "\n",
    "for alpha in [0.0, 0.5, 1.0]:\n",
    "    label = {0.0: 'Pure BM25', 0.5: 'Hybrid (50/50)', 1.0: 'Pure Semantic'}[alpha]\n",
    "    results = hybrid_search(demo_query, alpha=alpha, top_k=5)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Query: '{demo_query}' | Strategy: {label} (alpha={alpha})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for r in results:\n",
    "        print(f\"  {r['rank']}. [{r['category']:<12}] {r['description'][:55]}...\")\n",
    "        print(f\"     hybrid={r['hybrid_score']:.3f}  sem={r['semantic_score']:.3f}  bm25={r['bm25_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Search Quality Evaluation - SOLUTION\n",
    "\n",
    "Measure **precision@k** for each search strategy.  We define a ground-truth\n",
    "mapping from queries to expected categories and check how many of the top-k\n",
    "results belong to the correct category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth: queries mapped to their expected relevant category\n",
    "ground_truth = {\n",
    "    \"employee salary payroll\": \"HR\",\n",
    "    \"performance review feedback\": \"HR\",\n",
    "    \"recruitment hiring interview\": \"HR\",\n",
    "    \"revenue budget quarterly\": \"Finance\",\n",
    "    \"invoice payment accounts\": \"Finance\",\n",
    "    \"tax filing compliance\": \"Finance\",\n",
    "    \"campaign click conversion\": \"Marketing\",\n",
    "    \"customer segmentation purchase\": \"Marketing\",\n",
    "    \"social media engagement\": \"Marketing\",\n",
    "    \"server logs error monitoring\": \"Engineering\",\n",
    "    \"CI/CD pipeline build deployment\": \"Engineering\",\n",
    "    \"API usage rate limiting\": \"Engineering\",\n",
    "    \"contract vendor agreement SLA\": \"Legal\",\n",
    "    \"GDPR data processing privacy\": \"Legal\",\n",
    "    \"patent trademark intellectual property\": \"Legal\",\n",
    "}\n",
    "\n",
    "def precision_at_k(results, relevant_category, k):\n",
    "    \"\"\"Compute precision@k: fraction of top-k results in the relevant category.\"\"\"\n",
    "    top_k_results = results[:k]\n",
    "    relevant_count = sum(1 for r in top_k_results if r['category'] == relevant_category)\n",
    "    return relevant_count / k\n",
    "\n",
    "def semantic_search_as_results(query, top_k=10):\n",
    "    \"\"\"Pure semantic search returning results in the same format.\"\"\"\n",
    "    query_emb = model_mini.encode([query])\n",
    "    scores = cosine_similarity(query_emb, catalogue_embeddings)[0]\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'rank': len(results) + 1,\n",
    "            'asset_id': df.iloc[idx]['asset_id'],\n",
    "            'category': df.iloc[idx]['category'],\n",
    "            'description': df.iloc[idx]['description'],\n",
    "            'score': scores[idx],\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Evaluate all three strategies\n",
    "strategies = {\n",
    "    'BM25': lambda q, k: bm25_search(q, top_k=k),\n",
    "    'Semantic': lambda q, k: semantic_search_as_results(q, top_k=k),\n",
    "    'Hybrid': lambda q, k: hybrid_search(q, alpha=0.5, top_k=k),\n",
    "}\n",
    "\n",
    "k_values = [5, 10]\n",
    "eval_results = {strat: {f'P@{k}': [] for k in k_values} for strat in strategies}\n",
    "\n",
    "for query, relevant_cat in ground_truth.items():\n",
    "    for strat_name, search_fn in strategies.items():\n",
    "        results = search_fn(query, max(k_values))\n",
    "        for k in k_values:\n",
    "            p_at_k = precision_at_k(results, relevant_cat, k)\n",
    "            eval_results[strat_name][f'P@{k}'].append(p_at_k)\n",
    "\n",
    "# Compute averages\n",
    "print(\"Search Quality Evaluation (averaged over 15 queries)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Strategy':<12} {'P@5':>8} {'P@10':>8}\")\n",
    "print(\"-\" * 30)\n",
    "avg_scores = {}\n",
    "for strat_name in strategies:\n",
    "    p5 = np.mean(eval_results[strat_name]['P@5'])\n",
    "    p10 = np.mean(eval_results[strat_name]['P@10'])\n",
    "    avg_scores[strat_name] = {'P@5': p5, 'P@10': p10}\n",
    "    print(f\"{strat_name:<12} {p5:>8.3f} {p10:>8.3f}\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(strategies))\n",
    "width = 0.35\n",
    "strat_names = list(strategies.keys())\n",
    "bars1 = ax.bar(x - width/2, [avg_scores[s]['P@5'] for s in strat_names], width,\n",
    "               label='Precision@5', color='#3b82f6')\n",
    "bars2 = ax.bar(x + width/2, [avg_scores[s]['P@10'] for s in strat_names], width,\n",
    "               label='Precision@10', color='#10b981')\n",
    "\n",
    "ax.set_xlabel('Search Strategy')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Search Quality: Precision@K Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(strat_names)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1: Build a Data Catalogue RAG Pipeline - SOLUTION\n",
    "\n",
    "Implement a **Retrieval-Augmented Generation (RAG)** pipeline that:\n",
    "1. Accepts a natural-language question about the data catalogue\n",
    "2. Retrieves the top-5 most relevant assets via hybrid search\n",
    "3. Formats them as context\n",
    "4. Generates a structured answer summarising the findings\n",
    "\n",
    "> **Note:** Since we do not have an LLM endpoint in this lab environment we\n",
    "> simulate the generation step by programmatically extracting and summarising\n",
    "> key information from the retrieved documents.  In production you would pass\n",
    "> the context to an LLM (e.g. GPT-4, Claude, Llama) for free-form answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(results):\n",
    "    \"\"\"Format retrieved results into a context string for the RAG pipeline.\"\"\"\n",
    "    context_parts = []\n",
    "    for r in results:\n",
    "        context_parts.append(\n",
    "            f\"[{r['asset_id']}] Category: {r['category']} | \"\n",
    "            f\"Description: {r['description']}\"\n",
    "        )\n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "def generate_answer(question, results):\n",
    "    \"\"\"Simulate LLM generation by summarising retrieved documents.\n",
    "    \n",
    "    In production, replace this with an actual LLM call:\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "        answer = llm.generate(prompt)\n",
    "    \"\"\"\n",
    "    # Extract statistics from retrieved results\n",
    "    categories_found = Counter(r['category'] for r in results)\n",
    "    unique_descriptions = list(dict.fromkeys(r['description'] for r in results))\n",
    "    asset_ids = [r['asset_id'] for r in results]\n",
    "    avg_score = np.mean([r.get('hybrid_score', 0) for r in results])\n",
    "    \n",
    "    # Build structured answer\n",
    "    answer_parts = [\n",
    "        f\"Based on {len(results)} retrieved data assets, here is a summary:\\n\",\n",
    "        f\"**Relevant Categories:** {', '.join(f'{cat} ({cnt})' for cat, cnt in categories_found.most_common())}\\n\",\n",
    "        f\"**Matching Assets:** {', '.join(asset_ids)}\\n\",\n",
    "        f\"**Key Findings:**\",\n",
    "    ]\n",
    "    for i, desc in enumerate(unique_descriptions, 1):\n",
    "        answer_parts.append(f\"  {i}. {desc}\")\n",
    "    \n",
    "    answer_parts.append(f\"\\n**Average Relevance Score:** {avg_score:.3f}\")\n",
    "    answer_parts.append(f\"**Recommendation:** Focus on the {categories_found.most_common(1)[0][0]} \"\n",
    "                        f\"category assets which dominate the results for this query.\")\n",
    "    \n",
    "    return \"\\n\".join(answer_parts)\n",
    "\n",
    "\n",
    "def rag_query(question, alpha=0.5, top_k=5):\n",
    "    \"\"\"Full RAG pipeline: retrieve -> format context -> generate answer.\"\"\"\n",
    "    # Step 1: Retrieve\n",
    "    results = hybrid_search(question, alpha=alpha, top_k=top_k)\n",
    "    \n",
    "    # Step 2: Format context\n",
    "    context = format_context(results)\n",
    "    \n",
    "    # Step 3: Generate answer\n",
    "    answer = generate_answer(question, results)\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'context': context,\n",
    "        'answer': answer,\n",
    "        'results': results,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the RAG pipeline with several questions\n",
    "questions = [\n",
    "    \"What data assets contain employee salary information?\",\n",
    "    \"Which datasets track marketing campaign performance?\",\n",
    "    \"Are there any assets related to regulatory compliance or auditing?\",\n",
    "    \"What engineering monitoring and observability data do we have?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = rag_query(question)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUESTION: {result['question']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n--- Retrieved Context ---\")\n",
    "    print(result['context'])\n",
    "    print(f\"\\n--- Generated Answer ---\")\n",
    "    print(result['answer'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2: Query Expansion - SOLUTION\n",
    "\n",
    "Improve retrieval by **expanding** the original query with semantically related\n",
    "terms drawn from the catalogue vocabulary.  We:\n",
    "1. Build a vocabulary from all unique words in descriptions\n",
    "2. Embed the vocabulary with the same model\n",
    "3. Find words most similar to the query to generate expansion terms\n",
    "4. Append expansion terms to the original query and re-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from corpus\n",
    "all_words = set()\n",
    "for desc in descriptions:\n",
    "    for word in desc.lower().split():\n",
    "        # Keep only alphabetic tokens >= 3 chars\n",
    "        cleaned = ''.join(c for c in word if c.isalpha())\n",
    "        if len(cleaned) >= 3:\n",
    "            all_words.add(cleaned)\n",
    "\n",
    "vocab_list = sorted(all_words)\n",
    "print(f\"Vocabulary size: {len(vocab_list)} unique terms\")\n",
    "\n",
    "# Embed the vocabulary\n",
    "print(\"Encoding vocabulary...\")\n",
    "vocab_embeddings = model_mini.encode(vocab_list, show_progress_bar=True)\n",
    "print(f\"Vocabulary embeddings shape: {vocab_embeddings.shape}\")\n",
    "\n",
    "\n",
    "def expand_query(query, n_expansion_terms=5):\n",
    "    \"\"\"Expand a query with semantically related terms from the corpus vocabulary.\"\"\"\n",
    "    # Encode the query\n",
    "    query_emb = model_mini.encode([query])\n",
    "    \n",
    "    # Compute similarity to all vocab terms\n",
    "    similarities = cosine_similarity(query_emb, vocab_embeddings)[0]\n",
    "    \n",
    "    # Get query words to exclude them from expansion\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # Rank vocab by similarity, skip words already in query\n",
    "    ranked_indices = np.argsort(similarities)[::-1]\n",
    "    expansion_terms = []\n",
    "    for idx in ranked_indices:\n",
    "        term = vocab_list[idx]\n",
    "        if term not in query_words and similarities[idx] > 0.1:\n",
    "            expansion_terms.append((term, similarities[idx]))\n",
    "        if len(expansion_terms) >= n_expansion_terms:\n",
    "            break\n",
    "    \n",
    "    return expansion_terms\n",
    "\n",
    "\n",
    "def expanded_hybrid_search(query, alpha=0.5, top_k=10, n_expansion=5):\n",
    "    \"\"\"Perform hybrid search with query expansion.\"\"\"\n",
    "    # Get expansion terms\n",
    "    expansion = expand_query(query, n_expansion_terms=n_expansion)\n",
    "    expansion_words = [term for term, score in expansion]\n",
    "    expanded_query = query + \" \" + \" \".join(expansion_words)\n",
    "    \n",
    "    return expanded_query, expansion, hybrid_search(expanded_query, alpha=alpha, top_k=top_k)\n",
    "\n",
    "\n",
    "# Demonstrate query expansion and improved retrieval\n",
    "demo_queries = [\n",
    "    \"employee records\",\n",
    "    \"financial audit\",\n",
    "    \"cloud infrastructure\",\n",
    "]\n",
    "\n",
    "for query in demo_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Original query: '{query}'\")\n",
    "    \n",
    "    # Get expansion terms\n",
    "    expansion = expand_query(query, n_expansion_terms=5)\n",
    "    print(f\"Expansion terms: {', '.join(f'{t} ({s:.3f})' for t, s in expansion)}\")\n",
    "    \n",
    "    expanded_query, _, expanded_results = expanded_hybrid_search(query, top_k=5)\n",
    "    print(f\"Expanded query:  '{expanded_query}'\")\n",
    "    \n",
    "    # Compare original vs expanded results\n",
    "    original_results = hybrid_search(query, alpha=0.5, top_k=5)\n",
    "    \n",
    "    print(f\"\\n  {'Original Results':<40} | {'Expanded Results':<40}\")\n",
    "    print(f\"  {'-'*40} | {'-'*40}\")\n",
    "    for orig, exp in zip(original_results, expanded_results):\n",
    "        orig_str = f\"[{orig['category']:<10}] {orig['description'][:25]}...\"\n",
    "        exp_str = f\"[{exp['category']:<10}] {exp['description'][:25]}...\"\n",
    "        print(f\"  {orig_str:<40} | {exp_str:<40}\")\n",
    "\n",
    "# Quantitative evaluation: compare precision with and without expansion\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"Precision@5 Comparison: Standard vs Query-Expanded Hybrid Search\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "p5_standard = []\n",
    "p5_expanded = []\n",
    "\n",
    "for query, relevant_cat in ground_truth.items():\n",
    "    # Standard hybrid\n",
    "    std_results = hybrid_search(query, alpha=0.5, top_k=5)\n",
    "    p5_std = precision_at_k(std_results, relevant_cat, 5)\n",
    "    p5_standard.append(p5_std)\n",
    "    \n",
    "    # Expanded hybrid\n",
    "    _, _, exp_results = expanded_hybrid_search(query, alpha=0.5, top_k=5)\n",
    "    p5_exp = precision_at_k(exp_results, relevant_cat, 5)\n",
    "    p5_expanded.append(p5_exp)\n",
    "\n",
    "print(f\"\\nAverage P@5 (Standard Hybrid): {np.mean(p5_standard):.3f}\")\n",
    "print(f\"Average P@5 (Expanded Hybrid): {np.mean(p5_expanded):.3f}\")\n",
    "improvement = np.mean(p5_expanded) - np.mean(p5_standard)\n",
    "print(f\"Improvement: {improvement:+.3f} ({improvement/max(np.mean(p5_standard), 1e-9)*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Compare** embedding models (MiniLM vs MPNet) using intra-class similarity\n",
    "2. **Build** a BM25 keyword index for fast term-based retrieval\n",
    "3. **Fuse** BM25 and semantic scores into a hybrid search with tuneable alpha\n",
    "4. **Evaluate** search quality with precision@k on ground-truth queries\n",
    "5. **Implement** a RAG pipeline that retrieves catalogue context and generates structured answers\n",
    "6. **Expand** queries with semantically related vocabulary for improved recall\n",
    "\n",
    "---\n",
    "\n",
    "*Data Discovery: Harnessing AI, AGI & Vector Databases | AI Elevate*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}