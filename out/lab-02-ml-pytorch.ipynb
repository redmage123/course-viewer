{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2: Machine Learning with PyTorch\n\n## From Classical ML to Deep Learning Foundations\n\n**Duration:** 90-120 minutes | **Difficulty:** Intermediate | **Prerequisites:** Lab 1\n\n---\n\n## Overview\n\nThis lab bridges classical machine learning and deep learning by teaching PyTorch fundamentals through hands-on implementation of ML algorithms. You'll learn to build, train, and evaluate models using the same patterns used in production deep learning systems.\n\n### Lab Structure\n\n| Part | Topic | Key Concepts |\n|------|-------|--------------|\n| **Part 1** | PyTorch Tensors | Creating tensors, tensor operations, automatic differentiation (autograd) |\n| **Part 2** | Linear Regression | Training loop from scratch, MSE loss, nn.Module, gradient descent |\n| **Part 3** | Logistic Regression | Sigmoid function, BCE loss, binary classification, decision boundaries |\n| **Part 4** | Support Vector Machines | Kernel trick (linear, RBF, polynomial), margins, support vectors |\n| **Part 5** | Model Evaluation | Confusion matrix, precision, recall, F1-score, classification report |\n\n### Key Pattern You'll Learn\n\nThe PyTorch training loop used in all deep learning:\n\n```python\nfor epoch in range(n_epochs):\n    y_pred = model(X)           # Forward pass\n    loss = criterion(y_pred, y) # Compute loss\n    optimizer.zero_grad()       # Clear gradients\n    loss.backward()             # Backward pass\n    optimizer.step()            # Update weights\n```\n\n### Libraries Used\n\n- **PyTorch** (torch, nn, optim) - Deep learning framework\n- **scikit-learn** - SVM, train_test_split, metrics\n- **NumPy, Pandas, Matplotlib** - Data manipulation and visualization\n\n---\n\n### Learning Objectives\n\nBy the end of this lab, you will be able to:\n\n1. **Create and manipulate PyTorch tensors** - the building blocks of deep learning\n2. **Use automatic differentiation** - PyTorch's killer feature for training models\n3. **Implement Linear Regression** - from scratch and with nn.Module\n4. **Build Logistic Regression** - for binary classification\n5. **Train Support Vector Machines** - with different kernels\n6. **Evaluate models properly** - confusion matrix, precision, recall, F1\n\n### How This Lab Works\n\nThis lab follows the **Learn → Practice** pattern:\n\n1. **Demonstration cells** show working examples with explanations\n2. **Exercise cells** ask you to apply what you learned (`# YOUR CODE HERE`)\n3. **Expected outputs** help you verify your solutions\n\n---\n\n**Let's begin!** Run the setup cell below first."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP - Run this cell first!\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"  Setup Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  NumPy version: {np.__version__}\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n  You're ready to start the lab!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: PyTorch Tensors\n",
    "\n",
    "## What is a Tensor?\n",
    "\n",
    "**Tensors** are the fundamental data structure in PyTorch - like NumPy arrays but with:\n",
    "- **GPU support** - Run computations on graphics cards for massive speedups\n",
    "- **Automatic differentiation** - Automatically compute gradients for training\n",
    "\n",
    "| NumPy | PyTorch | Description |\n",
    "|-------|---------|-------------|\n",
    "| `np.array()` | `torch.tensor()` | Create from list |\n",
    "| `np.zeros()` | `torch.zeros()` | Array of zeros |\n",
    "| `np.ones()` | `torch.ones()` | Array of ones |\n",
    "| `np.random.rand()` | `torch.rand()` | Random [0, 1) |\n",
    "| `arr.shape` | `tensor.shape` | Get dimensions |\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Creating Tensors\n",
    "\n",
    "### Demonstration: Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMONSTRATION: Creating Tensors\n",
    "# ============================================\n",
    "\n",
    "# Method 1: From a Python list\n",
    "t1 = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(\"1. From list:\")\n",
    "print(f\"   torch.tensor([1, 2, 3, 4, 5]) = {t1}\")\n",
    "print(f\"   Shape: {t1.shape}, Dtype: {t1.dtype}\")\n",
    "print()\n",
    "\n",
    "# Method 2: From NumPy array\n",
    "np_arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.from_numpy(np_arr)\n",
    "print(\"2. From NumPy:\")\n",
    "print(f\"   torch.from_numpy(np_arr) =\\n{t2}\")\n",
    "print()\n",
    "\n",
    "# Method 3: Zeros and Ones\n",
    "t_zeros = torch.zeros(2, 3)  # 2 rows, 3 columns\n",
    "t_ones = torch.ones(2, 3)\n",
    "print(\"3. Zeros (2x3):\")\n",
    "print(t_zeros)\n",
    "print()\n",
    "\n",
    "# Method 4: Random tensors\n",
    "t_rand = torch.rand(2, 3)    # Uniform [0, 1)\n",
    "t_randn = torch.randn(2, 3)  # Normal distribution (mean=0, std=1)\n",
    "print(\"4. Random (uniform):\")\n",
    "print(t_rand)\n",
    "print()\n",
    "\n",
    "# Method 5: Specifying dtype\n",
    "t_float = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "print(f\"5. With dtype: {t_float} (dtype: {t_float.dtype})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Create Tensors\n",
    "\n",
    "**Your Task:** Create the tensors specified below.\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "a) tensor([10, 20, 30, 40, 50])\n",
    "b) Shape: torch.Size([3, 4])\n",
    "c) Random tensor with shape torch.Size([2, 5])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 1.1: Create Tensors\n",
    "# ============================================\n",
    "\n",
    "# a) Create a tensor containing [10, 20, 30, 40, 50]\n",
    "tensor_a = None  # YOUR CODE HERE\n",
    "\n",
    "# b) Create a 3x4 tensor filled with zeros\n",
    "tensor_b = None  # YOUR CODE HERE\n",
    "\n",
    "# c) Create a 2x5 tensor with random values (uniform)\n",
    "tensor_c = None  # YOUR CODE HERE\n",
    "\n",
    "# ---- Test your answers ----\n",
    "print(f\"a) {tensor_a}\")\n",
    "print(f\"b) Shape: {tensor_b.shape if tensor_b is not None else 'None'}\")\n",
    "print(f\"c) Random tensor with shape {tensor_c.shape if tensor_c is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 Tensor Operations\n",
    "\n",
    "### Demonstration: Math Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMONSTRATION: Tensor Operations\n",
    "# ============================================\n",
    "\n",
    "a = torch.tensor([[1., 2.], [3., 4.]])\n",
    "b = torch.tensor([[5., 6.], [7., 8.]])\n",
    "\n",
    "print(\"Tensor a:\")\n",
    "print(a)\n",
    "print(\"\\nTensor b:\")\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "# Element-wise operations\n",
    "print(\"Element-wise addition (a + b):\")\n",
    "print(a + b)\n",
    "print()\n",
    "\n",
    "print(\"Element-wise multiplication (a * b):\")\n",
    "print(a * b)\n",
    "print()\n",
    "\n",
    "# Matrix multiplication (VERY important for ML!)\n",
    "print(\"Matrix multiplication (a @ b):\")\n",
    "print(a @ b)  # Same as torch.matmul(a, b)\n",
    "print()\n",
    "\n",
    "# Aggregations\n",
    "print(f\"Sum of all elements: a.sum() = {a.sum()}\")\n",
    "print(f\"Mean of all elements: a.mean() = {a.mean()}\")\n",
    "print(f\"Sum per column: a.sum(dim=0) = {a.sum(dim=0)}\")\n",
    "print(f\"Sum per row: a.sum(dim=1) = {a.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Operations Summary\n",
    "\n",
    "| Operation | Code | Description |\n",
    "|-----------|------|-------------|\n",
    "| Add | `a + b` | Element-wise addition |\n",
    "| Multiply | `a * b` | Element-wise multiplication |\n",
    "| Matrix multiply | `a @ b` | Matrix multiplication |\n",
    "| Sum | `a.sum()` | Sum all elements |\n",
    "| Mean | `a.mean()` | Average of all elements |\n",
    "| Reshape | `a.reshape(r, c)` | Change shape |\n",
    "| Transpose | `a.T` | Swap rows/columns |\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 1.2: Tensor Operations\n",
    "\n",
    "**Your Task:** Perform the specified operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 1.2: Tensor Operations\n",
    "# ============================================\n",
    "\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
    "y = torch.tensor([[7., 8., 9.], [10., 11., 12.]])\n",
    "\n",
    "print(\"x =\")\n",
    "print(x)\n",
    "print(\"\\ny =\")\n",
    "print(y)\n",
    "print()\n",
    "\n",
    "# a) Add x and y element-wise\n",
    "result_add = None  # YOUR CODE HERE\n",
    "\n",
    "# b) Calculate the mean of x\n",
    "result_mean = None  # YOUR CODE HERE\n",
    "\n",
    "# c) Calculate the sum of each row of x (dim=1)\n",
    "result_row_sum = None  # YOUR CODE HERE\n",
    "\n",
    "# d) Reshape x to be 3 rows x 2 columns\n",
    "result_reshape = None  # YOUR CODE HERE\n",
    "\n",
    "# ---- Test your answers ----\n",
    "print(f\"a) x + y =\\n{result_add}\")\n",
    "print(f\"\\nb) Mean of x = {result_mean}\")\n",
    "print(f\"\\nc) Sum of each row = {result_row_sum}\")\n",
    "print(f\"\\nd) x reshaped to 3x2:\\n{result_reshape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 Automatic Differentiation (Autograd)\n",
    "\n",
    "**This is PyTorch's killer feature!** It automatically computes gradients, which we need for training models.\n",
    "\n",
    "### Why Do We Need Gradients?\n",
    "\n",
    "Machine learning training works by:\n",
    "1. Making a prediction\n",
    "2. Calculating how wrong it is (loss)\n",
    "3. **Computing gradients** to know which direction to adjust weights\n",
    "4. Updating weights to reduce the error\n",
    "\n",
    "PyTorch does step 3 automatically!\n",
    "\n",
    "### Demonstration: Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMONSTRATION: Automatic Differentiation\n",
    "# ============================================\n",
    "\n",
    "# Create a tensor and tell PyTorch to track gradients\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"requires_grad = {x.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# Define a function: y = x^2 + 2x + 1\n",
    "y = x**2 + 2*x + 1\n",
    "print(f\"y = x² + 2x + 1 = {y.item()}\")\n",
    "print()\n",
    "\n",
    "# Compute the gradient (derivative): dy/dx = 2x + 2\n",
    "y.backward()\n",
    "\n",
    "# At x=3: dy/dx = 2(3) + 2 = 8\n",
    "print(f\"dy/dx = 2x + 2 = {x.grad.item()}\")\n",
    "print()\n",
    "print(\"PyTorch computed the derivative automatically!\")\n",
    "print(\"This is how neural networks learn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration: Gradient with Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMONSTRATION: Multiple Variables\n",
    "# ============================================\n",
    "\n",
    "# Two parameters (like weights in ML)\n",
    "w = torch.tensor([2.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Input\n",
    "x = torch.tensor([3.0])\n",
    "\n",
    "# Forward pass: y = w*x + b (linear equation!)\n",
    "y = w * x + b\n",
    "print(f\"w = {w.item()}, b = {b.item()}, x = {x.item()}\")\n",
    "print(f\"y = w*x + b = {y.item()}\")\n",
    "print()\n",
    "\n",
    "# Compute gradients\n",
    "y.backward()\n",
    "\n",
    "print(\"Gradients:\")\n",
    "print(f\"  dy/dw = x = {w.grad.item()}\")\n",
    "print(f\"  dy/db = 1 = {b.grad.item()}\")\n",
    "print()\n",
    "print(\"These gradients tell us how to adjust w and b!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Practice with Autograd\n",
    "\n",
    "**Your Task:** Compute gradients for the given function.\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "y = 3x² - 4x + 5 at x=2: y = 9.0\n",
    "dy/dx = 6x - 4 at x=2: dy/dx = 8.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 1.3: Practice with Autograd\n",
    "# ============================================\n",
    "\n",
    "# Create x = 2.0 with gradient tracking\n",
    "x = None  # YOUR CODE HERE: torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Compute y = 3x² - 4x + 5\n",
    "y = None  # YOUR CODE HERE\n",
    "\n",
    "# Compute the gradient\n",
    "# YOUR CODE HERE: call y.backward()\n",
    "\n",
    "# ---- Test your answers ----\n",
    "if x is not None and y is not None:\n",
    "    print(f\"y = 3x² - 4x + 5 at x=2: y = {y.item()}\")\n",
    "    print(f\"dy/dx = 6x - 4 at x=2: dy/dx = {x.grad.item()}\")\n",
    "else:\n",
    "    print(\"Complete the code above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Linear Regression\n",
    "\n",
    "## What is Linear Regression?\n",
    "\n",
    "Linear regression finds the best line to fit data: **y = wx + b**\n",
    "\n",
    "- **w** (weight): The slope of the line\n",
    "- **b** (bias): The y-intercept\n",
    "\n",
    "We'll train a model by:\n",
    "1. Starting with random w and b\n",
    "2. Making predictions\n",
    "3. Measuring error (MSE loss)\n",
    "4. Using gradients to improve w and b\n",
    "5. Repeat!\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP: Generate Linear Regression Data\n",
    "# ============================================\n",
    "\n",
    "# True relationship: y = 3x + 2 (plus some noise)\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "X_np = np.random.rand(n_samples, 1) * 10  # X values from 0 to 10\n",
    "y_np = 3 * X_np + 2 + np.random.randn(n_samples, 1) * 1.5  # y = 3x + 2 + noise\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X_np, dtype=torch.float32)\n",
    "y = torch.tensor(y_np, dtype=torch.float32)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X.numpy(), y.numpy(), alpha=0.7, label='Data points')\n",
    "plt.plot([0, 10], [2, 32], 'r--', linewidth=2, label='True line: y = 3x + 2')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Training Data')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Our goal: Learn w≈3 and b≈2 from the data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 Linear Regression from Scratch\n",
    "\n",
    "### Demonstration: The Training Loop\n",
    "\n",
    "This is the core pattern for ALL deep learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMONSTRATION: Training Loop from Scratch\n",
    "# ============================================\n",
    "\n",
    "# Step 1: Initialize parameters randomly\n",
    "w = torch.randn(1, requires_grad=True)  # Random weight\n",
    "b = torch.zeros(1, requires_grad=True)  # Bias starts at 0\n",
    "\n",
    "print(f\"Initial parameters: w = {w.item():.4f}, b = {b.item():.4f}\")\n",
    "print()\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "\n",
    "# Store loss history for plotting\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # STEP 2: Forward pass - make predictions\n",
    "    y_pred = w * X + b\n",
    "    \n",
    "    # STEP 3: Compute loss (Mean Squared Error)\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "    \n",
    "    # STEP 4: Backward pass - compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # STEP 5: Update parameters (gradient descent)\n",
    "    with torch.no_grad():  # Don't track these operations\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        \n",
    "        # Clear gradients for next iteration\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Print progress every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch {epoch+1:3d}/{n_epochs} | Loss: {loss.item():.4f} | w: {w.item():.4f}, b: {b.item():.4f}')\n",
    "\n",
    "print()\n",
    "print(f\"Final parameters: w = {w.item():.4f}, b = {b.item():.4f}\")\n",
    "print(f\"True parameters:  w = 3.0000, b = 2.0000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "axes[0].plot(losses, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training Loss Over Time')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Learned line vs data\n",
    "axes[1].scatter(X.numpy(), y.numpy(), alpha=0.6, label='Data')\n",
    "X_line = torch.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_line = w.detach() * X_line + b.detach()\n",
    "axes[1].plot(X_line.numpy(), y_line.numpy(), 'r-', linewidth=2,\n",
    "             label=f'Learned: y = {w.item():.2f}x + {b.item():.2f}')\n",
    "axes[1].plot([0, 10], [2, 32], 'g--', linewidth=2, alpha=0.5, label='True: y = 3x + 2')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Linear Regression Fit')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop Pattern\n",
    "\n",
    "```python\n",
    "for epoch in range(n_epochs):\n",
    "    # 1. Forward pass: make prediction\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # 2. Compute loss\n",
    "    loss = loss_function(y_pred, y)\n",
    "    \n",
    "    # 3. Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # 4. Update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 5. Clear gradients\n",
    "    optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 Linear Regression with nn.Module\n",
    "\n",
    "PyTorch provides `nn.Module` to make building models easier.\n",
    "\n",
    "### Demonstration: Using nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMONSTRATION: nn.Module\n",
    "# ============================================\n",
    "\n",
    "# Define a model class\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # nn.Linear(input_features, output_features)\n",
    "        # This automatically creates w and b for us!\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define how data flows through the model\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create model instance\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer (handles the gradient descent for us)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Model structure:\")\n",
    "print(model)\n",
    "print()\n",
    "print(\"Initial parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with nn.Module (much cleaner!)\n",
    "n_epochs = 100\n",
    "losses_nn = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update parameters\n",
    "    \n",
    "    losses_nn.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch {epoch+1:3d}/{n_epochs} | Loss: {loss.item():.4f}')\n",
    "\n",
    "# Get learned parameters\n",
    "w_learned = model.linear.weight.item()\n",
    "b_learned = model.linear.bias.item()\n",
    "print(f\"\\nLearned: w = {w_learned:.4f}, b = {b_learned:.4f}\")\n",
    "print(f\"True:    w = 3.0000, b = 2.0000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Build Your Own Linear Regression Model\n",
    "\n",
    "**Your Task:** Complete the model class and training loop.\n",
    "\n",
    "**Expected:** The model should learn w ≈ 3 and b ≈ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 2.1: Build Linear Regression\n",
    "# ============================================\n",
    "\n",
    "# Reset random seed\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Define your model\n",
    "class MyLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE: Create a linear layer (1 input, 1 output)\n",
    "        self.linear = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE: Return the output of the linear layer\n",
    "        return None\n",
    "\n",
    "# Create model, loss function, and optimizer\n",
    "my_model = MyLinearRegression()\n",
    "my_criterion = None  # YOUR CODE HERE: nn.MSELoss()\n",
    "my_optimizer = None  # YOUR CODE HERE: optim.SGD(my_model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "my_losses = []\n",
    "for epoch in range(100):\n",
    "    # YOUR CODE HERE: Complete the training loop\n",
    "    # 1. Forward pass: y_pred = my_model(X)\n",
    "    # 2. Compute loss: loss = my_criterion(y_pred, y)\n",
    "    # 3. Zero gradients: my_optimizer.zero_grad()\n",
    "    # 4. Backward pass: loss.backward()\n",
    "    # 5. Update weights: my_optimizer.step()\n",
    "    pass\n",
    "\n",
    "# Print results\n",
    "if my_model.linear is not None:\n",
    "    print(f\"Learned: w = {my_model.linear.weight.item():.4f}, b = {my_model.linear.bias.item():.4f}\")\n",
    "    print(f\"True:    w = 3.0000, b = 2.0000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Logistic Regression (Classification)\n",
    "\n",
    "## What is Logistic Regression?\n",
    "\n",
    "For **binary classification** (two classes), we need to output a probability.\n",
    "\n",
    "**Key idea:** Use the **sigmoid function** to squash outputs to [0, 1]\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "- If σ(z) > 0.5 → Predict class 1\n",
    "- If σ(z) < 0.5 → Predict class 0\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Generate Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP: Generate Classification Data\n",
    "# ============================================\n",
    "\n",
    "# Create a classification dataset\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features (important for gradient descent!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train_scaled[y_train == 0, 0], X_train_scaled[y_train == 0, 1],\n",
    "            c='blue', label='Class 0', alpha=0.7, edgecolors='black')\n",
    "plt.scatter(X_train_scaled[y_train == 1, 0], X_train_scaled[y_train == 1, 1],\n",
    "            c='red', label='Class 1', alpha=0.7, edgecolors='black')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Binary Classification Dataset')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration: The Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMONSTRATION: Sigmoid Function\n",
    "# ============================================\n",
    "\n",
    "z = np.linspace(-10, 10, 100)\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(z, sigmoid, 'b-', linewidth=2)\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', label='Threshold = 0.5')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.fill_between(z, sigmoid, 0.5, where=(sigmoid > 0.5), alpha=0.3, color='green', label='Predict Class 1')\n",
    "plt.fill_between(z, sigmoid, 0.5, where=(sigmoid < 0.5), alpha=0.3, color='red', label='Predict Class 0')\n",
    "plt.xlabel('z (linear output)')\n",
    "plt.ylabel('σ(z) = Probability')\n",
    "plt.title('Sigmoid Function: σ(z) = 1 / (1 + e^(-z))')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"The sigmoid function:\")\n",
    "print(\"- Converts any number to a probability (0 to 1)\")\n",
    "print(\"- When z > 0, probability > 0.5 → Predict class 1\")\n",
    "print(\"- When z < 0, probability < 0.5 → Predict class 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.2 Building a Logistic Regression Model\n",
    "\n",
    "### Demonstration: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMONSTRATION: Logistic Regression Model\n",
    "# ============================================\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear transformation, then sigmoid\n",
    "        z = self.linear(x)\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "# Create model\n",
    "log_model = LogisticRegressionModel(input_dim=2)\n",
    "\n",
    "# Binary Cross-Entropy Loss (for classification)\n",
    "criterion_bce = nn.BCELoss()\n",
    "\n",
    "# Optimizer (Adam often works better than SGD)\n",
    "optimizer_log = optim.Adam(log_model.parameters(), lr=0.1)\n",
    "\n",
    "print(\"Model structure:\")\n",
    "print(log_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "n_epochs = 100\n",
    "losses_log = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    y_pred_prob = log_model(X_train_t)\n",
    "    loss = criterion_bce(y_pred_prob, y_train_t)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    y_pred_class = (y_pred_prob >= 0.5).float()\n",
    "    accuracy = (y_pred_class == y_train_t).float().mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer_log.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_log.step()\n",
    "    \n",
    "    losses_log.append(loss.item())\n",
    "    accuracies.append(accuracy.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch {epoch+1:3d}/{n_epochs} | Loss: {loss.item():.4f} | Accuracy: {accuracy.item():.4f}')\n",
    "\n",
    "# Test accuracy\n",
    "log_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = log_model(X_test_t)\n",
    "    y_test_class = (y_test_pred >= 0.5).float()\n",
    "    test_acc = (y_test_class == y_test_t).float().mean()\n",
    "\n",
    "print(f\"\\nTest Accuracy: {test_acc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(losses_log, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('BCE Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot(accuracies, 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "\n",
    "log_model.eval()\n",
    "with torch.no_grad():\n",
    "    Z = log_model(grid).numpy().reshape(xx.shape)\n",
    "\n",
    "axes[2].contourf(xx, yy, Z, levels=50, cmap='RdYlBu', alpha=0.8)\n",
    "axes[2].contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "axes[2].scatter(X_train_scaled[y_train == 0, 0], X_train_scaled[y_train == 0, 1],\n",
    "                c='blue', label='Class 0', edgecolors='black')\n",
    "axes[2].scatter(X_train_scaled[y_train == 1, 0], X_train_scaled[y_train == 1, 1],\n",
    "                c='red', label='Class 1', edgecolors='black')\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "axes[2].set_title(f'Decision Boundary (Test Acc: {test_acc.item():.2f})')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Build Your Own Classifier\n",
    "\n",
    "**Your Task:** Complete the logistic regression model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 3.1: Build a Classifier\n",
    "# ============================================\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class MyClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE: Create linear layer and sigmoid\n",
    "        self.linear = None\n",
    "        self.sigmoid = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE: Apply linear then sigmoid\n",
    "        return None\n",
    "\n",
    "# Create model, loss, and optimizer\n",
    "my_classifier = MyClassifier(input_dim=2)\n",
    "my_bce_loss = None    # YOUR CODE HERE: nn.BCELoss()\n",
    "my_opt = None         # YOUR CODE HERE: optim.Adam(my_classifier.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    # YOUR CODE HERE: Complete the training loop\n",
    "    pass\n",
    "\n",
    "# Test accuracy\n",
    "if my_classifier.linear is not None:\n",
    "    my_classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = my_classifier(X_test_t)\n",
    "        y_class = (y_pred >= 0.5).float()\n",
    "        acc = (y_class == y_test_t).float().mean()\n",
    "    print(f\"Test Accuracy: {acc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Support Vector Machines\n",
    "\n",
    "## What is an SVM?\n",
    "\n",
    "**Support Vector Machines** find the optimal hyperplane that maximizes the margin between classes.\n",
    "\n",
    "Key concepts:\n",
    "- **Margin:** Distance between the hyperplane and nearest points\n",
    "- **Support Vectors:** The points closest to the decision boundary\n",
    "- **Kernel Trick:** Transform data to make it linearly separable\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 Non-Linearly Separable Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP: Non-Linear Data (Moons)\n",
    "# ============================================\n",
    "\n",
    "# Generate \"moons\" data - two interleaving half circles\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1],\n",
    "            c='blue', label='Class 0', alpha=0.7, edgecolors='black')\n",
    "plt.scatter(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1],\n",
    "            c='red', label='Class 1', alpha=0.7, edgecolors='black')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Non-Linearly Separable Data (Moons)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"This data CANNOT be separated by a straight line!\")\n",
    "print(\"We need the kernel trick...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration: Comparing SVM Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMONSTRATION: SVM Kernels\n",
    "# ============================================\n",
    "\n",
    "# Train three SVMs with different kernels\n",
    "svm_linear = SVC(kernel='linear', C=1.0)\n",
    "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_poly = SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "# Fit all three\n",
    "svm_linear.fit(X_train_m, y_train_m)\n",
    "svm_rbf.fit(X_train_m, y_train_m)\n",
    "svm_poly.fit(X_train_m, y_train_m)\n",
    "\n",
    "# Compare accuracies\n",
    "print(\"Test Accuracies:\")\n",
    "print(f\"  Linear kernel: {svm_linear.score(X_test_m, y_test_m):.4f}\")\n",
    "print(f\"  RBF kernel:    {svm_rbf.score(X_test_m, y_test_m):.4f}\")\n",
    "print(f\"  Polynomial:    {svm_poly.score(X_test_m, y_test_m):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundaries\n",
    "def plot_svm(ax, model, X, y, title):\n",
    "    xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-0.5, X[:, 0].max()+0.5, 100),\n",
    "                          np.linspace(X[:, 1].min()-0.5, X[:, 1].max()+0.5, 100))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, levels=[-1, 0, 1], colors=['blue', 'red'], alpha=0.3)\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    ax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', edgecolors='black')\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='red', edgecolors='black')\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    sv = model.support_vectors_\n",
    "    ax.scatter(sv[:, 0], sv[:, 1], s=100, facecolors='none', edgecolors='green',\n",
    "               linewidths=2, label=f'Support Vectors ({len(sv)})')\n",
    "    \n",
    "    acc = model.score(X_test_m, y_test_m)\n",
    "    ax.set_title(f'{title}\\nTest Acc: {acc:.2f}')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "plot_svm(axes[0], svm_linear, X_train_m, y_train_m, 'Linear Kernel')\n",
    "plot_svm(axes[1], svm_rbf, X_train_m, y_train_m, 'RBF Kernel')\n",
    "plot_svm(axes[2], svm_poly, X_train_m, y_train_m, 'Polynomial Kernel')\n",
    "axes[0].legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice:\")\n",
    "print(\"- Linear kernel can't handle curved boundaries\")\n",
    "print(\"- RBF kernel creates a flexible, curved boundary\")\n",
    "print(\"- Support vectors (green circles) define the boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Train an SVM\n",
    "\n",
    "**Your Task:** Train an RBF SVM and experiment with the C parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 4.1: Train an SVM\n",
    "# ============================================\n",
    "\n",
    "# a) Create an RBF SVM with C=10\n",
    "my_svm = None  # YOUR CODE HERE: SVC(kernel='rbf', C=10)\n",
    "\n",
    "# b) Fit it on the training data\n",
    "# YOUR CODE HERE: my_svm.fit(X_train_m, y_train_m)\n",
    "\n",
    "# c) Calculate test accuracy\n",
    "if my_svm is not None:\n",
    "    accuracy = my_svm.score(X_test_m, y_test_m)\n",
    "    n_support = len(my_svm.support_vectors_)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Number of Support Vectors: {n_support}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Model Evaluation\n",
    "\n",
    "## Why Accuracy Isn't Enough\n",
    "\n",
    "Imagine a dataset with 95% class 0 and 5% class 1. A model that always predicts class 0 has 95% accuracy but is useless!\n",
    "\n",
    "We need better metrics:\n",
    "- **Precision:** Of all predicted positives, how many are correct?\n",
    "- **Recall:** Of all actual positives, how many did we find?\n",
    "- **F1 Score:** Harmonic mean of precision and recall\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 Confusion Matrix\n",
    "\n",
    "### Demonstration: Understanding the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMONSTRATION: Confusion Matrix\n",
    "# ============================================\n",
    "\n",
    "# Get predictions from our best SVM\n",
    "y_pred = svm_rbf.predict(X_test_m)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test_m, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "\n",
    "# Labels\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Predicted 0', 'Predicted 1'])\n",
    "ax.set_yticklabels(['Actual 0', 'Actual 1'])\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "ax.set_title('Confusion Matrix', fontsize=14)\n",
    "\n",
    "# Add numbers\n",
    "labels = [['TN', 'FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        color = 'white' if cm[i, j] > cm.max()/2 else 'black'\n",
    "        ax.text(j, i, f'{cm[i, j]}\\n({labels[i][j]})', \n",
    "                ha='center', va='center', fontsize=16, color=color)\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Legend:\")\n",
    "print(\"  TN (True Negative): Correctly predicted negative\")\n",
    "print(\"  FP (False Positive): Incorrectly predicted positive (Type I error)\")\n",
    "print(\"  FN (False Negative): Incorrectly predicted negative (Type II error)\")\n",
    "print(\"  TP (True Positive): Correctly predicted positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration: Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMONSTRATION: Calculating Metrics\n",
    "# ============================================\n",
    "\n",
    "# Extract values from confusion matrix\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "print(\"From the confusion matrix:\")\n",
    "print(f\"  TN = {TN}, FP = {FP}\")\n",
    "print(f\"  FN = {FN}, TP = {TP}\")\n",
    "print()\n",
    "\n",
    "# Calculate metrics manually\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}  = (TP + TN) / Total\")\n",
    "print(f\"  Precision: {precision:.4f}  = TP / (TP + FP)  'Of predicted positive, how many correct?'\")\n",
    "print(f\"  Recall:    {recall:.4f}  = TP / (TP + FN)  'Of actual positive, how many found?'\")\n",
    "print(f\"  F1 Score:  {f1:.4f}  = 2 * P * R / (P + R)\")\n",
    "print()\n",
    "\n",
    "# Using sklearn\n",
    "print(\"Classification Report (sklearn):\")\n",
    "print(classification_report(y_test_m, y_pred, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Evaluate Your Model\n",
    "\n",
    "**Your Task:** Calculate the confusion matrix and metrics for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 5.1: Evaluate Your Model\n",
    "# ============================================\n",
    "\n",
    "# Get predictions from the logistic regression model\n",
    "log_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_prob = log_model(X_test_t)\n",
    "    y_pred_log = (y_pred_prob >= 0.5).numpy().astype(int).flatten()\n",
    "\n",
    "# a) Create confusion matrix\n",
    "cm_log = None  # YOUR CODE HERE: confusion_matrix(y_test, y_pred_log)\n",
    "\n",
    "# b) Print classification report\n",
    "# YOUR CODE HERE: print(classification_report(y_test, y_pred_log))\n",
    "\n",
    "if cm_log is not None:\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lab Complete!\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Topic | Key Concepts |\n",
    "|-------|-------------|\n",
    "| **PyTorch Tensors** | `torch.tensor()`, `requires_grad=True`, operations |\n",
    "| **Autograd** | `.backward()`, automatic gradient computation |\n",
    "| **Linear Regression** | MSE loss, gradient descent, `nn.Linear` |\n",
    "| **Logistic Regression** | Sigmoid, BCE loss, binary classification |\n",
    "| **SVM** | Kernels (linear, RBF, poly), margin, support vectors |\n",
    "| **Evaluation** | Confusion matrix, precision, recall, F1-score |\n",
    "\n",
    "## The PyTorch Training Loop\n",
    "\n",
    "```python\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = model(X)           # Forward pass\n",
    "    loss = criterion(y_pred, y) # Compute loss\n",
    "    optimizer.zero_grad()       # Clear gradients\n",
    "    loss.backward()             # Backward pass\n",
    "    optimizer.step()            # Update weights\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Practice:** Try different learning rates and see how training changes\n",
    "- **Experiment:** Use different kernels and C values for SVM\n",
    "- **Next Lab:** Neural Networks and Deep Learning!\n",
    "\n",
    "---\n",
    "\n",
    "*Great work! Save your notebook (Ctrl+S) before closing.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}