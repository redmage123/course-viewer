{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Data Exploration & Visualization\n",
    "\n",
    "**Introduction to Data Science & Engineering - Day 1**\n",
    "\n",
    "| Duration | Difficulty | Framework | Exercises |\n",
    "|---|---|---|---|\n",
    "| 90 min | Beginner | pandas, matplotlib, seaborn | 5 |\n",
    "\n",
    "In this lab, you'll practice:\n",
    "- Loading and exploring datasets\n",
    "- Identifying data quality issues\n",
    "- Computing descriptive statistics\n",
    "- Creating visualizations with matplotlib and seaborn\n",
    "- Analyzing correlations\n",
    "- Customer segmentation\n",
    "- Time series patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Create and Load the Dataset\n",
    "\n",
    "We'll work with a synthetic e-commerce dataset with deliberate quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "\n",
    "# Generate dates over 2 years\n",
    "start_date = datetime(2023, 1, 1)\n",
    "dates = [start_date + timedelta(days=np.random.randint(0, 730)) for _ in range(n_samples)]\n",
    "\n",
    "# Customer segments\n",
    "segments = np.random.choice(['Premium', 'Standard', 'Basic', None], n_samples, p=[0.2, 0.4, 0.3, 0.1])\n",
    "\n",
    "# Product categories\n",
    "categories = np.random.choice(['Electronics', 'Clothing', 'Home & Garden', 'Books', 'Sports'], n_samples)\n",
    "\n",
    "data = {\n",
    "    'order_id': range(1, n_samples + 1),\n",
    "    'customer_id': np.random.randint(100, 600, n_samples),\n",
    "    'order_date': dates,\n",
    "    'product_category': categories,\n",
    "    'quantity': np.random.randint(1, 10, n_samples),\n",
    "    'unit_price': np.round(np.random.uniform(5, 500, n_samples), 2),\n",
    "    'customer_segment': segments,\n",
    "    'customer_age': np.random.randint(18, 75, n_samples).astype(float),\n",
    "    'satisfaction_score': np.random.choice([1, 2, 3, 4, 5, np.nan], n_samples, p=[0.05, 0.1, 0.2, 0.35, 0.2, 0.1]),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n",
    "    'is_returned': np.random.choice([0, 1], n_samples, p=[0.85, 0.15])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Inject quality issues\n",
    "# Missing values\n",
    "df.loc[np.random.choice(df.index, 80, replace=False), 'customer_age'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 40, replace=False), 'unit_price'] = np.nan\n",
    "\n",
    "# Outliers\n",
    "df.loc[np.random.choice(df.index, 10, replace=False), 'unit_price'] = np.random.uniform(2000, 5000, 10)\n",
    "df.loc[np.random.choice(df.index, 5, replace=False), 'quantity'] = np.random.randint(50, 100, 5)\n",
    "\n",
    "# Duplicates\n",
    "dup_indices = np.random.choice(df.index, 15, replace=False)\n",
    "duplicates = df.loc[dup_indices].copy()\n",
    "df = pd.concat([df, duplicates], ignore_index=True)\n",
    "\n",
    "# Calculate total_amount\n",
    "df['total_amount'] = df['quantity'] * df['unit_price']\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Basic Exploration\n",
    "\n",
    "Explore the dataset structure using pandas methods.\n",
    "\n",
    "**Your Task:** Use pandas methods to examine the shape, data types, descriptive statistics, missing values, and duplicates in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print the shape and data types of the dataset\n",
    "# Hint: Use df.shape and df.dtypes\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get descriptive statistics for all numeric columns\n",
    "# Hint: Use df.describe()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check for missing values and compute the percentage missing per column\n",
    "# Hint: Use df.isnull().sum() and divide by len(df)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check for duplicate rows and duplicate order_ids\n",
    "# Hint: Use df.duplicated().sum() and df['order_id'].duplicated().sum()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Quality Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Handle Missing Values\n",
    "\n",
    "**Your Task:** Implement a function that fills missing values using median imputation for numeric columns and 'Unknown' for categorical columns. After imputation, recalculate the `total_amount` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    \"\"\"Handle missing values in the dataset.\n",
    "    \n",
    "    Strategy: median for numeric columns, 'Unknown' for categorical.\n",
    "    After imputation, recalculate total_amount.\n",
    "    \n",
    "    Returns: cleaned DataFrame\n",
    "    \"\"\"\n",
    "    # TODO: Fill customer_age with median\n",
    "    # TODO: Fill unit_price with median\n",
    "    # TODO: Fill satisfaction_score with median\n",
    "    # TODO: Fill customer_segment with 'Unknown'\n",
    "    # TODO: Recalculate total_amount = quantity * unit_price\n",
    "    pass\n",
    "\n",
    "df = handle_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Remove Duplicates\n",
    "\n",
    "**Your Task:** Implement a function that removes duplicate orders based on `order_id`, keeping the first occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    \"\"\"Remove duplicate orders, keeping the first occurrence.\n",
    "    \n",
    "    Returns: deduplicated DataFrame and count of removed rows\n",
    "    \"\"\"\n",
    "    # TODO: Remove duplicates based on order_id, keep first\n",
    "    # TODO: Return cleaned df and count of removed rows\n",
    "    pass\n",
    "\n",
    "df, removed = remove_duplicates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Detect Outliers\n",
    "\n",
    "**Your Task:** Implement the IQR method for outlier detection. For each numeric column, calculate Q1, Q3, and IQR, then identify values outside the bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(series, factor=1.5):\n",
    "    \"\"\"Detect outliers using the IQR method.\n",
    "    \n",
    "    Args:\n",
    "        series: pandas Series to check\n",
    "        factor: IQR multiplier (default 1.5)\n",
    "    \n",
    "    Returns: tuple of (outlier_series, lower_bound, upper_bound)\n",
    "    \"\"\"\n",
    "    # TODO: Calculate Q1, Q3, and IQR\n",
    "    # TODO: Compute lower and upper bounds\n",
    "    # TODO: Filter series for values outside bounds\n",
    "    pass\n",
    "\n",
    "# Test on unit_price, quantity, total_amount\n",
    "for col in ['unit_price', 'quantity', 'total_amount']:\n",
    "    result = detect_outliers_iqr(df[col])\n",
    "    if result:\n",
    "        outliers, lower, upper = result\n",
    "        print(f\"{col}: {len(outliers)} outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers with box plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for ax, col in zip(axes, ['unit_price', 'quantity', 'total_amount']):\n",
    "    sns.boxplot(data=df, y=col, ax=ax, color='#3b82f6')\n",
    "    ax.set_title(f'{col} Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Distribution Analysis\n",
    "\n",
    "**Your Task:** Create a 2x2 subplot grid showing the distributions of `unit_price`, `customer_age`, `quantity`, and `satisfaction_score`. Use `sns.histplot` with `kde=True` for continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions(df):\n",
    "    \"\"\"Plot distributions of key numeric features.\n",
    "    \n",
    "    Create a 2x2 subplot with histograms for:\n",
    "    - unit_price, customer_age, quantity, satisfaction_score\n",
    "    Use sns.histplot with kde=True for continuous variables.\n",
    "    \"\"\"\n",
    "    # TODO: Create 2x2 subplot figure (14, 10)\n",
    "    # TODO: Plot histograms for each feature\n",
    "    # TODO: Set titles and call plt.tight_layout()\n",
    "    pass\n",
    "\n",
    "plot_distributions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Correlation Analysis\n",
    "\n",
    "**Your Task:** Compute the correlation matrix for all numeric columns and visualize it as a heatmap using `sns.heatmap` with annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(df):\n",
    "    \"\"\"Compute and visualize the correlation matrix for numeric columns.\n",
    "    \n",
    "    Use sns.heatmap with annot=True, cmap='coolwarm'.\n",
    "    \"\"\"\n",
    "    # TODO: Select numeric columns\n",
    "    # TODO: Compute correlation matrix\n",
    "    # TODO: Create heatmap visualization\n",
    "    pass\n",
    "\n",
    "plot_correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Category Analysis\n",
    "\n",
    "**Your Task:** Analyze sales by product category. Create two side-by-side plots: a horizontal bar chart of total revenue by category, and a pie chart of order distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categories(df):\n",
    "    \"\"\"Analyze sales by product category.\n",
    "    \n",
    "    Create two plots side by side:\n",
    "    1. Horizontal bar chart of total revenue by category\n",
    "    2. Pie chart of order distribution by category\n",
    "    \"\"\"\n",
    "    # TODO: Group by product_category and sum total_amount\n",
    "    # TODO: Create 1x2 subplot with barh and pie chart\n",
    "    pass\n",
    "\n",
    "analyze_categories(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Customer Segmentation Analysis\n",
    "\n",
    "**Your Task:** Analyze customer segments by creating two plots: average order value by segment (horizontal bar), and satisfaction score distribution by segment (box plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_segments(df):\n",
    "    \"\"\"Analyze customer segments.\n",
    "    \n",
    "    Create two plots:\n",
    "    1. Average order value by segment (barh)\n",
    "    2. Satisfaction score distribution by segment (boxplot)\n",
    "    \"\"\"\n",
    "    # TODO: Compute segment statistics\n",
    "    # TODO: Create 1x2 subplot with barh and boxplot\n",
    "    pass\n",
    "\n",
    "analyze_segments(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3: Time Series Analysis\n",
    "\n",
    "**Your Task:** Analyze trends over time. Create two stacked plots: monthly revenue trend (line) and monthly order count (bar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_time_series(df):\n",
    "    \"\"\"Analyze trends over time.\n",
    "    \n",
    "    Create two plots stacked vertically:\n",
    "    1. Monthly revenue trend (line plot)\n",
    "    2. Monthly order count (bar chart)\n",
    "    \n",
    "    Hint: Use df['order_date'].dt.to_period('M') for monthly grouping.\n",
    "    \"\"\"\n",
    "    # TODO: Create order_month from order_date\n",
    "    # TODO: Aggregate revenue and order count by month\n",
    "    # TODO: Create 2x1 subplot with line and bar charts\n",
    "    pass\n",
    "\n",
    "analyze_time_series(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.4: Regional Analysis\n",
    "\n",
    "**Your Task:** Analyze performance by region. Create two horizontal bar charts: total revenue by region and return rate by region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_regions(df):\n",
    "    \"\"\"Analyze performance by region.\n",
    "    \n",
    "    Create two plots:\n",
    "    1. Total revenue by region (barh)\n",
    "    2. Return rate by region (barh)\n",
    "    \"\"\"\n",
    "    # TODO: Group by region for revenue and return rate\n",
    "    # TODO: Create 1x2 subplot\n",
    "    pass\n",
    "\n",
    "analyze_regions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Advanced Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Multi-dimensional Analysis\n",
    "\n",
    "**Your Task:** Create a comprehensive 2x2 visualization combining scatter plots, heatmaps, bar charts, and violin plots to explore relationships across multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_dimensional_analysis(df):\n",
    "    \"\"\"Create a comprehensive 2x2 multi-dimensional visualization.\n",
    "    \n",
    "    Plots:\n",
    "    1. Scatter: Age vs Total Amount, colored by customer_segment\n",
    "    2. Heatmap: Average order value by Category x Region\n",
    "    3. Bar: Return rate by product category\n",
    "    4. Violin: Satisfaction score by return status\n",
    "    \"\"\"\n",
    "    # TODO: Create 2x2 subplot figure (16, 12)\n",
    "    # TODO: Implement each of the 4 visualizations\n",
    "    pass\n",
    "\n",
    "multi_dimensional_analysis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Generate and load** synthetic datasets with realistic quality issues\n",
    "2. **Assess data quality** -- missing values, duplicates, outliers\n",
    "3. **Clean data** using imputation, deduplication, and outlier detection\n",
    "4. **Compute statistics** and analyze distributions\n",
    "5. **Visualize data** with bar charts, histograms, scatter plots, heatmaps\n",
    "6. **Analyze trends** over time using time series grouping\n",
    "7. **Segment customers** and compare across dimensions\n",
    "\n",
    "---\n",
    "\n",
    "*Introduction to Data Science & Engineering | AI Elevate*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}