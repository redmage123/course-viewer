{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sol_title",
   "metadata": {},
   "source": [
    "# Lab 2: Building a Data Pipeline - SOLUTIONS\n",
    "\n",
    "**Introduction to Data Science & Engineering - Day 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sol_setup_md",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sol_extract_md",
   "metadata": {},
   "source": [
    "## Part 1: Extract — Source Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_orders",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 1: Orders data (simulating CSV)\n",
    "np.random.seed(42)\n",
    "n_orders = 1500\n",
    "\n",
    "start_date = datetime(2024, 1, 1)\n",
    "order_dates = [start_date + timedelta(days=np.random.randint(0, 365)) for _ in range(n_orders)]\n",
    "\n",
    "orders_data = {\n",
    "    'order_id': range(1001, 1001 + n_orders),\n",
    "    'customer_id': np.random.randint(1, 201, n_orders),\n",
    "    'product_id': np.random.randint(1, 51, n_orders),\n",
    "    'order_date': order_dates,\n",
    "    'quantity': np.random.randint(1, 8, n_orders),\n",
    "    'unit_price': np.round(np.random.uniform(10, 300, n_orders), 2),\n",
    "    'discount_pct': np.random.choice([0, 5, 10, 15, 20], n_orders, p=[0.5, 0.2, 0.15, 0.1, 0.05]),\n",
    "    'store_id': np.random.randint(1, 6, n_orders)\n",
    "}\n",
    "\n",
    "orders_df = pd.DataFrame(orders_data)\n",
    "\n",
    "# Inject some quality issues\n",
    "orders_df.loc[np.random.choice(orders_df.index, 30, replace=False), 'unit_price'] = np.nan\n",
    "orders_df.loc[np.random.choice(orders_df.index, 10, replace=False), 'quantity'] = -1\n",
    "orders_df.loc[np.random.choice(orders_df.index, 5, replace=False), 'customer_id'] = 999  # Invalid customer\n",
    "\n",
    "print(f\"Orders extracted: {len(orders_df)} rows\")\n",
    "orders_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_customers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 2: Customer data (simulating JSON API response)\n",
    "customers_json = {\n",
    "    \"customers\": [\n",
    "        {\n",
    "            \"customer_id\": i,\n",
    "            \"name\": f\"Customer_{i:03d}\",\n",
    "            \"email\": f\"customer{i}@email.com\",\n",
    "            \"segment\": np.random.choice(['Premium', 'Standard', 'Basic']),\n",
    "            \"join_date\": (datetime(2020, 1, 1) + timedelta(days=np.random.randint(0, 1460))).strftime('%Y-%m-%d'),\n",
    "            \"city\": np.random.choice(['Dublin', 'Cork', 'Galway', 'Limerick', 'Waterford']),\n",
    "            \"country\": \"Ireland\"\n",
    "        }\n",
    "        for i in range(1, 201)\n",
    "    ]\n",
    "}\n",
    "\n",
    "customers_df = pd.DataFrame(customers_json['customers'])\n",
    "customers_df['join_date'] = pd.to_datetime(customers_df['join_date'])\n",
    "\n",
    "print(f\"Customers extracted: {len(customers_df)} rows\")\n",
    "customers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_products",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 3: Product catalog (simulating dict/API)\n",
    "categories = ['Electronics', 'Clothing', 'Home & Garden', 'Books', 'Sports']\n",
    "products = []\n",
    "for i in range(1, 51):\n",
    "    cat = categories[(i - 1) % len(categories)]\n",
    "    products.append({\n",
    "        'product_id': i,\n",
    "        'product_name': f'{cat}_Item_{i:03d}',\n",
    "        'category': cat,\n",
    "        'brand': np.random.choice(['BrandA', 'BrandB', 'BrandC', 'BrandD']),\n",
    "        'cost_price': round(np.random.uniform(5, 150), 2),\n",
    "        'weight_kg': round(np.random.uniform(0.1, 20), 1)\n",
    "    })\n",
    "\n",
    "products_df = pd.DataFrame(products)\n",
    "print(f\"Products extracted: {len(products_df)} rows\")\n",
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_stores",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 4: Store reference data\n",
    "stores_data = {\n",
    "    'store_id': [1, 2, 3, 4, 5],\n",
    "    'store_name': ['Dublin Central', 'Cork Main', 'Galway West', 'Online', 'Limerick Hub'],\n",
    "    'store_type': ['Physical', 'Physical', 'Physical', 'Online', 'Physical'],\n",
    "    'region': ['East', 'South', 'West', 'National', 'South']\n",
    "}\n",
    "stores_df = pd.DataFrame(stores_data)\n",
    "print(f\"Stores extracted: {len(stores_df)} rows\")\n",
    "stores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sol_ex21_md",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Quality Assertions - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_ex21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_quality_checks(df, checks, source_name):\n",
    "    \"\"\"Run a list of quality checks and report results.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Quality Report: {source_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    all_passed = True\n",
    "    for check_name, check_fn, is_critical in checks:\n",
    "        try:\n",
    "            result = check_fn(df)\n",
    "            status = \"PASS\" if result else \"FAIL\"\n",
    "            marker = \"\\u2713\" if result else \"\\u2717\"\n",
    "            level = \"CRITICAL\" if (not result and is_critical) else \"\"\n",
    "            print(f\"  {marker} {check_name}: {status} {level}\")\n",
    "            if not result and is_critical:\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            print(f\"  \\u2717 {check_name}: ERROR - {e}\")\n",
    "            all_passed = False\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "order_checks = [\n",
    "    (\"Not empty\", lambda df: len(df) > 0, True),\n",
    "    (\"No null order_ids\", lambda df: df['order_id'].notna().all(), True),\n",
    "    (\"Unique order_ids\", lambda df: df['order_id'].is_unique, True),\n",
    "    (\"No null prices\", lambda df: df['unit_price'].notna().all(), False),\n",
    "    (\"Positive quantities\", lambda df: (df['quantity'] > 0).all(), False),\n",
    "    (\"Valid customer_ids (1-200)\", lambda df: df['customer_id'].between(1, 200).all(), False),\n",
    "    (\"Valid dates\", lambda df: pd.to_datetime(df['order_date'], errors='coerce').notna().all(), True),\n",
    "]\n",
    "\n",
    "orders_valid = run_quality_checks(orders_df, order_checks, \"Orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_ex21_cust",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_checks = [\n",
    "    (\"Not empty\", lambda df: len(df) > 0, True),\n",
    "    (\"Unique customer_ids\", lambda df: df['customer_id'].is_unique, True),\n",
    "    (\"No null emails\", lambda df: df['email'].notna().all(), True),\n",
    "    (\"Valid segments\", lambda df: df['segment'].isin(['Premium', 'Standard', 'Basic']).all(), True),\n",
    "]\n",
    "\n",
    "customers_valid = run_quality_checks(customers_df, customer_checks, \"Customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sol_ex31_md",
   "metadata": {},
   "source": [
    "## Exercise 3.1: Clean Orders - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_ex31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Orders before cleaning: {len(orders_df)}\")\n",
    "\n",
    "# 1. Handle missing prices - fill with product median\n",
    "orders_clean = orders_df.copy()\n",
    "median_price = orders_clean['unit_price'].median()\n",
    "orders_clean['unit_price'].fillna(median_price, inplace=True)\n",
    "\n",
    "# 2. Fix negative quantities\n",
    "orders_clean.loc[orders_clean['quantity'] <= 0, 'quantity'] = 1\n",
    "\n",
    "# 3. Remove invalid customer_ids\n",
    "orders_clean = orders_clean[orders_clean['customer_id'].between(1, 200)]\n",
    "\n",
    "print(f\"Orders after cleaning: {len(orders_clean)}\")\n",
    "print(f\"Removed {len(orders_df) - len(orders_clean)} invalid rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sol_ex32_md",
   "metadata": {},
   "source": [
    "## Exercise 3.2: Enrich and Derive Fields - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_ex32",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_clean['order_date'] = pd.to_datetime(orders_clean['order_date'])\n",
    "\n",
    "# Revenue calculations\n",
    "orders_clean['gross_amount'] = orders_clean['quantity'] * orders_clean['unit_price']\n",
    "orders_clean['discount_amount'] = orders_clean['gross_amount'] * orders_clean['discount_pct'] / 100\n",
    "orders_clean['net_amount'] = orders_clean['gross_amount'] - orders_clean['discount_amount']\n",
    "\n",
    "# Date dimensions\n",
    "orders_clean['order_year'] = orders_clean['order_date'].dt.year\n",
    "orders_clean['order_month'] = orders_clean['order_date'].dt.month\n",
    "orders_clean['order_quarter'] = orders_clean['order_date'].dt.quarter\n",
    "orders_clean['order_day_of_week'] = orders_clean['order_date'].dt.dayofweek\n",
    "orders_clean['is_weekend'] = orders_clean['order_day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "print(\"Derived fields created:\")\n",
    "orders_clean[['order_id', 'gross_amount', 'discount_amount', 'net_amount', 'order_quarter', 'is_weekend']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sol_ex41_md",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Create Dimension Tables - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_dim_cust",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_customers\n",
    "dim_customers = customers_df.copy()\n",
    "dim_customers = dim_customers.rename(columns={'customer_id': 'customer_key'})\n",
    "\n",
    "dim_customers['tenure_days'] = (pd.Timestamp('2024-12-31') - dim_customers['join_date']).dt.days\n",
    "dim_customers['tenure_years'] = (dim_customers['tenure_days'] / 365).round(1)\n",
    "\n",
    "print(f\"dim_customers: {dim_customers.shape}\")\n",
    "dim_customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_dim_prod",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_products\n",
    "dim_products = products_df.copy()\n",
    "dim_products = dim_products.rename(columns={'product_id': 'product_key'})\n",
    "\n",
    "dim_products['margin_tier'] = pd.cut(\n",
    "    dim_products['cost_price'],\n",
    "    bins=[0, 30, 80, 200],\n",
    "    labels=['Low Cost', 'Mid Cost', 'High Cost']\n",
    ")\n",
    "\n",
    "print(f\"dim_products: {dim_products.shape}\")\n",
    "dim_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_dim_time",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_time\n",
    "date_range = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')\n",
    "dim_time = pd.DataFrame({\n",
    "    'date_key': date_range,\n",
    "    'year': date_range.year,\n",
    "    'quarter': date_range.quarter,\n",
    "    'month': date_range.month,\n",
    "    'month_name': date_range.strftime('%B'),\n",
    "    'week': date_range.isocalendar().week.astype(int),\n",
    "    'day_of_week': date_range.dayofweek,\n",
    "    'day_name': date_range.strftime('%A'),\n",
    "    'is_weekend': date_range.dayofweek.isin([5, 6]).astype(int),\n",
    "})\n",
    "\n",
    "print(f\"dim_time: {dim_time.shape}\")\n",
    "dim_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_dim_stores",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_stores (already clean)\n",
    "dim_stores = stores_df.copy()\n",
    "dim_stores = dim_stores.rename(columns={'store_id': 'store_key'})\n",
    "\n",
    "print(f\"dim_stores: {dim_stores.shape}\")\n",
    "dim_stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sol_ex42_md",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Create Fact Table - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_ex42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_orders = orders_clean[['order_id', 'customer_id', 'product_id', 'order_date', 'store_id',\n",
    "                             'quantity', 'unit_price', 'discount_pct',\n",
    "                             'gross_amount', 'discount_amount', 'net_amount']].copy()\n",
    "\n",
    "fact_orders = fact_orders.rename(columns={\n",
    "    'customer_id': 'customer_key',\n",
    "    'product_id': 'product_key',\n",
    "    'order_date': 'date_key',\n",
    "    'store_id': 'store_key'\n",
    "})\n",
    "\n",
    "print(f\"fact_orders: {fact_orders.shape}\")\n",
    "print(f\"Total Revenue: ${fact_orders['net_amount'].sum():,.2f}\")\n",
    "fact_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_validate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate referential integrity\n",
    "print(\"Star Schema Validation:\")\n",
    "print(f\"  Fact rows: {len(fact_orders)}\")\n",
    "print(f\"  Unique customers in fact: {fact_orders['customer_key'].nunique()}\")\n",
    "print(f\"  Unique products in fact: {fact_orders['product_key'].nunique()}\")\n",
    "\n",
    "orphan_customers = set(fact_orders['customer_key']) - set(dim_customers['customer_key'])\n",
    "orphan_products = set(fact_orders['product_key']) - set(dim_products['product_key'])\n",
    "orphan_stores = set(fact_orders['store_key']) - set(dim_stores['store_key'])\n",
    "\n",
    "print(f\"  Orphan customer keys: {len(orphan_customers)}\")\n",
    "print(f\"  Orphan product keys: {len(orphan_products)}\")\n",
    "print(f\"  Orphan store keys: {len(orphan_stores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sol_ex51_md",
   "metadata": {},
   "source": [
    "## Exercise 5.1: Load into SQLite - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_ex51",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = 'ecommerce_warehouse.db'\n",
    "\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Load tables\n",
    "fact_orders_sql = fact_orders.copy()\n",
    "fact_orders_sql['date_key'] = fact_orders_sql['date_key'].astype(str)\n",
    "\n",
    "fact_orders_sql.to_sql('fact_orders', conn, index=False, if_exists='replace')\n",
    "dim_customers.to_sql('dim_customers', conn, index=False, if_exists='replace')\n",
    "dim_products.to_sql('dim_products', conn, index=False, if_exists='replace')\n",
    "dim_time_sql = dim_time.copy()\n",
    "dim_time_sql['date_key'] = dim_time_sql['date_key'].astype(str)\n",
    "dim_time_sql.to_sql('dim_time', conn, index=False, if_exists='replace')\n",
    "dim_stores.to_sql('dim_stores', conn, index=False, if_exists='replace')\n",
    "\n",
    "# Verify\n",
    "for table in ['fact_orders', 'dim_customers', 'dim_products', 'dim_time', 'dim_stores']:\n",
    "    count = pd.read_sql(f\"SELECT COUNT(*) as cnt FROM {table}\", conn).iloc[0, 0]\n",
    "    print(f\"  {table}: {count} rows\")\n",
    "\n",
    "print(\"\\nSQLite database created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sol_ex52_md",
   "metadata": {},
   "source": [
    "## Exercise 5.2: Load into Parquet - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_ex52",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_dir = 'warehouse_parquet'\n",
    "os.makedirs(parquet_dir, exist_ok=True)\n",
    "\n",
    "fact_orders.to_parquet(f'{parquet_dir}/fact_orders.parquet', index=False)\n",
    "dim_customers.to_parquet(f'{parquet_dir}/dim_customers.parquet', index=False)\n",
    "dim_products.to_parquet(f'{parquet_dir}/dim_products.parquet', index=False)\n",
    "dim_time.to_parquet(f'{parquet_dir}/dim_time.parquet', index=False)\n",
    "dim_stores.to_parquet(f'{parquet_dir}/dim_stores.parquet', index=False)\n",
    "\n",
    "print(\"Parquet files created:\")\n",
    "for f in os.listdir(parquet_dir):\n",
    "    size = os.path.getsize(os.path.join(parquet_dir, f))\n",
    "    print(f\"  {f}: {size/1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sol_ex61_md",
   "metadata": {},
   "source": [
    "## Exercise 6.1: Revenue Analytics - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly revenue trend\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    t.month_name,\n",
    "    t.month,\n",
    "    COUNT(f.order_id) AS total_orders,\n",
    "    ROUND(SUM(f.net_amount), 2) AS total_revenue,\n",
    "    ROUND(AVG(f.net_amount), 2) AS avg_order_value\n",
    "FROM fact_orders f\n",
    "JOIN dim_time t ON DATE(f.date_key) = DATE(t.date_key)\n",
    "GROUP BY t.month, t.month_name\n",
    "ORDER BY t.month\n",
    "\"\"\"\n",
    "monthly_revenue = pd.read_sql(query, conn)\n",
    "print(monthly_revenue.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_top_cust",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top customers by revenue\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    c.name,\n",
    "    c.segment,\n",
    "    c.city,\n",
    "    COUNT(f.order_id) AS orders,\n",
    "    ROUND(SUM(f.net_amount), 2) AS total_spent,\n",
    "    ROUND(AVG(f.net_amount), 2) AS avg_order\n",
    "FROM fact_orders f\n",
    "JOIN dim_customers c ON f.customer_key = c.customer_key\n",
    "GROUP BY c.customer_key\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "top_customers = pd.read_sql(query, conn)\n",
    "print(top_customers.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_store_cat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category performance by store\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    s.store_name,\n",
    "    p.category,\n",
    "    COUNT(f.order_id) AS orders,\n",
    "    ROUND(SUM(f.net_amount), 2) AS revenue,\n",
    "    ROUND(AVG(f.discount_pct), 1) AS avg_discount\n",
    "FROM fact_orders f\n",
    "JOIN dim_stores s ON f.store_key = s.store_key\n",
    "JOIN dim_products p ON f.product_key = p.product_key\n",
    "GROUP BY s.store_name, p.category\n",
    "ORDER BY s.store_name, revenue DESC\n",
    "\"\"\"\n",
    "store_category = pd.read_sql(query, conn)\n",
    "print(store_category.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sol_cleanup_md",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sol_cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection and clean up\n",
    "conn.close()\n",
    "\n",
    "# Clean up temporary files\n",
    "import shutil\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "if os.path.exists(parquet_dir):\n",
    "    shutil.rmtree(parquet_dir)\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sol_summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Extract** data from multiple source formats (CSV, JSON, dict)\n",
    "2. **Validate** data quality with assertion-based checks\n",
    "3. **Transform** data — cleaning, enrichment, derived fields\n",
    "4. **Model** data into a star schema (fact + dimension tables)\n",
    "5. **Load** into both SQLite (for SQL queries) and Parquet (for analytics)\n",
    "6. **Analyze** with SQL joins across the star schema\n",
    "\n",
    "---\n",
    "\n",
    "*Introduction to Data Science & Engineering | AI Elevate*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}