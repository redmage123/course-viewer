{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: QLoRA Fine-Tuning with Mistral-7B\n",
    "\n",
    "In this hands-on lab, you'll fine-tune the Mistral-7B model using QLoRA on a custom instruction dataset.\n",
    "\n",
    "## Learning Objectives\n",
    "- Load a model with 4-bit quantization\n",
    "- Configure LoRA adapters\n",
    "- Prepare training data in the correct format\n",
    "- Train using SFTTrainer\n",
    "- Save and test the fine-tuned model\n",
    "\n",
    "## Requirements\n",
    "- GPU with 16GB+ VRAM (or use Google Colab with T4/A100)\n",
    "- Python 3.10+\n",
    "- PyTorch 2.0+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Quantization\n",
    "\n",
    "We'll use 4-bit quantization with NormalFloat4 (nf4) format to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BitsAndBytes configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                     # Load model in 4-bit\n",
    "    bnb_4bit_quant_type=\"nf4\",             # Use NormalFloat4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Compute in bfloat16\n",
    "    bnb_4bit_use_double_quant=True,        # Nested quantization for extra savings\n",
    ")\n",
    "\n",
    "print(\"Quantization config created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load the Base Model\n",
    "\n",
    "We'll load Mistral-7B with our quantization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name - you can change this to other models\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: vocab size = {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"Model loaded! Total parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configure LoRA\n",
    "\n",
    "LoRA adds small trainable matrices to the attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                                       # Rank of the update matrices\n",
    "    lora_alpha=32,                              # Scaling factor\n",
    "    target_modules=[                            # Which modules to adapt\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,                          # Dropout for regularization\n",
    "    bias=\"none\",                                # Don't train biases\n",
    "    task_type=\"CAUSAL_LM\",                      # Task type\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Prepare Training Data\n",
    "\n",
    "We'll create a small sample dataset for demonstration. In practice, you'd load your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data - Replace with your own dataset\n",
    "training_data = [\n",
    "    {\n",
    "        \"instruction\": \"Explain machine learning in simple terms.\",\n",
    "        \"response\": \"Machine learning is a type of artificial intelligence where computers learn patterns from data instead of being explicitly programmed. Think of it like teaching a child to recognize cats by showing them many pictures of cats, rather than giving them a list of rules about what makes a cat.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the difference between supervised and unsupervised learning?\",\n",
    "        \"response\": \"Supervised learning uses labeled data where we know the correct answers, like predicting house prices from features. Unsupervised learning finds patterns in unlabeled data, like grouping customers by behavior without predefined categories.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain what a neural network is.\",\n",
    "        \"response\": \"A neural network is a computing system inspired by biological brains. It consists of layers of interconnected nodes (neurons) that process information. Each connection has a weight that adjusts during training, allowing the network to learn complex patterns from data.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is overfitting in machine learning?\",\n",
    "        \"response\": \"Overfitting occurs when a model learns the training data too well, including its noise and outliers. The model performs excellently on training data but poorly on new, unseen data. It's like memorizing answers instead of understanding concepts.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain gradient descent.\",\n",
    "        \"response\": \"Gradient descent is an optimization algorithm that finds the minimum of a function by iteratively moving in the direction of steepest descent. In machine learning, it adjusts model parameters to minimize the difference between predictions and actual values.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Training samples: {len(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for training\n",
    "def format_instruction(sample):\n",
    "    \"\"\"Format a sample into the instruction template.\"\"\"\n",
    "    return f\"\"\"### Instruction:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Response:\n",
    "{sample['response']}\"\"\"\n",
    "\n",
    "# Create formatted texts\n",
    "formatted_data = [{\"text\": format_instruction(sample)} for sample in training_data]\n",
    "\n",
    "# Create Hugging Face dataset\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "print(\"Sample formatted data:\")\n",
    "print(dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Configure Training\n",
    "\n",
    "Set up training arguments using SFTConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./results\",                     # Output directory\n",
    "    num_train_epochs=3,                         # Number of training epochs\n",
    "    per_device_train_batch_size=1,              # Batch size per GPU\n",
    "    gradient_accumulation_steps=4,              # Accumulate gradients\n",
    "    learning_rate=2e-4,                         # Learning rate\n",
    "    weight_decay=0.01,                          # Weight decay\n",
    "    warmup_ratio=0.03,                          # Warmup ratio\n",
    "    lr_scheduler_type=\"cosine\",                 # LR scheduler\n",
    "    logging_steps=10,                           # Log every N steps\n",
    "    save_strategy=\"epoch\",                      # Save every epoch\n",
    "    fp16=True,                                  # Use FP16\n",
    "    max_seq_length=512,                         # Maximum sequence length\n",
    "    dataset_text_field=\"text\",                  # Field containing text\n",
    "    packing=False,                              # Don't pack sequences\n",
    ")\n",
    "\n",
    "print(\"Training configuration set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Trainer and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer created! Ready to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "adapter_path = \"./lora-adapter\"\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "print(f\"Adapter saved to {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Test the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def generate_response(instruction):\n",
    "    \"\"\"Generate a response for an instruction.\"\"\"\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "# Test with a sample question\n",
    "test_instruction = \"What is transfer learning?\"\n",
    "response = generate_response(test_instruction)\n",
    "\n",
    "print(f\"Instruction: {test_instruction}\")\n",
    "print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Modify the dataset**: Add more training examples relevant to your domain\n",
    "2. **Adjust hyperparameters**: Try different values for `r`, `lora_alpha`, and learning rate\n",
    "3. **Change target modules**: Experiment with different target modules\n",
    "4. **Increase epochs**: Train for more epochs and observe the effect\n",
    "5. **Compare results**: Test the same prompts on the base model vs fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Add your own experiments!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
