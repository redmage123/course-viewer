{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Data Preprocessing & Feature Engineering\n",
    "\n",
    "**AI/ML for Data Scientists - Day 1**\n",
    "\n",
    "In this lab, you'll practice:\n",
    "- Loading and exploring datasets\n",
    "- Handling missing values\n",
    "- Feature scaling and encoding\n",
    "- Feature engineering\n",
    "- Model training and evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load and Explore the Dataset\n",
    "\n",
    "We'll use a sample customer churn dataset for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset (simulating customer churn data)\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data = {\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'age': np.random.randint(18, 70, n_samples),\n",
    "    'tenure_months': np.random.randint(1, 72, n_samples),\n",
    "    'monthly_charges': np.random.uniform(20, 100, n_samples),\n",
    "    'total_charges': np.random.uniform(100, 5000, n_samples),\n",
    "    'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples),\n",
    "    'payment_method': np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'], n_samples),\n",
    "    'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], n_samples),\n",
    "    'tech_support': np.random.choice(['Yes', 'No', 'No internet'], n_samples),\n",
    "    'churn': np.random.choice([0, 1], n_samples, p=[0.73, 0.27])\n",
    "}\n",
    "\n",
    "# Add some missing values\n",
    "df = pd.DataFrame(data)\n",
    "df.loc[np.random.choice(df.index, 50), 'age'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 30), 'total_charges'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 20), 'tech_support'] = np.nan\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Explore the Dataset\n",
    "\n",
    "Use pandas methods to explore the dataset structure and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Check the data types and info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Get descriptive statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Check the target variable distribution\n",
    "df['churn'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Handling Missing Values\n",
    "\n",
    "Let's handle the missing values in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Impute Numeric Missing Values\n",
    "\n",
    "Use SimpleImputer to fill missing numeric values with the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Impute numeric columns\n",
    "numeric_cols = ['age', 'total_charges']\n",
    "\n",
    "# Create imputer with median strategy\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Fit and transform\n",
    "df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"Missing values after imputation:\")\n",
    "print(df[numeric_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Impute Categorical Missing Values\n",
    "\n",
    "Fill missing categorical values with the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Impute categorical columns\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "df[['tech_support']] = cat_imputer.fit_transform(df[['tech_support']])\n",
    "\n",
    "# Verify\n",
    "print(\"Missing values after imputation:\")\n",
    "print(df['tech_support'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature Engineering\n",
    "\n",
    "Create new features that might help the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Create New Features\n",
    "\n",
    "Create the following engineered features:\n",
    "1. `avg_monthly_charge`: total_charges / tenure_months\n",
    "2. `age_group`: Binned age (Young, Middle, Senior)\n",
    "3. `is_long_term`: 1 if contract is not month-to-month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Create engineered features\n",
    "\n",
    "# Average monthly charge\n",
    "df['avg_monthly_charge'] = df['total_charges'] / df['tenure_months'].replace(0, 1)\n",
    "\n",
    "# Age group\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 30, 50, 100], labels=['Young', 'Middle', 'Senior'])\n",
    "\n",
    "# Long-term contract indicator\n",
    "df['is_long_term'] = (df['contract_type'] != 'Month-to-month').astype(int)\n",
    "\n",
    "print(\"New features created:\")\n",
    "df[['avg_monthly_charge', 'age_group', 'is_long_term']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Feature Encoding\n",
    "\n",
    "Encode categorical variables for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: One-Hot Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns to encode\n",
    "cat_columns = ['contract_type', 'payment_method', 'internet_service', 'tech_support', 'age_group']\n",
    "\n",
    "# YOUR CODE HERE: One-hot encode categorical columns\n",
    "df_encoded = pd.get_dummies(df, columns=cat_columns, drop_first=True)\n",
    "\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Feature Scaling\n",
    "\n",
    "Scale numeric features for better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "feature_cols = [col for col in df_encoded.columns if col not in ['customer_id', 'churn']]\n",
    "X = df_encoded[feature_cols]\n",
    "y = df_encoded['churn']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Scale numeric features\n",
    "numeric_features = ['age', 'tenure_months', 'monthly_charges', 'total_charges', 'avg_monthly_charge']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data only!\n",
    "X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "\n",
    "# Transform test data\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "print(\"Features scaled successfully!\")\n",
    "X_train[numeric_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1: Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Train logistic regression\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_lr):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.2: Train a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Train random forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.3: Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Perform stratified k-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_scores_lr = cross_val_score(lr_model, X_train, y_train, cv=cv, scoring='f1')\n",
    "cv_scores_rf = cross_val_score(rf_model, X_train, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "print(f\"Logistic Regression CV F1: {cv_scores_lr.mean():.4f} (+/- {cv_scores_lr.std():.4f})\")\n",
    "print(f\"Random Forest CV F1: {cv_scores_rf.mean():.4f} (+/- {cv_scores_rf.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.4: Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Plot confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Logistic Regression\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Logistic Regression')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "# Random Forest\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_title('Random Forest')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.5: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Plot feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=feature_importance.head(15), x='importance', y='feature', palette='viridis')\n",
    "plt.title('Top 15 Feature Importance (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Load and explore** datasets with pandas\n",
    "2. **Handle missing values** using SimpleImputer\n",
    "3. **Engineer new features** from existing data\n",
    "4. **Encode categorical variables** with one-hot encoding\n",
    "5. **Scale numeric features** with StandardScaler\n",
    "6. **Train and evaluate** classification models\n",
    "7. **Perform cross-validation** for robust evaluation\n",
    "8. **Visualize results** with confusion matrices and feature importance\n",
    "\n",
    "---\n",
    "\n",
    "*AI/ML for Data Scientists | AI Elevate*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
