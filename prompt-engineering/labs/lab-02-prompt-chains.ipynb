{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Prompt Chains & Testing Workflows\n",
    "\n",
    "In this lab, you'll move beyond single prompts and build **multi-step prompt chains** that tackle complex tasks by breaking them into a series of connected steps. You'll also implement a prompt testing framework to compare and evaluate prompt quality systematically.\n",
    "\n",
    "## Learning Objectives\n",
    "- Build multi-step prompt chains for complex tasks\n",
    "- Implement prompt testing and evaluation\n",
    "- Design iterative refinement workflows\n",
    "\n",
    "**Duration:** 45 minutes | **Difficulty:** Intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple Prompt Chains\n",
    "\n",
    "A **prompt chain** breaks a complex task into discrete steps where the output of one step feeds into the next. This mirrors how humans tackle large problems: research first, then outline, then draft.\n",
    "\n",
    "Since we are working without a live LLM, each step uses a **transform function** that simulates realistic AI output. In production, you would replace these with actual API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "import textwrap\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChainStep:\n",
    "    \"\"\"A single step in a prompt chain.\"\"\"\n",
    "    name: str\n",
    "    prompt_template: str\n",
    "    transform_fn: Optional[Callable[[str], str]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StepResult:\n",
    "    \"\"\"Record of what happened at one step.\"\"\"\n",
    "    step_name: str\n",
    "    prompt_sent: str\n",
    "    output: str\n",
    "\n",
    "\n",
    "class PromptChain:\n",
    "    \"\"\"A sequential chain of prompts where each step's output feeds the next.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._steps: List[ChainStep] = []\n",
    "        self._trace: List[StepResult] = []\n",
    "\n",
    "    def add_step(self, name: str, prompt_template: str,\n",
    "                 transform_fn: Optional[Callable[[str], str]] = None) -> 'PromptChain':\n",
    "        \"\"\"Add a step to the chain.\n",
    "\n",
    "        Args:\n",
    "            name: Human-readable step name.\n",
    "            prompt_template: Prompt string with {input} placeholder.\n",
    "            transform_fn: Simulates AI output for this step.\n",
    "                          Receives the formatted prompt, returns output text.\n",
    "        Returns:\n",
    "            self (for fluent chaining).\n",
    "        \"\"\"\n",
    "        self._steps.append(ChainStep(name, prompt_template, transform_fn))\n",
    "        return self\n",
    "\n",
    "    def run(self, initial_input: str) -> str:\n",
    "        \"\"\"Execute the chain end-to-end.\"\"\"\n",
    "        self._trace = []\n",
    "        current = initial_input\n",
    "\n",
    "        for step in self._steps:\n",
    "            prompt = step.prompt_template.replace(\"{input}\", current)\n",
    "\n",
    "            if step.transform_fn:\n",
    "                output = step.transform_fn(prompt)\n",
    "            else:\n",
    "                output = prompt  # pass-through if no transform\n",
    "\n",
    "            self._trace.append(StepResult(\n",
    "                step_name=step.name,\n",
    "                prompt_sent=prompt,\n",
    "                output=output\n",
    "            ))\n",
    "            current = output\n",
    "\n",
    "        return current\n",
    "\n",
    "    def show_trace(self) -> None:\n",
    "        \"\"\"Pretty-print what happened at each step.\"\"\"\n",
    "        for i, result in enumerate(self._trace, 1):\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(f\"  Step {i}: {result.step_name}\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(f\"  PROMPT:\")\n",
    "            print(textwrap.indent(textwrap.shorten(result.prompt_sent, width=200), \"    \"))\n",
    "            print(f\"\\n  OUTPUT:\")\n",
    "            print(textwrap.indent(result.output, \"    \"))\n",
    "            print()\n",
    "\n",
    "    def get_trace(self) -> List[StepResult]:\n",
    "        \"\"\"Return the raw trace list.\"\"\"\n",
    "        return list(self._trace)\n",
    "\n",
    "\n",
    "# ── Simulated transform functions ─────────────────────────────────\n",
    "\n",
    "def sim_extract_topics(prompt: str) -> str:\n",
    "    \"\"\"Simulate extracting key topics from raw text.\"\"\"\n",
    "    return (\n",
    "        \"Key Topics Identified:\\n\"\n",
    "        \"1. Current adoption rates and growth trajectory\\n\"\n",
    "        \"2. Primary use cases in clinical settings\\n\"\n",
    "        \"3. Regulatory landscape and compliance requirements\\n\"\n",
    "        \"4. Cost-benefit analysis for hospital systems\\n\"\n",
    "        \"5. Patient outcome improvements backed by studies\"\n",
    "    )\n",
    "\n",
    "def sim_create_outline(prompt: str) -> str:\n",
    "    \"\"\"Simulate creating a structured outline from topics.\"\"\"\n",
    "    return (\n",
    "        \"Article Outline:\\n\"\n",
    "        \"I. Introduction - The AI revolution in healthcare\\n\"\n",
    "        \"   A. Hook: startling statistic on diagnostic errors\\n\"\n",
    "        \"   B. Thesis: AI is transforming clinical outcomes\\n\"\n",
    "        \"II. Adoption Landscape\\n\"\n",
    "        \"   A. Current rates: 38% of hospitals using some form of AI\\n\"\n",
    "        \"   B. Growth: projected 45% CAGR through 2028\\n\"\n",
    "        \"III. Clinical Use Cases\\n\"\n",
    "        \"   A. Diagnostic imaging (radiology, pathology)\\n\"\n",
    "        \"   B. Predictive patient monitoring\\n\"\n",
    "        \"   C. Drug discovery acceleration\\n\"\n",
    "        \"IV. Regulatory & Compliance\\n\"\n",
    "        \"   A. FDA approval pathways for AI/ML devices\\n\"\n",
    "        \"   B. HIPAA considerations for training data\\n\"\n",
    "        \"V. ROI and Outcomes\\n\"\n",
    "        \"   A. Cost savings from reduced misdiagnosis\\n\"\n",
    "        \"   B. Improved patient satisfaction scores\\n\"\n",
    "        \"VI. Conclusion and Recommendations\"\n",
    "    )\n",
    "\n",
    "def sim_write_intro(prompt: str) -> str:\n",
    "    \"\"\"Simulate writing an introduction from an outline.\"\"\"\n",
    "    return (\n",
    "        \"Every year, an estimated 12 million Americans receive a misdiagnosis \"\n",
    "        \"in outpatient settings, according to a study published in BMJ Quality \"\n",
    "        \"& Safety. For hospital administrators grappling with this challenge, \"\n",
    "        \"artificial intelligence offers a compelling answer. From radiology \"\n",
    "        \"suites where deep learning algorithms flag suspicious lesions to ICU \"\n",
    "        \"floors where predictive models identify patients at risk of sepsis \"\n",
    "        \"hours before traditional vital signs change, AI is no longer a \"\n",
    "        \"futuristic concept\\u2014it is a clinical reality.\\n\\n\"\n",
    "        \"With 38% of US hospitals already deploying some form of AI and the \"\n",
    "        \"market growing at a projected 45% compound annual growth rate, the \"\n",
    "        \"question is no longer whether to adopt, but how to adopt \"\n",
    "        \"responsibly. This article examines the current landscape, highlights \"\n",
    "        \"proven use cases, navigates the regulatory environment, and makes \"\n",
    "        \"the financial case for AI investment in clinical settings.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ── Build and run a 3-step chain ──────────────────────────────────\n",
    "\n",
    "chain = PromptChain()\n",
    "chain.add_step(\n",
    "    \"Extract Topics\",\n",
    "    \"Extract the 5 most important topics from the following subject area: {input}\",\n",
    "    transform_fn=sim_extract_topics\n",
    ")\n",
    "chain.add_step(\n",
    "    \"Create Outline\",\n",
    "    \"Create a detailed article outline organized around these topics:\\n{input}\",\n",
    "    transform_fn=sim_create_outline\n",
    ")\n",
    "chain.add_step(\n",
    "    \"Write Introduction\",\n",
    "    \"Write a compelling 150-word introduction for an article with this outline:\\n{input}\",\n",
    "    transform_fn=sim_write_intro\n",
    ")\n",
    "\n",
    "result = chain.run(\"AI in healthcare\")\n",
    "chain.show_trace()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  FINAL OUTPUT\")\n",
    "print(\"=\" * 60)\n",
    "print(textwrap.fill(result, width=72))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Workflow Patterns\n",
    "\n",
    "Real-world prompt workflows go beyond simple sequences. Three common patterns:\n",
    "\n",
    "| Pattern | Description | Use Case |\n",
    "|---------|-------------|----------|\n",
    "| **Sequential** | Steps run one after another | Article writing, report generation |\n",
    "| **Fan-out** | Same prompt applied to many inputs, results collected | Batch analysis, multi-document review |\n",
    "| **Iterative** | Same prompt re-applied to progressively refine output | Editing, polishing, improving quality |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowEngine:\n",
    "    \"\"\"Engine supporting three common prompt workflow patterns.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sequential(steps: List[Tuple[str, Callable[[str], str]]],\n",
    "                   initial_input: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Run steps in order, passing output forward.\n",
    "\n",
    "        Args:\n",
    "            steps: List of (step_name, transform_function) tuples.\n",
    "            initial_input: Starting text.\n",
    "        Returns:\n",
    "            List of (step_name, output) tuples.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        current = initial_input\n",
    "        for name, fn in steps:\n",
    "            current = fn(current)\n",
    "            results.append((name, current))\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def fan_out(transform_fn: Callable[[str], str],\n",
    "                inputs: List[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Apply the same transform to multiple inputs.\n",
    "\n",
    "        Returns:\n",
    "            List of (input_text, output_text) tuples.\n",
    "        \"\"\"\n",
    "        return [(inp, transform_fn(inp)) for inp in inputs]\n",
    "\n",
    "    @staticmethod\n",
    "    def iterative(transform_fn: Callable[[str, int], str],\n",
    "                  initial_input: str,\n",
    "                  iterations: int = 3) -> List[Tuple[int, str]]:\n",
    "        \"\"\"Refine output over multiple iterations.\n",
    "\n",
    "        Args:\n",
    "            transform_fn: Receives (current_text, iteration_number) and\n",
    "                          returns refined text.\n",
    "            initial_input: Starting text.\n",
    "            iterations: Number of refinement rounds.\n",
    "        Returns:\n",
    "            List of (round_number, output_text) tuples.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        current = initial_input\n",
    "        for i in range(1, iterations + 1):\n",
    "            current = transform_fn(current, i)\n",
    "            results.append((i, current))\n",
    "        return results\n",
    "\n",
    "\n",
    "engine = WorkflowEngine()\n",
    "\n",
    "# ── Demo 1: Sequential ─────────────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"  PATTERN: Sequential\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "seq_steps = [\n",
    "    (\"Brainstorm\", lambda text: (\n",
    "        f\"Ideas generated from '{text}':\\n\"\n",
    "        \"- Personalized learning paths using ML\\n\"\n",
    "        \"- Automated grading with NLP\\n\"\n",
    "        \"- Intelligent tutoring chatbots\"\n",
    "    )),\n",
    "    (\"Prioritize\", lambda text: (\n",
    "        \"Prioritized ideas (by impact):\\n\"\n",
    "        \"1. [HIGH]  Personalized learning paths - addresses diverse needs\\n\"\n",
    "        \"2. [MED]   Intelligent tutoring chatbots - scalable support\\n\"\n",
    "        \"3. [LOW]   Automated grading - saves time but lower strategic value\"\n",
    "    )),\n",
    "    (\"Action Plan\", lambda text: (\n",
    "        \"90-Day Action Plan:\\n\"\n",
    "        \"Month 1: Pilot personalized learning with 2 courses\\n\"\n",
    "        \"Month 2: Integrate tutoring chatbot on platform\\n\"\n",
    "        \"Month 3: Evaluate results, plan automated grading pilot\"\n",
    "    )),\n",
    "]\n",
    "\n",
    "seq_results = engine.sequential(seq_steps, \"AI in education\")\n",
    "for name, output in seq_results:\n",
    "    print(f\"\\n  [{name}]\")\n",
    "    print(textwrap.indent(output, \"    \"))\n",
    "\n",
    "# ── Demo 2: Fan-out ────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  PATTERN: Fan-out (Sentiment Analysis)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "reviews = [\n",
    "    \"Absolutely love this product! Best purchase I have made all year.\",\n",
    "    \"It works okay but the battery life is disappointing for the price.\",\n",
    "    \"Terrible customer service. Waited 3 weeks for a response.\",\n",
    "    \"Good value for money. Solid build quality and fast shipping.\",\n",
    "    \"The software crashes constantly. Returning it this week.\"\n",
    "]\n",
    "\n",
    "sentiment_map = {\n",
    "    0: (\"POSITIVE\", 0.92),\n",
    "    1: (\"MIXED\",    0.61),\n",
    "    2: (\"NEGATIVE\", 0.15),\n",
    "    3: (\"POSITIVE\", 0.78),\n",
    "    4: (\"NEGATIVE\", 0.08),\n",
    "}\n",
    "\n",
    "def sim_sentiment(review: str) -> str:\n",
    "    idx = reviews.index(review) if review in reviews else 0\n",
    "    label, conf = sentiment_map.get(idx, (\"NEUTRAL\", 0.50))\n",
    "    return f\"{label} (confidence: {conf:.0%})\"\n",
    "\n",
    "fan_results = engine.fan_out(sim_sentiment, reviews)\n",
    "print(f\"\\n  {'Review (truncated)':<55} {'Sentiment'}\")\n",
    "print(\"  \" + \"-\" * 75)\n",
    "for review, sentiment in fan_results:\n",
    "    short = review[:52] + \"...\" if len(review) > 52 else review\n",
    "    print(f\"  {short:<55} {sentiment}\")\n",
    "\n",
    "# ── Demo 3: Iterative ──────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  PATTERN: Iterative (Product Description Refinement)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "refinement_stages = [\n",
    "    \"SmartFit Pro is a fitness tracker. It tracks steps and heart rate. \"\n",
    "    \"It has a screen and a band. Buy it now.\",\n",
    "\n",
    "    \"The SmartFit Pro fitness tracker monitors your steps, heart rate, \"\n",
    "    \"and sleep patterns in real time. Its vibrant OLED display and \"\n",
    "    \"comfortable silicone band make it ideal for all-day wear.\",\n",
    "\n",
    "    \"Meet SmartFit Pro\\u2014the fitness tracker that works as hard as you do. \"\n",
    "    \"Track steps, heart rate, and sleep with medical-grade sensors. \"\n",
    "    \"The brilliant OLED display delivers real-time insights at a glance, \"\n",
    "    \"while the ultra-light silicone band ensures all-day comfort. \"\n",
    "    \"With 7-day battery life and water resistance to 50m, SmartFit Pro \"\n",
    "    \"keeps up no matter where your day takes you.\"\n",
    "]\n",
    "\n",
    "def sim_refine(text: str, iteration: int) -> str:\n",
    "    idx = min(iteration, len(refinement_stages)) - 1\n",
    "    return refinement_stages[idx]\n",
    "\n",
    "iter_results = engine.iterative(sim_refine, refinement_stages[0], iterations=3)\n",
    "for round_num, output in iter_results:\n",
    "    word_count = len(output.split())\n",
    "    print(f\"\\n  [Round {round_num}] ({word_count} words)\")\n",
    "    print(textwrap.indent(textwrap.fill(output, width=68), \"    \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Build a Content Production Chain\n",
    "\n",
    "Create a 4-step content production chain that takes a topic and produces a polished piece of content. You need to:\n",
    "\n",
    "1. **Research** - Extract key points from a topic\n",
    "2. **Outline** - Structure the key points into sections\n",
    "3. **Draft** - Write a first draft from the outline\n",
    "4. **Polish** - Refine the tone and formatting\n",
    "\n",
    "Write a `transform_fn` for each step that simulates realistic output. Run the chain on the topic \"AI in healthcare\" and display the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Build a 4-step content production chain:\n",
    "# Step 1: Research - Extract key points from a topic\n",
    "# Step 2: Outline - Structure the key points\n",
    "# Step 3: Draft - Write first draft from outline\n",
    "# Step 4: Polish - Refine tone and format\n",
    "\n",
    "# Define your transform functions\n",
    "# def research_fn(prompt: str) -> str:\n",
    "#     return \"...\"  # Simulate extracted key points\n",
    "\n",
    "# def outline_fn(prompt: str) -> str:\n",
    "#     return \"...\"  # Simulate structured outline\n",
    "\n",
    "# def draft_fn(prompt: str) -> str:\n",
    "#     return \"...\"  # Simulate first draft\n",
    "\n",
    "# def polish_fn(prompt: str) -> str:\n",
    "#     return \"...\"  # Simulate polished final version\n",
    "\n",
    "chain = PromptChain()\n",
    "\n",
    "# Add your steps here\n",
    "# chain.add_step(\"Research\", \"Identify the 5 most important aspects of: {input}\",\n",
    "#                transform_fn=research_fn)\n",
    "# chain.add_step(\"Outline\", \"Create a structured outline from: {input}\",\n",
    "#                transform_fn=outline_fn)\n",
    "# chain.add_step(\"Draft\", \"Write a 200-word first draft based on: {input}\",\n",
    "#                transform_fn=draft_fn)\n",
    "# chain.add_step(\"Polish\", \"Refine this draft for a professional audience: {input}\",\n",
    "#                transform_fn=polish_fn)\n",
    "\n",
    "# Run the chain\n",
    "# result = chain.run(\"AI in healthcare\")\n",
    "# chain.show_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Prompt Testing & Evaluation\n",
    "\n",
    "Professional prompt engineers don't just write prompts and hope for the best. They **test** them against known inputs, score the outputs, and run A/B comparisons. The `PromptTester` class below provides a lightweight framework for doing exactly that.\n",
    "\n",
    "Each test case specifies:\n",
    "- An **input** that will be fed to the prompt\n",
    "- **Expected keywords** the output should contain\n",
    "- An **expected format** pattern the output should match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestCase:\n",
    "    \"\"\"A single test case for prompt evaluation.\"\"\"\n",
    "    name: str\n",
    "    input_text: str\n",
    "    expected_keywords: List[str]\n",
    "    expected_format: str  # regex pattern the output should match\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TestResult:\n",
    "    \"\"\"Evaluation result for one test case against one prompt.\"\"\"\n",
    "    test_name: str\n",
    "    relevance: int       # 1-5: does the output address the input?\n",
    "    completeness: int    # 1-5: how many expected keywords appear?\n",
    "    format_compliance: int  # 1-5: does the output match expected format?\n",
    "    consistency: int     # 1-5: is the quality consistent across tests?\n",
    "\n",
    "    @property\n",
    "    def total(self) -> int:\n",
    "        return self.relevance + self.completeness + self.format_compliance + self.consistency\n",
    "\n",
    "    @property\n",
    "    def dimensions(self) -> Dict[str, int]:\n",
    "        return {\n",
    "            \"relevance\": self.relevance,\n",
    "            \"completeness\": self.completeness,\n",
    "            \"format_compliance\": self.format_compliance,\n",
    "            \"consistency\": self.consistency,\n",
    "        }\n",
    "\n",
    "\n",
    "class PromptTester:\n",
    "    \"\"\"Framework for testing and comparing prompts.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._results: Dict[str, List[TestResult]] = {}  # prompt_label -> results\n",
    "\n",
    "    def evaluate(self, prompt_label: str,\n",
    "                 prompt_template: str,\n",
    "                 simulate_fn: Callable[[str], str],\n",
    "                 test_cases: List[TestCase]) -> List[TestResult]:\n",
    "        \"\"\"Run test cases against a prompt and score the outputs.\n",
    "\n",
    "        Args:\n",
    "            prompt_label: Name for this prompt variant (e.g. 'Prompt A').\n",
    "            prompt_template: Prompt string with {input} placeholder.\n",
    "            simulate_fn: Function that simulates AI output for the prompt.\n",
    "            test_cases: List of TestCase objects.\n",
    "        Returns:\n",
    "            List of TestResult objects.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for tc in test_cases:\n",
    "            filled = prompt_template.replace(\"{input}\", tc.input_text)\n",
    "            output = simulate_fn(filled)\n",
    "            output_lower = output.lower()\n",
    "\n",
    "            # Relevance: does the output reference the input topic?\n",
    "            input_words = set(tc.input_text.lower().split())\n",
    "            overlap = sum(1 for w in input_words if w in output_lower)\n",
    "            relevance = min(1 + int(overlap / max(len(input_words) * 0.2, 1)), 5)\n",
    "\n",
    "            # Completeness: what fraction of expected keywords appear?\n",
    "            found = sum(1 for kw in tc.expected_keywords if kw.lower() in output_lower)\n",
    "            ratio = found / max(len(tc.expected_keywords), 1)\n",
    "            completeness = max(1, min(int(ratio * 5) + 1, 5))\n",
    "\n",
    "            # Format compliance: does the output match the expected pattern?\n",
    "            if tc.expected_format:\n",
    "                match = re.search(tc.expected_format, output, re.IGNORECASE | re.DOTALL)\n",
    "                format_compliance = 5 if match else 2\n",
    "            else:\n",
    "                format_compliance = 3\n",
    "\n",
    "            # Consistency: based on output length being reasonable\n",
    "            word_count = len(output.split())\n",
    "            consistency = 5 if 30 <= word_count <= 500 else (3 if word_count > 10 else 1)\n",
    "\n",
    "            results.append(TestResult(\n",
    "                test_name=tc.name,\n",
    "                relevance=relevance,\n",
    "                completeness=completeness,\n",
    "                format_compliance=format_compliance,\n",
    "                consistency=consistency,\n",
    "            ))\n",
    "\n",
    "        self._results[prompt_label] = results\n",
    "        return results\n",
    "\n",
    "    def compare(self, label_a: str, label_b: str) -> str:\n",
    "        \"\"\"Compare two evaluated prompts and declare a winner.\"\"\"\n",
    "        if label_a not in self._results or label_b not in self._results:\n",
    "            return \"Both prompts must be evaluated first.\"\n",
    "\n",
    "        total_a = sum(r.total for r in self._results[label_a])\n",
    "        total_b = sum(r.total for r in self._results[label_b])\n",
    "\n",
    "        print(f\"\\n  {label_a}: {total_a} pts  vs  {label_b}: {total_b} pts\")\n",
    "        if total_a > total_b:\n",
    "            winner = label_a\n",
    "        elif total_b > total_a:\n",
    "            winner = label_b\n",
    "        else:\n",
    "            winner = \"TIE\"\n",
    "        print(f\"  Winner: {winner}\\n\")\n",
    "        return winner\n",
    "\n",
    "    def report(self) -> None:\n",
    "        \"\"\"Print a formatted evaluation report for all tested prompts.\"\"\"\n",
    "        dims = [\"relevance\", \"completeness\", \"format_compliance\", \"consistency\"]\n",
    "        for label, results in self._results.items():\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"  Evaluation Report: {label}\")\n",
    "            print(\"=\" * 60)\n",
    "            header = f\"  {'Test Case':<20}\"\n",
    "            for d in dims:\n",
    "                header += f\" {d[:8]:>8}\"\n",
    "            header += f\" {'TOTAL':>7}\"\n",
    "            print(header)\n",
    "            print(\"  \" + \"-\" * (len(header) - 2))\n",
    "\n",
    "            grand_total = 0\n",
    "            for r in results:\n",
    "                row = f\"  {r.test_name:<20}\"\n",
    "                for d in dims:\n",
    "                    row += f\" {r.dimensions[d]:>7}/5\"\n",
    "                row += f\" {r.total:>6}/20\"\n",
    "                print(row)\n",
    "                grand_total += r.total\n",
    "\n",
    "            max_possible = len(results) * 20\n",
    "            print(f\"\\n  Grand Total: {grand_total}/{max_possible} \"\n",
    "                  f\"({grand_total/max_possible:.0%})\")\n",
    "            print()\n",
    "\n",
    "\n",
    "# ── Demo: comparing two customer support prompts ────────────────\n",
    "\n",
    "# Test cases for a customer support response task\n",
    "test_cases = [\n",
    "    TestCase(\n",
    "        name=\"refund_request\",\n",
    "        input_text=\"I want a refund for order #12345. The product arrived damaged.\",\n",
    "        expected_keywords=[\"apologize\", \"refund\", \"order\", \"damaged\", \"process\", \"timeline\"],\n",
    "        expected_format=r\"(sorry|apologize|apolog).*(refund|return)\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        name=\"feature_question\",\n",
    "        input_text=\"Does your premium plan include API access and team management?\",\n",
    "        expected_keywords=[\"premium\", \"API\", \"team\", \"plan\", \"features\", \"included\"],\n",
    "        expected_format=r\"(premium|plan).*(API|api).*(team|manage)\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        name=\"complaint\",\n",
    "        input_text=\"Your app has been crashing every day this week. Very frustrated.\",\n",
    "        expected_keywords=[\"sorry\", \"crash\", \"issue\", \"team\", \"fix\", \"update\"],\n",
    "        expected_format=r\"(sorry|apologize|understand).*(fix|resolv|investigat)\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Prompt A: basic approach\n",
    "prompt_a = \"Reply to this customer message: {input}\"\n",
    "\n",
    "# Prompt B: CRAFT-enhanced\n",
    "prompt_b = (\n",
    "    \"Context: You are handling a support ticket for a SaaS product. \"\n",
    "    \"The customer's satisfaction score is at risk. \"\n",
    "    \"Role: Act as a senior customer success representative with 8 years \"\n",
    "    \"of experience in tech support. \"\n",
    "    \"Task: Respond to the following customer message. Acknowledge their \"\n",
    "    \"concern, provide a specific solution or next step, and include a \"\n",
    "    \"timeline. Message: {input} \"\n",
    "    \"Format: Greeting, empathy statement, solution paragraph, and \"\n",
    "    \"closing with next steps. \"\n",
    "    \"Tone: Empathetic, professional, and solution-oriented.\"\n",
    ")\n",
    "\n",
    "# Simulated outputs for Prompt A (basic - less complete)\n",
    "sim_outputs_a = {\n",
    "    \"refund_request\": (\n",
    "        \"Hi, we can process your refund. Please send the item back \"\n",
    "        \"and we will handle it. Let us know if you need anything else.\"\n",
    "    ),\n",
    "    \"feature_question\": (\n",
    "        \"Yes, the premium plan has API access. You can manage your \"\n",
    "        \"team from the dashboard. Check our pricing page for details.\"\n",
    "    ),\n",
    "    \"complaint\": (\n",
    "        \"We know about the crash issue. Our team is working on it. \"\n",
    "        \"Try restarting the app for now.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Simulated outputs for Prompt B (CRAFT - more thorough)\n",
    "sim_outputs_b = {\n",
    "    \"refund_request\": (\n",
    "        \"Dear Customer,\\n\\n\"\n",
    "        \"I sincerely apologize that your order #12345 arrived damaged. \"\n",
    "        \"That is not the experience we want for our customers, and I \"\n",
    "        \"completely understand your frustration.\\n\\n\"\n",
    "        \"I have initiated a full refund to your original payment method. \"\n",
    "        \"You should see the credit within 3-5 business days. There is no \"\n",
    "        \"need to return the damaged item. Additionally, I would like to \"\n",
    "        \"offer you a 15% discount on your next order as a gesture of \"\n",
    "        \"goodwill.\\n\\n\"\n",
    "        \"If you have any questions, please reply here or call us at \"\n",
    "        \"1-800-555-0199. We value your business and want to make this right.\\n\\n\"\n",
    "        \"Best regards,\\nCustomer Success Team\"\n",
    "    ),\n",
    "    \"feature_question\": (\n",
    "        \"Hello!\\n\\n\"\n",
    "        \"Great question about our Premium plan. Yes, both features are \"\n",
    "        \"included:\\n\\n\"\n",
    "        \"- **API Access**: Full REST API with 10,000 requests/month, \"\n",
    "        \"webhook support, and SDKs for Python, JavaScript, and Go.\\n\"\n",
    "        \"- **Team Management**: Add up to 25 team members, assign roles \"\n",
    "        \"(admin, editor, viewer), and manage permissions from a central \"\n",
    "        \"dashboard.\\n\\n\"\n",
    "        \"The Premium plan is $49/month when billed annually. I would be \"\n",
    "        \"happy to set up a 14-day free trial so your team can evaluate \"\n",
    "        \"both features. Just reply and I will send the activation link.\\n\\n\"\n",
    "        \"Best,\\nCustomer Success Team\"\n",
    "    ),\n",
    "    \"complaint\": (\n",
    "        \"Hi there,\\n\\n\"\n",
    "        \"I am truly sorry about the repeated crashes you have been \"\n",
    "        \"experiencing. I understand how frustrating that must be, \"\n",
    "        \"especially when it disrupts your daily workflow.\\n\\n\"\n",
    "        \"Our engineering team has identified the issue and a fix is \"\n",
    "        \"scheduled for release in version 4.2.1 this Thursday. In the \"\n",
    "        \"meantime, clearing the app cache (Settings > Storage > Clear \"\n",
    "        \"Cache) should reduce crash frequency. I have also flagged your \"\n",
    "        \"account for priority support.\\n\\n\"\n",
    "        \"I will personally follow up with you on Friday to confirm the \"\n",
    "        \"update resolved the issue. Thank you for your patience.\\n\\n\"\n",
    "        \"Warm regards,\\nCustomer Success Team\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def sim_a(prompt: str) -> str:\n",
    "    for key, output in sim_outputs_a.items():\n",
    "        if key.replace(\"_\", \" \") in prompt.lower() or \\\n",
    "           any(w in prompt.lower() for w in key.split(\"_\")):\n",
    "            return output\n",
    "    return \"Thank you for reaching out. We will look into this.\"\n",
    "\n",
    "def sim_b(prompt: str) -> str:\n",
    "    for key, output in sim_outputs_b.items():\n",
    "        if key.replace(\"_\", \" \") in prompt.lower() or \\\n",
    "           any(w in prompt.lower() for w in key.split(\"_\")):\n",
    "            return output\n",
    "    return \"Thank you for reaching out. We will look into this.\"\n",
    "\n",
    "# Run evaluation\n",
    "tester = PromptTester()\n",
    "tester.evaluate(\"Prompt A (basic)\", prompt_a, sim_a, test_cases)\n",
    "tester.evaluate(\"Prompt B (CRAFT)\", prompt_b, sim_b, test_cases)\n",
    "\n",
    "# Report and compare\n",
    "tester.report()\n",
    "tester.compare(\"Prompt A (basic)\", \"Prompt B (CRAFT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: A/B Test Your Prompts\n",
    "\n",
    "Pick a task (e.g., writing product descriptions, answering technical questions, summarizing documents). Create two prompt variants and three test cases, then evaluate and compare them.\n",
    "\n",
    "**Requirements:**\n",
    "- Two meaningfully different prompt variants (not just rewording)\n",
    "- Three test cases with expected keywords and format patterns\n",
    "- Simulated output functions for each variant\n",
    "- A comparison report identifying the winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create two versions of a prompt for the same task\n",
    "# Define 3 test cases and compare them\n",
    "\n",
    "prompt_a = \"Version A: {input}\"\n",
    "prompt_b = \"Version B: {input}\"\n",
    "\n",
    "# Define test cases\n",
    "my_test_cases = [\n",
    "    # TestCase(name=\"test_1\", input_text=\"...\",\n",
    "    #          expected_keywords=[...], expected_format=r\"...\"),\n",
    "    # TestCase(name=\"test_2\", input_text=\"...\",\n",
    "    #          expected_keywords=[...], expected_format=r\"...\"),\n",
    "    # TestCase(name=\"test_3\", input_text=\"...\",\n",
    "    #          expected_keywords=[...], expected_format=r\"...\"),\n",
    "]\n",
    "\n",
    "# Define simulated output functions\n",
    "# def my_sim_a(prompt: str) -> str:\n",
    "#     return \"...\"\n",
    "\n",
    "# def my_sim_b(prompt: str) -> str:\n",
    "#     return \"...\"\n",
    "\n",
    "# Evaluate and compare\n",
    "# my_tester = PromptTester()\n",
    "# my_tester.evaluate(\"My Prompt A\", prompt_a, my_sim_a, my_test_cases)\n",
    "# my_tester.evaluate(\"My Prompt B\", prompt_b, my_sim_b, my_test_cases)\n",
    "# my_tester.report()\n",
    "# my_tester.compare(\"My Prompt A\", \"My Prompt B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualization - Chain Performance\n",
    "\n",
    "Good visualizations help you communicate the value of prompt engineering to stakeholders. This section produces three charts:\n",
    "\n",
    "1. **Bar chart** - Processing \"time\" (simulated) across chain steps\n",
    "2. **Heatmap** - Test case scores across evaluation dimensions\n",
    "3. **Line chart** - Quality improvement across iterative refinement rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ── Chart 1: Chain Step Processing Time ─────────────────────────\n",
    "ax1 = axes[0]\n",
    "steps_names = [\"Extract\\nTopics\", \"Create\\nOutline\", \"Write\\nIntro\", \"Polish\\n& Edit\"]\n",
    "sim_times = [1.2, 2.1, 3.8, 2.5]  # simulated seconds\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
    "\n",
    "bars = ax1.bar(steps_names, sim_times, color=colors, edgecolor='white', linewidth=1.5)\n",
    "for bar, t in zip(bars, sim_times):\n",
    "    ax1.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,\n",
    "             f\"{t:.1f}s\", ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax1.set_ylabel('Processing Time (seconds)', fontsize=11)\n",
    "ax1.set_title('Chain Step Duration', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylim(0, max(sim_times) + 1)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "# ── Chart 2: Test Score Heatmap ────────────────────────────────\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Use the scores from the Prompt B evaluation above\n",
    "dims = [\"relevance\", \"completeness\", \"format_compl.\", \"consistency\"]\n",
    "test_names = [\"refund_req\", \"feature_q\", \"complaint\"]\n",
    "\n",
    "# Simulated score matrix (tests x dimensions)\n",
    "score_matrix = np.array([\n",
    "    [5, 5, 5, 5],  # refund_request - Prompt B\n",
    "    [4, 5, 5, 5],  # feature_question\n",
    "    [5, 4, 5, 5],  # complaint\n",
    "])\n",
    "\n",
    "im = ax2.imshow(score_matrix, cmap='RdYlGn', aspect='auto', vmin=1, vmax=5)\n",
    "ax2.set_xticks(range(len(dims)))\n",
    "ax2.set_xticklabels(dims, fontsize=9, rotation=30, ha='right')\n",
    "ax2.set_yticks(range(len(test_names)))\n",
    "ax2.set_yticklabels(test_names, fontsize=10)\n",
    "ax2.set_title('Test Scores (CRAFT Prompt)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Annotate cells\n",
    "for i in range(len(test_names)):\n",
    "    for j in range(len(dims)):\n",
    "        ax2.text(j, i, str(score_matrix[i, j]),\n",
    "                 ha='center', va='center', fontsize=14, fontweight='bold',\n",
    "                 color='white' if score_matrix[i, j] >= 4 else 'black')\n",
    "\n",
    "plt.colorbar(im, ax=ax2, shrink=0.8, label='Score (1-5)')\n",
    "\n",
    "# ── Chart 3: Iterative Refinement Quality ──────────────────────\n",
    "ax3 = axes[2]\n",
    "\n",
    "rounds = [0, 1, 2, 3]\n",
    "quality_scores = [8, 14, 18, 22]  # simulated total quality scores out of 25\n",
    "word_counts = [15, 28, 45, 62]    # simulated word counts\n",
    "\n",
    "line1 = ax3.plot(rounds, quality_scores, 'o-', color='#27ae60',\n",
    "                 linewidth=2.5, markersize=8, label='Quality Score')\n",
    "ax3.set_ylabel('Quality Score (/25)', fontsize=11, color='#27ae60')\n",
    "ax3.set_ylim(0, 25)\n",
    "ax3.tick_params(axis='y', labelcolor='#27ae60')\n",
    "\n",
    "ax3_twin = ax3.twinx()\n",
    "line2 = ax3_twin.plot(rounds, word_counts, 's--', color='#3498db',\n",
    "                       linewidth=2, markersize=7, label='Word Count')\n",
    "ax3_twin.set_ylabel('Word Count', fontsize=11, color='#3498db')\n",
    "ax3_twin.tick_params(axis='y', labelcolor='#3498db')\n",
    "\n",
    "ax3.set_xlabel('Refinement Round', fontsize=11)\n",
    "ax3.set_xticks(rounds)\n",
    "ax3.set_xticklabels(['Initial', 'Round 1', 'Round 2', 'Round 3'])\n",
    "ax3.set_title('Iterative Refinement Progress', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Combined legend\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax3.legend(lines, labels, loc='upper left', fontsize=9)\n",
    "\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3_twin.spines['top'].set_visible(False)\n",
    "\n",
    "plt.suptitle('Prompt Chain & Testing Analytics', fontsize=16,\n",
    "             fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ]
}