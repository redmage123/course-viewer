{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2: Machine Learning with PyTorch - SOLUTIONS\n\n## From Classical ML to Deep Learning Foundations\n\n**Duration:** 90-120 minutes | **Difficulty:** Intermediate | **Prerequisites:** Lab 1\n\n**This notebook contains all solutions. Use for reference after attempting the exercises.**\n\n---\n\n## Overview\n\nThis lab bridges classical machine learning and deep learning by teaching PyTorch fundamentals through hands-on implementation of ML algorithms. You'll learn to build, train, and evaluate models using the same patterns used in production deep learning systems.\n\n### Lab Structure\n\n| Part | Topic | Key Concepts |\n|------|-------|--------------|\n| **Part 1** | PyTorch Tensors | Creating tensors, tensor operations, automatic differentiation (autograd) |\n| **Part 2** | Linear Regression | Training loop from scratch, MSE loss, nn.Module, gradient descent |\n| **Part 3** | Logistic Regression | Sigmoid function, BCE loss, binary classification, decision boundaries |\n| **Part 4** | Support Vector Machines | Kernel trick (linear, RBF, polynomial), margins, support vectors |\n| **Part 5** | Model Evaluation | Confusion matrix, precision, recall, F1-score, classification report |\n\n### Key Pattern You'll Learn\n\nThe PyTorch training loop used in all deep learning:\n\n```python\nfor epoch in range(n_epochs):\n    y_pred = model(X)           # Forward pass\n    loss = criterion(y_pred, y) # Compute loss\n    optimizer.zero_grad()       # Clear gradients\n    loss.backward()             # Backward pass\n    optimizer.step()            # Update weights\n```\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Setup Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: PyTorch Tensors - Solutions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 1.1 Solution: Creating PyTorch Tensors\n\nTensors are the fundamental data structure in PyTorch - like NumPy arrays\nbut with GPU acceleration and automatic differentiation support.\n\"\"\"\n\n# Create a 1D tensor from a Python list\n# torch.tensor() converts Python/NumPy data to a PyTorch tensor\ntensor_a = torch.tensor([10, 20, 30, 40, 50])\n\n# Create a 3x4 tensor filled with zeros\n# Useful for initializing weight matrices or placeholders\ntensor_b = torch.zeros(3, 4)\n\n# Create a 2x5 tensor with random values between 0 and 1\n# torch.rand() uses uniform distribution [0, 1)\ntensor_c = torch.rand(2, 5)\n\n# Print results to verify\nprint(f\"a) {tensor_a}\")\nprint(f\"b) Shape: {tensor_b.shape}\")\nprint(f\"c) Random tensor with shape {tensor_c.shape}\")"
  },
  {
   "cell_type": "markdown",
   "source": "\"\"\"\nExercise 1.2 Solution: Tensor Operations\n\nPyTorch tensors support element-wise operations, aggregations,\nand shape transformations - all essential for neural network computations.\n\"\"\"\n\nx = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\ny = torch.tensor([[7., 8., 9.], [10., 11., 12.]])\n\nprint(\"x =\")\nprint(x)\nprint(\"\\ny =\")\nprint(y)\nprint()\n\n# Element-wise addition: adds corresponding elements\n# Same as x.add(y) or torch.add(x, y)\nresult_add = x + y\n\n# Mean of all elements: returns single scalar value\n# Can also specify dimension: x.mean(dim=0) for column means\nresult_mean = x.mean()\n\n# Sum along dimension 1 (rows): collapses columns\n# dim=0 would sum columns, dim=1 sums rows\nresult_row_sum = x.sum(dim=1)\n\n# Reshape: change tensor dimensions while preserving data\n# Total elements must remain same: 2*3 = 3*2 = 6\nresult_reshape = x.reshape(3, 2)\n\n# Print results\nprint(f\"a) x + y =\\n{result_add}\")\nprint(f\"\\nb) Mean of x = {result_mean}\")\nprint(f\"\\nc) Sum of each row = {result_row_sum}\")\nprint(f\"\\nd) x reshaped to 3x2:\\n{result_reshape}\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "\"\"\"\nExercise 1.3 Solution: Automatic Differentiation (Autograd)\n\nAutograd is PyTorch's automatic differentiation engine that powers neural network training.\nIt tracks operations on tensors and computes gradients automatically.\n\"\"\"\n\n# Create tensor with gradient tracking enabled\n# requires_grad=True tells PyTorch to track all operations on this tensor\nx = torch.tensor([2.0], requires_grad=True)\n\n# Compute y = 3x² - 4x + 5\n# PyTorch builds a computation graph as we perform operations\ny = 3*x**2 - 4*x + 5\n\n# Compute gradients via backpropagation\n# This calculates dy/dx and stores it in x.grad\ny.backward()\n\n# Print results\n# At x=2: y = 3(4) - 4(2) + 5 = 12 - 8 + 5 = 9\n# dy/dx = 6x - 4, at x=2: 6(2) - 4 = 8\nprint(f\"y = 3x² - 4x + 5 at x=2: y = {y.item()}\")\nprint(f\"dy/dx = 6x - 4 at x=2: dy/dx = {x.grad.item()}\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Code Explanation: Exercise 1.3\n\n| Line | Code | Explanation |\n|------|------|-------------|\n| 1 | `x = torch.tensor([2.0], requires_grad=True)` | **Creates tensor with gradient tracking.** PyTorch will record all operations on x for backpropagation. |\n| 2 | `y = 3*x**2 - 4*x + 5` | **Builds computation graph.** Each operation creates a node. PyTorch stores how to compute gradients. |\n| 3 | `y.backward()` | **Backpropagation.** Computes dy/dx using chain rule and stores result in x.grad. |\n| 4 | `x.grad` | **Access computed gradient.** Contains the derivative value after backward() is called. |\n\n**Mathematical Verification:**\n```\nFunction:    y = 3x² - 4x + 5\nDerivative:  dy/dx = 6x - 4\n\nAt x = 2:\n  y = 3(2)² - 4(2) + 5 = 12 - 8 + 5 = 9\n  dy/dx = 6(2) - 4 = 12 - 4 = 8\n```\n\n**The Computation Graph:**\n```\nx (leaf) → x² → 3x² → 3x² - 4x → 3x² - 4x + 5 (y)\n                ↑\n               4x ←\n```\n\n**Why autograd matters:**\n- Eliminates manual gradient calculation (error-prone for complex networks)\n- Enables training of deep networks with millions of parameters\n- Foundation of all deep learning: gradients tell us how to adjust weights",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 1.2 SOLUTION: Tensor Operations\n",
    "# ============================================\n",
    "\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
    "y = torch.tensor([[7., 8., 9.], [10., 11., 12.]])\n",
    "\n",
    "print(\"x =\")\n",
    "print(x)\n",
    "print(\"\\ny =\")\n",
    "print(y)\n",
    "print()\n",
    "\n",
    "# a) Add x and y element-wise\n",
    "result_add = x + y\n",
    "\n",
    "# b) Calculate the mean of x\n",
    "result_mean = x.mean()\n",
    "\n",
    "# c) Calculate the sum of each row of x (dim=1)\n",
    "result_row_sum = x.sum(dim=1)\n",
    "\n",
    "# d) Reshape x to be 3 rows x 2 columns\n",
    "result_reshape = x.reshape(3, 2)\n",
    "\n",
    "# ---- Test ----\n",
    "print(f\"a) x + y =\\n{result_add}\")\n",
    "print(f\"\\nb) Mean of x = {result_mean}\")\n",
    "print(f\"\\nc) Sum of each row = {result_row_sum}\")\n",
    "print(f\"\\nd) x reshaped to 3x2:\\n{result_reshape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 2.1 Solution: Linear Regression Model\n\nThis solution implements linear regression using PyTorch's nn.Module.\nThe training loop demonstrates the core pattern used in all deep learning.\n\"\"\"\n\ntorch.manual_seed(123)\n\nclass MyLinearRegression(nn.Module):\n    \"\"\"\n    Linear regression model: y = wx + b\n    \n    Uses nn.Linear which contains learnable weight and bias parameters.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        # nn.Linear(in_features, out_features) creates y = xW^T + b\n        # 1 input feature (x), 1 output (y)\n        self.linear = nn.Linear(1, 1)\n    \n    def forward(self, x):\n        # Forward pass: compute prediction given input\n        return self.linear(x)\n\n# Instantiate the model\nmy_model = MyLinearRegression()\n\n# Mean Squared Error loss: measures average squared difference\n# L = (1/n) * Σ(y_pred - y_true)²\nmy_criterion = nn.MSELoss()\n\n# Stochastic Gradient Descent optimizer\n# lr=0.01 means weights update by: w = w - 0.01 * gradient\nmy_optimizer = optim.SGD(my_model.parameters(), lr=0.01)\n\n# Training loop: the core pattern of all deep learning\nmy_losses = []\nfor epoch in range(100):\n    # STEP 1: Forward pass - compute predictions\n    y_pred = my_model(X)\n    \n    # STEP 2: Compute loss - how wrong are we?\n    loss = my_criterion(y_pred, y)\n    \n    # STEP 3: Zero gradients - clear previous gradients\n    # (gradients accumulate by default, which we don't want)\n    my_optimizer.zero_grad()\n    \n    # STEP 4: Backward pass - compute gradients via backprop\n    loss.backward()\n    \n    # STEP 5: Update weights - apply gradient descent step\n    my_optimizer.step()\n    \n    my_losses.append(loss.item())\n    \n    if (epoch + 1) % 20 == 0:\n        print(f'Epoch {epoch+1}/100 | Loss: {loss.item():.4f}')\n\n# Print learned parameters\nprint(f\"\\nLearned: w = {my_model.linear.weight.item():.4f}, b = {my_model.linear.bias.item():.4f}\")\nprint(f\"True:    w = 3.0000, b = 2.0000\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Code Explanation: Exercise 2.1\n\n**Model Definition:**\n| Line | Code | Explanation |\n|------|------|-------------|\n| 1 | `class MyLinearRegression(nn.Module)` | **Inherits from nn.Module.** All PyTorch models must inherit from this base class. |\n| 2 | `super().__init__()` | **Initializes parent class.** Required for nn.Module to set up internal tracking. |\n| 3 | `self.linear = nn.Linear(1, 1)` | **Creates linear layer.** Parameters: (input_features, output_features). Contains weight w and bias b. |\n| 4 | `def forward(self, x)` | **Defines forward pass.** Called when you do `model(x)`. Returns predictions. |\n\n**Training Components:**\n| Component | Code | Purpose |\n|-----------|------|---------|\n| Loss Function | `nn.MSELoss()` | Measures prediction error: mean of (prediction - target)² |\n| Optimizer | `optim.SGD(..., lr=0.01)` | Updates weights using gradients. lr = learning rate. |\n\n**Training Loop (The 5 Essential Steps):**\n```python\nfor epoch in range(n_epochs):\n    y_pred = model(X)           # 1. Forward: compute predictions\n    loss = criterion(y_pred, y) # 2. Loss: how wrong are we?\n    optimizer.zero_grad()       # 3. Zero: clear old gradients\n    loss.backward()             # 4. Backward: compute new gradients\n    optimizer.step()            # 5. Update: adjust weights\n```\n\n**Why each step matters:**\n1. **Forward pass** - runs input through model to get predictions\n2. **Compute loss** - quantifies error (lower is better)\n3. **Zero gradients** - gradients accumulate by default; we want fresh ones\n4. **Backward pass** - autograd computes ∂loss/∂weights\n5. **Optimizer step** - applies gradient descent: w_new = w_old - lr × gradient",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Linear Regression - Solutions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 3.1 Solution: Logistic Regression Classifier\n\nLogistic regression extends linear regression for binary classification\nby applying sigmoid function to squash outputs to [0, 1] probabilities.\n\"\"\"\n\ntorch.manual_seed(42)\n\nclass MyClassifier(nn.Module):\n    \"\"\"\n    Binary classifier using logistic regression.\n    \n    Architecture: Linear layer → Sigmoid activation\n    Output is probability of class 1.\n    \"\"\"\n    def __init__(self, input_dim):\n        super().__init__()\n        # Linear layer: maps input features to single output\n        self.linear = nn.Linear(input_dim, 1)\n        # Sigmoid: squashes output to (0, 1) range\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x):\n        # Apply linear transformation then sigmoid\n        z = self.linear(x)    # z = wx + b (can be any real number)\n        return self.sigmoid(z) # probability in (0, 1)\n\n# Create model with 2 input features\nmy_classifier = MyClassifier(input_dim=2)\n\n# Binary Cross Entropy loss for classification\n# Penalizes confident wrong predictions heavily\nmy_bce_loss = nn.BCELoss()\n\n# Adam optimizer: adaptive learning rate, often faster than SGD\nmy_opt = optim.Adam(my_classifier.parameters(), lr=0.1)\n\n# Training loop\nfor epoch in range(100):\n    # Forward pass: get predicted probabilities\n    y_pred = my_classifier(X_train_t)\n    \n    # Compute BCE loss\n    loss = my_bce_loss(y_pred, y_train_t)\n    \n    # Standard backprop steps\n    my_opt.zero_grad()\n    loss.backward()\n    my_opt.step()\n    \n    if (epoch + 1) % 20 == 0:\n        # Calculate accuracy: threshold at 0.5\n        acc = ((y_pred >= 0.5).float() == y_train_t).float().mean()\n        print(f'Epoch {epoch+1}/100 | Loss: {loss.item():.4f} | Acc: {acc.item():.4f}')\n\n# Evaluate on test set\nmy_classifier.eval()  # Switch to evaluation mode\nwith torch.no_grad():  # Disable gradient tracking for inference\n    y_pred = my_classifier(X_test_t)\n    y_class = (y_pred >= 0.5).float()  # Convert probabilities to 0/1\n    acc = (y_class == y_test_t).float().mean()\nprint(f\"\\nTest Accuracy: {acc.item():.4f}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Code Explanation: Exercise 3.1\n\n**Model Architecture:**\n| Component | Code | Purpose |\n|-----------|------|---------|\n| Linear layer | `nn.Linear(input_dim, 1)` | Computes z = w₁x₁ + w₂x₂ + b |\n| Sigmoid | `nn.Sigmoid()` | Converts z to probability: σ(z) = 1/(1+e^(-z)) |\n\n**The Sigmoid Function:**\n```\nσ(z) = 1 / (1 + e^(-z))\n\nz = -∞ → σ(z) = 0\nz = 0  → σ(z) = 0.5\nz = +∞ → σ(z) = 1\n```\n\n**Loss and Optimizer:**\n| Component | Code | Explanation |\n|-----------|------|-------------|\n| `nn.BCELoss()` | Binary Cross Entropy | L = -[y·log(p) + (1-y)·log(1-p)]. Heavily penalizes confident wrong predictions. |\n| `optim.Adam()` | Adaptive optimizer | Combines momentum and adaptive learning rates. Often faster than SGD. |\n\n**Inference Mode:**\n| Code | Purpose |\n|------|---------|\n| `model.eval()` | Disables dropout, uses running stats for batch norm |\n| `torch.no_grad()` | Disables gradient tracking (faster, less memory) |\n| `y_pred >= 0.5` | Converts probabilities to class labels |\n\n**Why logistic regression for classification:**\n- Linear regression outputs can be any real number\n- Classification needs probabilities (0 to 1)\n- Sigmoid transforms linear output to valid probability\n- Decision boundary: predict 1 if probability ≥ 0.5",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 2.1 SOLUTION: Build Linear Regression\n",
    "# ============================================\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the model\n",
    "class MyLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # SOLUTION: Create a linear layer (1 input, 1 output)\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # SOLUTION: Return the output of the linear layer\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create model, loss function, and optimizer\n",
    "my_model = MyLinearRegression()\n",
    "my_criterion = nn.MSELoss()  # SOLUTION\n",
    "my_optimizer = optim.SGD(my_model.parameters(), lr=0.01)  # SOLUTION\n",
    "\n",
    "# Training loop\n",
    "my_losses = []\n",
    "for epoch in range(100):\n",
    "    # SOLUTION: Complete the training loop\n",
    "    # 1. Forward pass\n",
    "    y_pred = my_model(X)\n",
    "    \n",
    "    # 2. Compute loss\n",
    "    loss = my_criterion(y_pred, y)\n",
    "    \n",
    "    # 3. Zero gradients\n",
    "    my_optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Update weights\n",
    "    my_optimizer.step()\n",
    "    \n",
    "    my_losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch {epoch+1}/100 | Loss: {loss.item():.4f}')\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nLearned: w = {my_model.linear.weight.item():.4f}, b = {my_model.linear.bias.item():.4f}\")\n",
    "print(f\"True:    w = 3.0000, b = 2.0000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\"\"\"\nExercise 4.1 Solution: Support Vector Machine\n\nSVMs find the optimal hyperplane that separates classes with maximum margin.\nThe RBF kernel enables non-linear decision boundaries.\n\"\"\"\n\n# Create SVM with RBF (Radial Basis Function) kernel\n# kernel='rbf': uses Gaussian kernel for non-linear boundaries\n# C=10: regularization parameter (higher = less regularization, fits training data more closely)\nmy_svm = SVC(kernel='rbf', C=10)\n\n# Train the SVM on the training data\n# SVM finds support vectors: points closest to decision boundary\nmy_svm.fit(X_train_m, y_train_m)\n\n# Evaluate on test data\n# .score() returns accuracy: (correct predictions / total)\naccuracy = my_svm.score(X_test_m, y_test_m)\n\n# Get number of support vectors\n# These are the training points that define the decision boundary\nn_support = len(my_svm.support_vectors_)\n\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Number of Support Vectors: {n_support}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Code Explanation: Exercise 4.1\n\n| Line | Code | Explanation |\n|------|------|-------------|\n| 1 | `SVC(kernel='rbf', C=10)` | **Creates SVM classifier.** RBF kernel enables non-linear boundaries. C controls regularization. |\n| 2 | `my_svm.fit(X_train_m, y_train_m)` | **Trains the SVM.** Finds optimal hyperplane and identifies support vectors. |\n| 3 | `my_svm.score(X_test_m, y_test_m)` | **Computes test accuracy.** Returns fraction of correctly classified samples. |\n| 4 | `my_svm.support_vectors_` | **Accesses support vectors.** Training points that lie on or within the margin. |\n\n**SVM Kernels:**\n| Kernel | Formula | Use Case |\n|--------|---------|----------|\n| `linear` | K(x,y) = x·y | Linearly separable data |\n| `rbf` | K(x,y) = exp(-γ‖x-y‖²) | Most common, handles non-linear |\n| `poly` | K(x,y) = (x·y + c)^d | Polynomial decision boundaries |\n\n**The C Parameter:**\n```\nC = 0.1  → High regularization, wider margin, may underfit\nC = 1    → Balanced (default)\nC = 10   → Low regularization, narrow margin, fits training data closely\nC = 100  → Very low regularization, may overfit\n```\n\n**What are Support Vectors?**\n- Training points that lie on or inside the margin boundary\n- The decision boundary depends ONLY on these points\n- Removing non-support vectors doesn't change the model\n- Fewer support vectors = simpler, more generalizable model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Generate classification data\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=300, n_features=2, n_redundant=0, n_informative=2,\n",
    "    n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExercise 5.1 Solution: Model Evaluation\n\nThis solution demonstrates comprehensive model evaluation using\nconfusion matrix and classification report for detailed metrics.\n\"\"\"\n\n# Set model to evaluation mode and get predictions\nlog_model.eval()\nwith torch.no_grad():\n    # Get probability predictions\n    y_pred_prob = log_model(X_test_t)\n    # Convert to class labels: 1 if probability >= 0.5, else 0\n    y_pred_log = (y_pred_prob >= 0.5).numpy().astype(int).flatten()\n\n# Create confusion matrix\n# Rows = actual class, Columns = predicted class\n# [[TN, FP], [FN, TP]]\ncm_log = confusion_matrix(y_test, y_pred_log)\n\nprint(\"Confusion Matrix:\")\nprint(cm_log)\nprint()\n\n# Print detailed classification report\n# Shows precision, recall, and F1-score for each class\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred_log, target_names=['Class 0', 'Class 1']))"
  },
  {
   "cell_type": "markdown",
   "source": "### Code Explanation: Exercise 5.1\n\n| Line | Code | Explanation |\n|------|------|-------------|\n| 1 | `confusion_matrix(y_test, y_pred_log)` | **Creates 2×2 confusion matrix.** Compares actual vs predicted labels. |\n| 2 | `classification_report(...)` | **Generates detailed metrics.** Precision, recall, F1-score per class. |\n| 3 | `target_names=['Class 0', 'Class 1']` | **Labels for the report.** Makes output more readable. |\n\n**Understanding the Confusion Matrix:**\n```\n                  Predicted\n                  0     1\nActual  0      [[TN,   FP],    TN = True Negative (correct 0)\n        1       [FN,   TP]]    FP = False Positive (wrong 1)\n                               FN = False Negative (wrong 0)\n                               TP = True Positive (correct 1)\n```\n\n**Classification Metrics:**\n| Metric | Formula | Interpretation |\n|--------|---------|----------------|\n| **Precision** | TP / (TP + FP) | Of predicted positives, how many are correct? |\n| **Recall** | TP / (TP + FN) | Of actual positives, how many did we find? |\n| **F1-Score** | 2 × (P × R) / (P + R) | Harmonic mean of precision and recall |\n| **Support** | Count | Number of actual instances per class |\n\n**When to prioritize each metric:**\n- **Precision**: When false positives are costly (spam detection)\n- **Recall**: When false negatives are costly (disease detection)\n- **F1-Score**: When you need balance between precision and recall\n- **Accuracy**: When classes are balanced and errors are equally costly",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Support Vector Machines - Solutions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Generate moons data\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 4.1 SOLUTION: Train an SVM\n",
    "# ============================================\n",
    "\n",
    "# a) Create an RBF SVM with C=10\n",
    "my_svm = SVC(kernel='rbf', C=10)  # SOLUTION\n",
    "\n",
    "# b) Fit it on the training data\n",
    "my_svm.fit(X_train_m, y_train_m)  # SOLUTION\n",
    "\n",
    "# c) Calculate test accuracy\n",
    "accuracy = my_svm.score(X_test_m, y_test_m)\n",
    "n_support = len(my_svm.support_vectors_)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Number of Support Vectors: {n_support}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Model Evaluation - Solutions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Train a logistic regression model for evaluation\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "\n",
    "log_model = LogisticRegressionModel(input_dim=2)\n",
    "criterion_bce = nn.BCELoss()\n",
    "optimizer_log = optim.Adam(log_model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred_prob = log_model(X_train_t)\n",
    "    loss = criterion_bce(y_pred_prob, y_train_t)\n",
    "    optimizer_log.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_log.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCISE 5.1 SOLUTION: Evaluate Your Model\n",
    "# ============================================\n",
    "\n",
    "# Get predictions from the logistic regression model\n",
    "log_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_prob = log_model(X_test_t)\n",
    "    y_pred_log = (y_pred_prob >= 0.5).numpy().astype(int).flatten()\n",
    "\n",
    "# a) Create confusion matrix\n",
    "cm_log = confusion_matrix(y_test, y_pred_log)  # SOLUTION\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_log)\n",
    "print()\n",
    "\n",
    "# b) Print classification report\n",
    "print(\"Classification Report:\")  # SOLUTION\n",
    "print(classification_report(y_test, y_pred_log, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Solutions Summary\n",
    "\n",
    "## Exercise 1.1: Create Tensors\n",
    "```python\n",
    "tensor_a = torch.tensor([10, 20, 30, 40, 50])\n",
    "tensor_b = torch.zeros(3, 4)\n",
    "tensor_c = torch.rand(2, 5)\n",
    "```\n",
    "\n",
    "## Exercise 1.2: Tensor Operations\n",
    "```python\n",
    "result_add = x + y\n",
    "result_mean = x.mean()\n",
    "result_row_sum = x.sum(dim=1)\n",
    "result_reshape = x.reshape(3, 2)\n",
    "```\n",
    "\n",
    "## Exercise 1.3: Autograd\n",
    "```python\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = 3*x**2 - 4*x + 5\n",
    "y.backward()\n",
    "```\n",
    "\n",
    "## Exercise 2.1: Linear Regression\n",
    "```python\n",
    "self.linear = nn.Linear(1, 1)\n",
    "return self.linear(x)\n",
    "my_criterion = nn.MSELoss()\n",
    "my_optimizer = optim.SGD(my_model.parameters(), lr=0.01)\n",
    "```\n",
    "\n",
    "## Exercise 3.1: Logistic Regression\n",
    "```python\n",
    "self.linear = nn.Linear(input_dim, 1)\n",
    "self.sigmoid = nn.Sigmoid()\n",
    "return self.sigmoid(self.linear(x))\n",
    "my_bce_loss = nn.BCELoss()\n",
    "my_opt = optim.Adam(my_classifier.parameters(), lr=0.1)\n",
    "```\n",
    "\n",
    "## Exercise 4.1: SVM\n",
    "```python\n",
    "my_svm = SVC(kernel='rbf', C=10)\n",
    "my_svm.fit(X_train_m, y_train_m)\n",
    "```\n",
    "\n",
    "## Exercise 5.1: Evaluation\n",
    "```python\n",
    "cm_log = confusion_matrix(y_test, y_pred_log)\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}