{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 3: Generative AI with Ollama - SOLUTIONS\n",
    "\n",
    "**Duration:** 90-120 minutes | **Difficulty:** Intermediate to Advanced\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "1. Connect to and use Ollama for local LLM inference\n",
    "2. Generate text using the Llama model\n",
    "3. Apply prompt engineering techniques for better results\n",
    "4. Control generation with temperature and other parameters\n",
    "5. Build multi-turn conversations with chat history\n",
    "6. Implement Retrieval-Augmented Generation (RAG)\n",
    "7. Understand fine-tuning concepts with LoRA and QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# Check connection to Ollama\n",
    "try:\n",
    "    models = ollama.list()\n",
    "    print(\"Connected to Ollama!\")\n",
    "    print(\"\\nAvailable models:\")\n",
    "    for model in models.get('models', []):\n",
    "        print(f\"  - {model['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama: {e}\")\n",
    "    print(\"\\nMake sure Ollama is running: ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Basic Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text generation\n",
    "response = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt='What is machine learning in one sentence?'\n",
    ")\n",
    "print(\"Response:\")\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Generate Your First Response - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "my_response = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt='What is an API?'\n",
    ")\n",
    "answer = my_response['response']\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Write a Role-Based Prompt - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "chef_prompt = \"\"\"You are a professional chef who specializes in quick, easy meals \n",
    "that anyone can make at home with common ingredients.\n",
    "\n",
    "Suggest a simple dinner recipe that can be made in under 30 minutes.\n",
    "Include a list of ingredients and brief cooking instructions.\"\"\"\n",
    "\n",
    "recipe_response = ollama.generate(model='llama3.2', prompt=chef_prompt)\n",
    "\n",
    "if recipe_response:\n",
    "    print(recipe_response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Generate JSON Output - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "movie_prompt = \"\"\"Generate information about a famous movie.\n",
    "\n",
    "Respond with ONLY valid JSON in this exact format:\n",
    "{\"title\": \"...\", \"director\": \"...\", \"year\": YYYY, \"rating\": X.X}\n",
    "\n",
    "The rating should be out of 10. Do not include any other text, just the JSON.\"\"\"\n",
    "\n",
    "movie_response = ollama.generate(model='llama3.2', prompt=movie_prompt)\n",
    "\n",
    "try:\n",
    "    movie_data = json.loads(movie_response['response'].strip())\n",
    "except:\n",
    "    movie_data = None\n",
    "\n",
    "if movie_data:\n",
    "    print(\"Movie data:\")\n",
    "    for key, value in movie_data.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Generation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Exercise 3.1: Experiment with Temperature - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "creative_prompt = \"Suggest a creative and unique name for a coffee shop.\"\n",
    "\n",
    "low_temp_response = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt=creative_prompt,\n",
    "    options={'temperature': 0.2}\n",
    ")\n",
    "\n",
    "high_temp_response = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt=creative_prompt,\n",
    "    options={'temperature': 0.9}\n",
    ")\n",
    "\n",
    "print(\"Low temp (0.2):\")\n",
    "print(low_temp_response['response'])\n",
    "print(\"\\nHigh temp (0.9):\")\n",
    "print(high_temp_response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Chat Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Create a Multi-Turn Conversation - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'My favorite color is blue.'}\n",
    "]\n",
    "\n",
    "response1 = ollama.chat(model='llama3.2', messages=messages)\n",
    "\n",
    "if response1:\n",
    "    print(\"Assistant:\", response1['message']['content'])\n",
    "    \n",
    "    messages.append(response1['message'])\n",
    "    messages.append({'role': 'user', 'content': 'What is my favorite color?'})\n",
    "    \n",
    "    response2 = ollama.chat(model='llama3.2', messages=messages)\n",
    "    \n",
    "    if response2:\n",
    "        print(\"\\nAssistant:\", response2['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Create a Specialized Chatbot - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "system_prompt = \"\"\"You are a helpful and patient Python programming tutor. \n",
    "You explain concepts clearly using simple language and always provide code examples.\n",
    "When answering questions, you break down complex topics into easy-to-understand steps.\n",
    "You encourage learning and provide helpful tips.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': system_prompt},\n",
    "    {'role': 'user', 'content': 'What is a list comprehension in Python?'}\n",
    "]\n",
    "\n",
    "tutor_response = ollama.chat(model='llama3.2', messages=messages)\n",
    "\n",
    "if tutor_response:\n",
    "    print(\"Python Tutor:\")\n",
    "    print(tutor_response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Building a Simple Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Exercise 5.1: Build a Sentiment Analyzer - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def analyze_sentiment(text):\n",
    "    prompt = f\"\"\"Analyze the sentiment of the following text.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Respond with ONLY valid JSON in this exact format:\n",
    "{{\"sentiment\": \"positive/negative/neutral\", \"confidence\": \"high/medium/low\"}}\n",
    "\n",
    "Do not include any other text.\"\"\"\n",
    "    \n",
    "    response = ollama.generate(\n",
    "        model='llama3.2',\n",
    "        prompt=prompt,\n",
    "        options={'temperature': 0.1}\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        return json.loads(response['response'].strip())\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "test_texts = [\n",
    "    \"I absolutely love this product! Best purchase ever!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"The weather today is cloudy.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = analyze_sentiment(text)\n",
    "    if result:\n",
    "        print(f\"Text: {text[:50]}...\")\n",
    "        print(f\"  Sentiment: {result.get('sentiment')}\")\n",
    "        print(f\"  Confidence: {result.get('confidence')}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Exercise 5.2: Build a Q&A Bot - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def answer_question(context, question):\n",
    "    prompt = f\"\"\"Answer the question based ONLY on the provided context.\n",
    "If the answer is not in the context, say \"I don't know based on the provided context.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = ollama.generate(\n",
    "        model='llama3.2',\n",
    "        prompt=prompt,\n",
    "        options={'temperature': 0.2}\n",
    "    )\n",
    "    \n",
    "    return response['response'].strip()\n",
    "\n",
    "context = \"\"\"\n",
    "Python was created by Guido van Rossum and first released in 1991. \n",
    "It emphasizes code readability and uses significant indentation. \n",
    "Python supports multiple programming paradigms including procedural, \n",
    "object-oriented, and functional programming. The language is named \n",
    "after the British comedy group Monty Python.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Who created Python?\",\n",
    "    \"When was Python first released?\",\n",
    "    \"What is Python's mascot?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    answer = answer_question(context, q)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {answer}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for RAG\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = ollama.embed(model='llama3.2', input=text)\n",
    "    return response['embeddings'][0]\n",
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self, model='llama3.2'):\n",
    "        self.model = model\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def add_documents(self, docs: List[str]):\n",
    "        for doc in docs:\n",
    "            embedding = get_embedding(doc)\n",
    "            self.documents.append(doc)\n",
    "            self.embeddings.append(embedding)\n",
    "        print(f\"Added {len(docs)} documents. Total: {len(self.documents)}\")\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 2) -> List[str]:\n",
    "        query_embedding = get_embedding(query)\n",
    "        similarities = []\n",
    "        for i, doc_embedding in enumerate(self.embeddings):\n",
    "            sim = cosine_similarity(query_embedding, doc_embedding)\n",
    "            similarities.append((sim, i))\n",
    "        similarities.sort(reverse=True)\n",
    "        top_indices = [idx for _, idx in similarities[:top_k]]\n",
    "        return [self.documents[i] for i in top_indices]\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 2) -> str:\n",
    "        relevant_docs = self.retrieve(question, top_k)\n",
    "        context = \"\\n\\n\".join(relevant_docs)\n",
    "        prompt = f\"\"\"Use the following context to answer the question. \n",
    "If the answer is not in the context, say \"I don't have information about that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        response = ollama.generate(model=self.model, prompt=prompt, options={'temperature': 0.3})\n",
    "        return response['response'].strip()\n",
    "\n",
    "# Create and populate RAG system\n",
    "rag = SimpleRAG()\n",
    "knowledge_base = [\n",
    "    \"The Eiffel Tower is located in Paris, France. It was built in 1889 and stands 330 meters tall.\",\n",
    "    \"The Great Wall of China is over 21,000 kilometers long and was built over many centuries.\",\n",
    "    \"Python programming language was created by Guido van Rossum and released in 1991.\",\n",
    "    \"Machine learning is a subset of AI that enables computers to learn from data.\",\n",
    "    \"The Amazon rainforest produces about 20% of the world's oxygen.\"\n",
    "]\n",
    "rag.add_documents(knowledge_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Exercise 6.1: Extend the RAG Knowledge Base - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "my_documents = [\n",
    "    \"The Olympic Games originated in ancient Greece around 776 BC.\",\n",
    "    \"Basketball was invented by James Naismith in 1891 in Springfield, Massachusetts.\",\n",
    "    \"The FIFA World Cup is held every four years and is the most watched sporting event.\",\n",
    "    \"Tennis uses a scoring system of 15, 30, 40, and game points.\",\n",
    "    \"The marathon race is 26.2 miles long, commemorating the legend of Pheidippides.\"\n",
    "]\n",
    "\n",
    "rag.add_documents(my_documents)\n",
    "\n",
    "my_questions = [\n",
    "    \"Who invented basketball?\",\n",
    "    \"How long is a marathon?\",\n",
    "    \"When did the Olympic Games start?\"\n",
    "]\n",
    "\n",
    "for q in my_questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    answer = rag.query(q)\n",
    "    print(f\"A: {answer}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Exercise 6.2: Implement Document Chunking - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def chunk_text(text: str, chunk_size: int = 100, overlap: int = 20) -> List[str]:\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    if len(words) <= chunk_size:\n",
    "        return [text.strip()]\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk = ' '.join(words[start:end])\n",
    "        chunks.append(chunk.strip())\n",
    "        \n",
    "        if end >= len(words):\n",
    "            break\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test\n",
    "long_document = \"\"\"\n",
    "Artificial intelligence has transformed the technology landscape dramatically over the past decade. \n",
    "Machine learning algorithms now power everything from recommendation systems to autonomous vehicles.\n",
    "Deep learning, a subset of machine learning, uses neural networks with many layers to learn complex patterns.\n",
    "Natural language processing enables computers to understand and generate human language.\n",
    "Computer vision allows machines to interpret and analyze visual information from the world.\n",
    "Reinforcement learning teaches agents to make decisions through trial and error.\n",
    "The field continues to advance rapidly, with new breakthroughs announced regularly.\n",
    "Ethical considerations around AI bias and fairness have become increasingly important.\n",
    "Researchers are working on making AI systems more transparent and explainable.\n",
    "The future of AI holds both tremendous promise and significant challenges for society.\n",
    "\"\"\"\n",
    "\n",
    "chunks = chunk_text(long_document, chunk_size=50, overlap=10)\n",
    "print(f\"Created {len(chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk[:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Fine-tuning Concepts with LoRA and QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data formats demonstration\n",
    "instruction_format = {\n",
    "    \"instruction\": \"Summarize the following text in one sentence.\",\n",
    "    \"input\": \"The quick brown fox jumps over the lazy dog. This sentence contains every letter.\",\n",
    "    \"output\": \"A pangram sentence featuring a fox demonstrates all 26 alphabet letters.\"\n",
    "}\n",
    "\n",
    "chat_format = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How do I reverse a string in Python?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Use slicing: `reversed_string = original_string[::-1]`\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Instruction Format:\")\n",
    "print(json.dumps(instruction_format, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Exercise 7.1: Create Training Data - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Fitness Coach Training Data\n",
    "my_training_data = [\n",
    "    {\n",
    "        \"instruction\": \"Provide fitness advice for a beginner.\",\n",
    "        \"input\": \"I want to start exercising but I've never worked out before. Where do I begin?\",\n",
    "        \"output\": \"Start with 20-30 minutes of walking 3-4 times per week. Once comfortable, add bodyweight exercises like squats, push-ups, and planks. Focus on consistency over intensity - it's better to do a little regularly than to overdo it and quit.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Suggest a workout modification.\",\n",
    "        \"input\": \"I have bad knees and can't do regular squats. What can I do instead?\",\n",
    "        \"output\": \"Try wall sits or chair-assisted squats to reduce knee strain. You can also do glute bridges, which work similar muscles without stressing your knees. Swimming and cycling are excellent low-impact alternatives for leg strength.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Provide nutrition advice for fitness.\",\n",
    "        \"input\": \"What should I eat before a morning workout?\",\n",
    "        \"output\": \"Have a light snack 30-60 minutes before: a banana, toast with peanut butter, or a small bowl of oatmeal. Avoid heavy meals. If exercising within 30 minutes of waking, just have water - your body can use stored energy for moderate workouts.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Create a simple workout plan.\",\n",
    "        \"input\": \"Can you give me a quick 15-minute home workout?\",\n",
    "        \"output\": \"Try this circuit: 1 minute jumping jacks, 10 squats, 10 push-ups (or knee push-ups), 30-second plank, 10 lunges each leg, rest 30 seconds. Repeat twice. This hits all major muscle groups and gets your heart rate up!\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Motivate someone who wants to give up.\",\n",
    "        \"input\": \"I've been working out for 2 weeks and haven't lost any weight. Should I quit?\",\n",
    "        \"output\": \"Don't quit! Two weeks is too early to see major results - your body is adapting. Focus on non-scale victories: more energy, better sleep, improved mood. Muscle weighs more than fat, so you might be recomposing. Take progress photos and measurements instead.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def validate_training_data(data):\n",
    "    required_keys = ['instruction', 'input', 'output']\n",
    "    for i, example in enumerate(data):\n",
    "        for key in required_keys:\n",
    "            if key not in example or example[key] is None:\n",
    "                print(f\"Example {i+1} missing '{key}'\")\n",
    "                return False\n",
    "    print(f\"All {len(data)} examples are valid!\")\n",
    "    return True\n",
    "\n",
    "validate_training_data(my_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Exercise 7.2: Design a LoRA Configuration - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - LoRA config for fitness coach\n",
    "my_lora_config = {\n",
    "    \"r\": 16,                # Medium rank - good balance for instruction following\n",
    "    \"lora_alpha\": 32,       # 2x the rank as recommended\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    \"lora_dropout\": 0.05,   # Light dropout for regularization\n",
    "}\n",
    "\n",
    "my_training_config = {\n",
    "    \"num_epochs\": 3,        # Enough for small dataset to converge\n",
    "    \"batch_size\": 4,        # Small batch for limited GPU memory\n",
    "    \"learning_rate\": 2e-4,  # Standard LoRA learning rate\n",
    "    \"warmup_ratio\": 0.05,   # 5% warmup\n",
    "}\n",
    "\n",
    "print(\"Fitness Coach LoRA Config:\")\n",
    "print(json.dumps(my_lora_config, indent=2))\n",
    "print(\"\\nTraining Config:\")\n",
    "print(json.dumps(my_training_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "---\n",
    "# Lab Complete!\n",
    "\n",
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- **Basic Generation**: Use `ollama.generate()` for text completion\n",
    "- **Prompt Engineering**: Role-based prompts, structured output, JSON responses\n",
    "- **Parameters**: Control creativity with temperature\n",
    "- **Chat API**: Multi-turn conversations with `ollama.chat()`\n",
    "- **Applications**: Build summarizers, sentiment analyzers, and Q&A bots\n",
    "- **RAG**: Implement retrieval-augmented generation with embeddings\n",
    "- **Fine-tuning**: Understand LoRA/QLoRA for efficient model adaptation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
