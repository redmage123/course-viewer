{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Sensitive Data Detection & AI Cataloguing\n",
    "\n",
    "**Data Discovery: Harnessing AI, AGI & Vector Databases - Day 2**\n",
    "\n",
    "| Duration | Difficulty | Framework | Exercises |\n",
    "|---|---|---|---|\n",
    "| 90 min | Intermediate | pandas, re, spacy, scikit-learn, chromadb, matplotlib | 5 |\n",
    "\n",
    "In this lab, you'll practice:\n",
    "- Scanning text for PII using regex patterns\n",
    "- Using spaCy NER for entity extraction and hybrid detection\n",
    "- Computing automated risk scores for data assets\n",
    "- Building a compliance dashboard with matplotlib\n",
    "- Integrating risk metadata into a vector catalogue\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Student Notes & Background\n\n### Why Sensitive Data Detection Matters\n\nEvery organisation handles data that, if exposed, could harm individuals or violate regulations. **Personally Identifiable Information (PII)** — Social Security numbers, credit card numbers, email addresses, phone numbers, medical records — is scattered across documents, databases, and emails, often without anyone knowing exactly where it lives.\n\nSensitive data detection is the process of **automatically scanning** data assets to find PII and other regulated content. This is a critical building block for:\n- **GDPR compliance** — knowing where EU personal data resides\n- **HIPAA compliance** — protecting patient health information\n- **PCI-DSS compliance** — securing credit card data\n- **CCPA compliance** — enabling data subject access and deletion requests\n\nManual detection doesn't scale. An enterprise with thousands of documents needs automated scanning that combines **pattern matching** (regex) with **AI-based entity recognition** (NER) for comprehensive coverage.\n\n### Key Concepts\n\n#### 1. Regex-Based PII Scanning\n**Regular expressions** are the first line of defence for PII detection. Common patterns include:\n\n| PII Type | Pattern | Example |\n|---|---|---|\n| **SSN** | `\\d{3}-\\d{2}-\\d{4}` | 123-45-6789 |\n| **Credit Card** | `\\d{4}-\\d{4}-\\d{4}-\\d{4}` | 4532-1234-5678-9012 |\n| **Email** | `[\\w.+-]+@[\\w-]+\\.[\\w.]+` | john.smith@company.com |\n| **Phone** | `\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}` | (555) 123-4567 |\n\n**Strengths:** Fast, deterministic, high precision for well-formatted data.\n**Weaknesses:** Cannot detect names, organisations, or locations. Misses non-standard formats. Produces false positives on data that matches the pattern but isn't PII (e.g., a product ID that looks like an SSN).\n\n#### 2. Named Entity Recognition (NER) with spaCy\n**NER** uses a trained neural network to identify and classify entities in text. spaCy's `en_core_web_sm` model recognises:\n\n| Entity Type | Description | Examples |\n|---|---|---|\n| **PERSON** | Named individuals | \"John Smith\", \"Dr. Garcia\" |\n| **ORG** | Organisations | \"Acme Corp\", \"MedPlus Health\" |\n| **GPE** | Geopolitical entities (cities, countries) | \"New York\", \"Seattle\" |\n\nNER complements regex by detecting PII types that have no fixed format — you can't write a regex for every possible person's name. The combination of regex + NER is called **hybrid detection** and achieves much higher recall than either method alone.\n\n#### 3. Risk Scoring\nA **risk score** (0–100) quantifies how sensitive a document is based on the types and volume of PII found. Typical scoring weights reflect the severity of potential harm:\n\n| Factor | Points | Rationale |\n|---|---|---|\n| SSN found | +30 | Direct identity theft risk |\n| Credit card found | +25 | Financial fraud risk |\n| Email found | +10 | Phishing/spam risk |\n| Phone found | +10 | Social engineering risk |\n| PERSON entities | +5 each (max 15) | Identity linkage risk |\n| Medical document | +15 | HIPAA regulatory exposure |\n| Financial document | +10 | PCI-DSS/SOX exposure |\n\nScores are capped at 100 and mapped to **risk tiers**: Critical (76–100), High (51–75), Medium (26–50), Low (0–25). Risk tiers drive prioritisation — Critical assets get remediated first.\n\n#### 4. Compliance Dashboards\nA **compliance dashboard** visualises the PII landscape across your document corpus. Effective dashboards answer four questions at a glance:\n1. **What PII do we have?** — Distribution of PII types (SSN, credit card, email, etc.)\n2. **How risky is our data?** — Risk tier breakdown (Critical/High/Medium/Low)\n3. **Which departments are most exposed?** — Average risk by document type\n4. **Which regulations apply?** — Count of documents subject to GDPR, HIPAA, PCI-DSS, CCPA\n\n#### 5. Vector Catalogue with Risk Metadata\nBuilding on Lab 1's vector catalogue, this lab adds **risk metadata** to each document's embedding entry. This enables **filtered semantic search** — for example, \"find employee personal data\" filtered to only Critical-risk documents. ChromaDB supports `where` clauses that filter on metadata fields before computing similarity, making these queries efficient even on large catalogues.\n\n### What You'll Build\n\nIn this lab, you will:\n1. **Scan** 200 synthetic enterprise documents for PII using regex patterns for SSNs, credit cards, emails, and phone numbers\n2. **Extract** named entities (PERSON, ORG, GPE) using spaCy NER and combine with regex findings for hybrid detection\n3. **Compute** a 0–100 risk score for each document based on PII types, volume, and document category\n4. **Build** a 2×2 compliance dashboard showing PII distribution, risk tiers, department risk, and regulatory exposure\n5. **Create** a ChromaDB vector catalogue with risk metadata and perform filtered semantic queries\n\n### Prerequisites\n- Completion of Lab 1 (or familiarity with pandas, TF-IDF, and ChromaDB basics)\n- Understanding of regular expressions (basic pattern matching)\n- No prior NLP experience required — spaCy usage is introduced step by step\n\n### Tips\n- When writing regex patterns, use `\\b` word boundaries to avoid matching substrings (e.g., matching \"123-45-6789\" inside a longer number)\n- spaCy's small model (`en_core_web_sm`) is fast but less accurate than larger models — expect some missed entities\n- The risk scoring formula is deliberately simple; in production you'd weight factors based on your organisation's specific regulatory exposure\n- For the compliance dashboard, use `plt.suptitle()` for an overall title and `plt.tight_layout()` to prevent label overlap\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "\n",
    "# ML & Vector DB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Synthetic Documents\n",
    "\n",
    "We'll create ~200 synthetic text documents that simulate HR memos, financial reports, medical forms, and other enterprise content containing various types of PII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "first_names = ['John', 'Jane', 'Robert', 'Maria', 'David', 'Sarah', 'Michael', 'Emily', 'James', 'Lisa']\n",
    "last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis', 'Rodriguez', 'Wilson']\n",
    "companies = ['Acme Corp', 'GlobalTech', 'MedPlus Health', 'FinanceFirst', 'DataDriven Inc']\n",
    "cities = ['New York', 'San Francisco', 'Chicago', 'Boston', 'Seattle', 'Austin', 'Denver', 'Atlanta']\n",
    "\n",
    "def random_ssn():\n",
    "    return f\"{np.random.randint(100,999)}-{np.random.randint(10,99)}-{np.random.randint(1000,9999)}\"\n",
    "\n",
    "def random_cc():\n",
    "    return f\"{np.random.randint(4000,4999)}-{np.random.randint(1000,9999)}-{np.random.randint(1000,9999)}-{np.random.randint(1000,9999)}\"\n",
    "\n",
    "def random_email(first, last):\n",
    "    domains = ['company.com', 'email.org', 'corp.net', 'enterprise.io']\n",
    "    return f\"{first.lower()}.{last.lower()}@{np.random.choice(domains)}\"\n",
    "\n",
    "def random_phone():\n",
    "    return f\"({np.random.randint(200,999)}) {np.random.randint(200,999)}-{np.random.randint(1000,9999)}\"\n",
    "\n",
    "templates = {\n",
    "    'hr_memo': [\n",
    "        \"Employee {name} (SSN: {ssn}) has been promoted to Senior Analyst effective March 2024. Contact: {email}, Phone: {phone}. Based in {city}.\",\n",
    "        \"Termination notice for {name}, SSN: {ssn}. Final paycheck to be sent to address on file. HR contact: {email}. Processed by {company}.\",\n",
    "        \"{name} from {company} submitted a leave request. Employee ID: EMP-{emp_id}. Emergency contact phone: {phone}. Location: {city}.\",\n",
    "        \"Salary adjustment memo: {name} (SSN: {ssn}) annual compensation increased to ${salary:,}. Effective date: January 2024. Department: {company}.\",\n",
    "    ],\n",
    "    'financial_report': [\n",
    "        \"Invoice #INV-{inv_id} for {company}: Payment of ${amount:,.2f} via credit card {cc}. Approved by {name}. Contact: {email}.\",\n",
    "        \"Expense report submitted by {name} ({email}) for ${amount:,.2f}. Corporate card ending {cc_last4}. Reimbursement approved by finance team at {company}.\",\n",
    "        \"Quarterly financial summary for {company}: Revenue ${amount:,.2f}. Prepared by {name}, CFO. Confidential. Phone: {phone}.\",\n",
    "        \"Wire transfer confirmation: ${amount:,.2f} sent to account ending {acct_last4} for {name} at {company}. Reference: TXN-{txn_id}.\",\n",
    "    ],\n",
    "    'medical_form': [\n",
    "        \"Patient: {name}, DOB: {dob}, SSN: {ssn}. Diagnosis: Type 2 Diabetes. Prescribed Metformin 500mg. Dr. {doctor} at {city} Medical Center.\",\n",
    "        \"Insurance claim for {name} (Member ID: MED-{med_id}). Procedure: Annual physical exam. Provider: {company} Health. Phone: {phone}.\",\n",
    "        \"Medical records request for {name}, DOB: {dob}. Records to be sent to {doctor} at {city} General Hospital. Patient email: {email}.\",\n",
    "    ],\n",
    "    'marketing_data': [\n",
    "        \"Campaign analytics report for {company}: {impressions:,} impressions, {clicks:,} clicks, {conversions} conversions. Manager: {name}, {email}.\",\n",
    "        \"Customer profile: {name}, {city}. Purchase history includes {purchases} orders. Email: {email}. Phone: {phone}. Loyalty tier: Gold.\",\n",
    "        \"Event registration: {name} from {company} registered for AI Summit 2024 in {city}. Contact: {email}. Dietary: vegetarian.\",\n",
    "    ],\n",
    "    'legal_document': [\n",
    "        \"Non-disclosure agreement between {name} and {company}. Effective date: January 2024. Jurisdiction: {city}. Contact: {email}.\",\n",
    "        \"Data processing agreement: {company} processes personal data of EU residents per GDPR Art. 28. DPO: {name}, {email}, {phone}.\",\n",
    "        \"Contract #CTR-{ctr_id} between {name} and {company}. Value: ${amount:,.2f}. Signed in {city}. Witness: {witness}.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "documents = []\n",
    "for i in range(200):\n",
    "    doc_type = np.random.choice(list(templates.keys()))\n",
    "    template = np.random.choice(templates[doc_type])\n",
    "    first = np.random.choice(first_names)\n",
    "    last = np.random.choice(last_names)\n",
    "    name = f\"{first} {last}\"\n",
    "    \n",
    "    doc_text = template.format(\n",
    "        name=name,\n",
    "        ssn=random_ssn(),\n",
    "        cc=random_cc(),\n",
    "        cc_last4=f\"{np.random.randint(1000,9999)}\",\n",
    "        email=random_email(first, last),\n",
    "        phone=random_phone(),\n",
    "        city=np.random.choice(cities),\n",
    "        company=np.random.choice(companies),\n",
    "        salary=np.random.randint(50000, 200000),\n",
    "        amount=np.random.uniform(100, 500000),\n",
    "        emp_id=np.random.randint(10000, 99999),\n",
    "        inv_id=np.random.randint(10000, 99999),\n",
    "        txn_id=np.random.randint(100000, 999999),\n",
    "        acct_last4=f\"{np.random.randint(1000,9999)}\",\n",
    "        med_id=np.random.randint(100000, 999999),\n",
    "        dob=f\"{np.random.randint(1,12):02d}/{np.random.randint(1,28):02d}/{np.random.randint(1950,2000)}\",\n",
    "        doctor=f\"Dr. {np.random.choice(last_names)}\",\n",
    "        impressions=np.random.randint(10000, 1000000),\n",
    "        clicks=np.random.randint(100, 50000),\n",
    "        conversions=np.random.randint(10, 1000),\n",
    "        purchases=np.random.randint(1, 50),\n",
    "        ctr_id=np.random.randint(10000, 99999),\n",
    "        witness=f\"{np.random.choice(first_names)} {np.random.choice(last_names)}\",\n",
    "    )\n",
    "    \n",
    "    documents.append({\n",
    "        'doc_id': f'DOC-{i+1:04d}',\n",
    "        'doc_type': doc_type,\n",
    "        'text': doc_text,\n",
    "        'department': doc_type.replace('_', ' ').title().split()[0],\n",
    "    })\n",
    "\n",
    "docs_df = pd.DataFrame(documents)\n",
    "print(f\"Generated {len(docs_df)} documents\")\n",
    "print(f\"\\nDocument type distribution:\")\n",
    "print(docs_df['doc_type'].value_counts())\n",
    "print(f\"\\nSample document:\")\n",
    "print(docs_df.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Regex PII Scanning\n",
    "\n",
    "Build a regex-based scanner to detect SSNs, credit card numbers, emails, and phone numbers in each document.\n",
    "\n",
    "**Your Task:** Implement the scanner and compute precision/recall against the known document types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_pii_regex(text):\n",
    "    \"\"\"Scan text for PII using regex patterns.\n",
    "    \n",
    "    Detect: SSN, credit card, email, phone\n",
    "    \n",
    "    Returns: dict of {pii_type: [matches]}\n",
    "    \"\"\"\n",
    "    # TODO: Define regex patterns for SSN, credit_card, email, phone\n",
    "    # TODO: Apply each pattern to the text\n",
    "    # TODO: Return dict of findings\n",
    "    pass\n",
    "\n",
    "# TODO: Apply scan_pii_regex to all documents\n",
    "# TODO: Add columns for each PII type count\n",
    "# TODO: Print summary statistics\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2: NER with spaCy\n",
    "\n",
    "Use spaCy's Named Entity Recognition to extract PERSON, ORG, and GPE entities, then combine with regex findings for hybrid detection.\n",
    "\n",
    "**Your Task:** Extract NER entities and merge with regex results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ner_entities(text):\n",
    "    \"\"\"Extract named entities using spaCy.\n",
    "    \n",
    "    Extract: PERSON, ORG, GPE entities\n",
    "    \n",
    "    Returns: dict of {entity_type: [entities]}\n",
    "    \"\"\"\n",
    "    # TODO: Process text with spaCy nlp()\n",
    "    # TODO: Extract PERSON, ORG, GPE entities\n",
    "    # TODO: Return dict of findings\n",
    "    pass\n",
    "\n",
    "# TODO: Apply to all documents and add NER columns\n",
    "# TODO: Create a combined 'total_pii_types' column (regex + NER unique types)\n",
    "# TODO: Print a sample showing both regex and NER findings\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Risk Scoring\n",
    "\n",
    "Compute a 0-100 risk score for each document based on the types and volume of PII found, and the applicable regulations.\n",
    "\n",
    "**Your Task:** Implement a risk scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_risk_score(row):\n",
    "    \"\"\"Compute a 0-100 risk score for a document.\n",
    "    \n",
    "    Scoring factors:\n",
    "    - SSN found: +30 points\n",
    "    - Credit card found: +25 points\n",
    "    - Email found: +10 points\n",
    "    - Phone found: +10 points\n",
    "    - PERSON entities found: +5 per entity (max 15)\n",
    "    - Medical document type: +15 points\n",
    "    - Financial document type: +10 points\n",
    "    \n",
    "    Cap at 100.\n",
    "    \n",
    "    Returns: integer risk score 0-100\n",
    "    \"\"\"\n",
    "    # TODO: Compute score based on PII findings\n",
    "    # TODO: Add document type bonus\n",
    "    # TODO: Cap at 100 and return\n",
    "    pass\n",
    "\n",
    "# TODO: Apply risk scoring to all documents\n",
    "# TODO: Assign risk tiers: Critical (76-100), High (51-75), Medium (26-50), Low (0-25)\n",
    "# TODO: Print distribution of risk tiers\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Compliance Dashboard\n",
    "\n",
    "Create a 2x2 matplotlib dashboard showing PII distribution, risk tiers, document types vs risk, and regulation applicability.\n",
    "\n",
    "**Your Task:** Build the compliance dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_compliance_dashboard(docs_df):\n",
    "    \"\"\"Build a 2x2 compliance dashboard.\n",
    "    \n",
    "    Plots:\n",
    "    1. Top-left: PII type distribution (bar chart)\n",
    "    2. Top-right: Risk tier distribution (pie chart)\n",
    "    3. Bottom-left: Average risk score by document type (horizontal bar)\n",
    "    4. Bottom-right: Applicable regulations count (bar chart)\n",
    "       - GDPR: docs with PERSON entities\n",
    "       - HIPAA: medical_form docs\n",
    "       - PCI-DSS: docs with credit card findings\n",
    "       - CCPA: docs with email + phone\n",
    "    \"\"\"\n",
    "    # TODO: Create 2x2 subplot figure (16, 12)\n",
    "    # TODO: Implement each of the 4 visualisations\n",
    "    pass\n",
    "\n",
    "build_compliance_dashboard(docs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3: Catalogue Integration\n",
    "\n",
    "Build a vector store that includes risk metadata, then perform filtered queries to find high-risk documents matching specific search criteria.\n",
    "\n",
    "**Your Task:** Create a ChromaDB collection with risk metadata and run filtered semantic queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_risk_catalogue(docs_df):\n",
    "    \"\"\"Build a vector catalogue with risk metadata.\n",
    "    \n",
    "    Steps:\n",
    "    1. Load SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    2. Encode document texts\n",
    "    3. Create ChromaDB collection 'risk_catalogue'\n",
    "    4. Add with metadata: doc_type, risk_score, risk_tier\n",
    "    \n",
    "    Returns: (collection, model)\n",
    "    \"\"\"\n",
    "    # TODO: Load model and encode texts\n",
    "    # TODO: Create ChromaDB collection\n",
    "    # TODO: Add embeddings with risk metadata\n",
    "    # TODO: Return (collection, model)\n",
    "    pass\n",
    "\n",
    "catalogue_result = build_risk_catalogue(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_risk_search(collection, query, risk_tier=None, n_results=5):\n",
    "    \"\"\"Search the risk catalogue with optional risk tier filter.\n",
    "    \n",
    "    If risk_tier is provided, filter results to that tier.\n",
    "    Print query, results with doc_type, risk_score, and text preview.\n",
    "    \"\"\"\n",
    "    # TODO: Build where clause if risk_tier is provided\n",
    "    # TODO: Query collection\n",
    "    # TODO: Print formatted results\n",
    "    pass\n",
    "\n",
    "# Test queries\n",
    "if catalogue_result:\n",
    "    collection, model = catalogue_result\n",
    "    filtered_risk_search(collection, \"employee personal data\", risk_tier=\"Critical\")\n",
    "    filtered_risk_search(collection, \"financial transactions and payments\")\n",
    "    filtered_risk_search(collection, \"medical patient records\", risk_tier=\"Critical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Scan** documents for PII using regex patterns with precision/recall awareness\n",
    "2. **Extract** named entities with spaCy NER for hybrid PII detection\n",
    "3. **Score** data assets for compliance risk on a 0-100 scale\n",
    "4. **Visualise** compliance posture with a multi-chart dashboard\n",
    "5. **Integrate** risk metadata into a vector catalogue for filtered semantic search\n",
    "\n",
    "---\n",
    "\n",
    "*Data Discovery: Harnessing AI, AGI & Vector Databases | AI Elevate*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}