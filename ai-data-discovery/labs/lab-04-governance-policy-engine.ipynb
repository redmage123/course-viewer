{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 4: Data Governance & Automated Policy Engine\n\n**Data Discovery: Harnessing AI, AGI & Vector Databases - Day 2**\n\n| Duration | Framework | Sections |\n|---|---|---|\n| 90 min | pandas, numpy, matplotlib, networkx | 6 |\n\nIn this lab, you'll explore:\n- Defining governance policies as executable rule functions\n- Scanning data assets against policies to detect violations\n- Modelling data lineage as a directed graph\n- Simulating role-based access control (RBAC)\n- Building a compliance dashboard\n- Generating automated compliance reports\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "f1w7u1s3oi9",
   "source": "## Student Notes & Background\n\n### Why Data Governance Matters\n\nAs organisations accumulate thousands of data assets across departments, the question shifts from *\"Can we find this data?\"* to *\"Should anyone be accessing this data, and is it compliant with our policies?\"* **Data governance** is the framework of policies, processes, and controls that ensures data is managed responsibly throughout its lifecycle.\n\nWithout automated governance, organisations face:\n- **Compliance violations** — PII stored without encryption, breaching GDPR, HIPAA, or CCPA\n- **Security risks** — unauthorised users accessing sensitive data\n- **Data decay** — stale, orphaned, or redundant data consuming resources\n- **Audit failures** — inability to demonstrate compliance to regulators\n\n### Key Concepts\n\n#### 1. Policy-as-Code\nTraditional governance relies on written policies that humans interpret and enforce manually. **Policy-as-code** encodes governance rules as executable functions that can be run automatically against every data asset in your catalogue. Each policy check takes an asset's metadata and returns a verdict: *compliant* or *violation* with a detail message.\n\nThe benefits are significant:\n- **Consistency** — every asset is evaluated by the same rules\n- **Speed** — hundreds of assets scanned in seconds rather than weeks of manual audit\n- **Auditability** — every check produces a traceable record\n- **Composability** — new policies can be added without modifying existing ones\n\n#### 2. Policy Severity Levels\nNot all violations are equal. A standard severity classification:\n\n| Severity | Description | Response Time |\n|---|---|---|\n| **Critical** | Immediate risk of data breach or regulatory penalty | Fix within 24 hours |\n| **High** | Significant compliance gap requiring prompt action | Fix within 1 week |\n| **Medium** | Best-practice deviation that should be addressed | Fix within 1 month |\n| **Low** | Minor improvement opportunity | Address in next review cycle |\n\n#### 3. Data Lineage\n**Data lineage** tracks how data flows through an organisation — from source systems (databases, APIs, files), through transformations (ETL jobs, ML pipelines, aggregations), to outputs (dashboards, reports, exports). Lineage is modelled as a **directed acyclic graph (DAG)** where:\n- **Nodes** represent data assets or processing steps\n- **Edges** represent data flow from upstream to downstream\n\nLineage analysis reveals:\n- **Bottleneck nodes** — transforms that many pipelines depend on (high failure impact)\n- **Long dependency chains** — complex pipelines that are harder to debug and audit\n- **Blast radius** — if a source system fails, which downstream outputs are affected?\n\n#### 4. Role-Based Access Control (RBAC)\n**RBAC** restricts data access based on a user's role rather than their individual identity. A typical enterprise model:\n\n| Role | Access Level | Typical Users |\n|---|---|---|\n| **Admin** | All sensitivity levels | Data platform team, CTO |\n| **Analyst** | Up to Confidential | Data analysts, business intelligence |\n| **Engineer** | Up to Internal | Software engineers, DevOps |\n| **Viewer** | Public only | General staff, external partners |\n\nA **violation** occurs when a user in a lower-privilege role accesses data above their clearance level. Monitoring access patterns helps detect both policy gaps and potential security incidents.\n\n#### 5. Compliance Dashboards\nA **compliance dashboard** provides at-a-glance visibility into your governance posture. Effective dashboards show:\n- Overall compliance score (percentage of clean assets)\n- Violations broken down by severity, category, and policy\n- Trends over time (is compliance improving or degrading?)\n- Sensitivity distribution of violating assets\n\n#### 6. Compliance Reports\nAutomated **compliance reports** translate raw violation data into actionable documents for different audiences:\n- **Executive summary** for leadership (score, headline numbers, rating)\n- **Critical findings** for the security team (specific assets and violations)\n- **Department breakdown** for data stewards (their team's compliance posture)\n- **Recommendations** for the governance committee (prioritised remediation actions)\n\n### What You'll Build\n\nIn this lab, you will:\n1. **Define** 6 governance policy check functions covering PII encryption, retention periods, stale data, owner assignment, access control, and backup compliance\n2. **Build** an automated scanner that runs all policies against 300 synthetic data assets and collects violations into a structured DataFrame\n3. **Model** data lineage as a NetworkX directed graph with source, transform, and output nodes, then analyse it for bottlenecks and long dependency chains\n4. **Simulate** 1,000 RBAC access requests across 4 roles and visualise violation rates as a heatmap\n5. **Create** a 6-panel governance dashboard with matplotlib showing severity breakdowns, compliance rates, and an overall score\n6. **Generate** a formatted compliance report with executive summary, critical findings, department breakdown, and actionable recommendations\n\n### Prerequisites\n- Familiarity with pandas DataFrames, numpy, and matplotlib\n- Understanding of set operations (for access control checks)\n- Concepts from Labs 1-2: data asset metadata, sensitivity levels, PII detection\n\n### Tips\n- Each policy function follows the same signature: `def check_policy(row) -> (bool, str)` — this makes them composable and testable\n- The synthetic data includes deliberate governance gaps (unencrypted PII, missing owners, stale assets) so you will find violations\n- For the lineage graph, `networkx` provides powerful analysis functions like `nx.dag_longest_path()`, `nx.descendants()`, and `nx.betweenness_centrality()`\n- When building the dashboard, use `plt.tight_layout()` to prevent label overlap in the 2×3 grid\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Synthetic Data Assets\n",
    "\n",
    "We'll create ~300 data assets with governance-relevant metadata including PII flags, encryption status, retention periods, and access patterns."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "categories = ['HR', 'Finance', 'Marketing', 'Engineering', 'Legal']\n",
    "owners = ['alice', 'bob', 'carol', 'dave', 'eve', None]\n",
    "sensitivity_levels = ['Public', 'Internal', 'Confidential', 'Restricted']\n",
    "\n",
    "n_assets = 300\n",
    "assets = []\n",
    "\n",
    "all_users = ['alice', 'bob', 'carol', 'dave', 'eve', 'frank', 'grace', 'henry', 'iris', 'jack']\n",
    "\n",
    "for i in range(n_assets):\n",
    "    cat = np.random.choice(categories)\n",
    "    sens = np.random.choice(sensitivity_levels, p=[0.15, 0.30, 0.30, 0.25])\n",
    "    has_pii = cat in ['HR', 'Legal'] or (cat == 'Finance' and np.random.random() < 0.5)\n",
    "    has_encryption = (sens in ['Confidential', 'Restricted'] and np.random.random() < 0.7) or np.random.random() < 0.2\n",
    "    \n",
    "    # Determine approved users based on sensitivity\n",
    "    if sens == 'Restricted':\n",
    "        n_approved = np.random.randint(1, 4)\n",
    "    elif sens == 'Confidential':\n",
    "        n_approved = np.random.randint(2, 6)\n",
    "    else:\n",
    "        n_approved = np.random.randint(3, 10)\n",
    "    approved = list(np.random.choice(all_users, size=n_approved, replace=False))\n",
    "    \n",
    "    # Actual users (sometimes includes unauthorized)\n",
    "    n_actual = np.random.randint(1, min(n_approved + 3, len(all_users)))\n",
    "    actual = list(np.random.choice(all_users, size=n_actual, replace=False))\n",
    "    \n",
    "    assets.append({\n",
    "        'asset_id': f'ASSET-{i+1:04d}',\n",
    "        'name': f'{cat.lower()}_asset_{i+1:04d}',\n",
    "        'category': cat,\n",
    "        'owner': np.random.choice(owners, p=[0.2, 0.2, 0.2, 0.2, 0.15, 0.05]),\n",
    "        'sensitivity': sens,\n",
    "        'has_pii': has_pii,\n",
    "        'has_encryption': has_encryption,\n",
    "        'last_access_days_ago': np.random.randint(0, 400),\n",
    "        'retention_days': np.random.choice([90, 180, 365, 730, 1095]),\n",
    "        'days_since_creation': np.random.randint(30, 1200),\n",
    "        'access_count_30d': np.random.randint(0, 200),\n",
    "        'has_backup': np.random.random() < 0.6,\n",
    "        'approved_users': approved,\n",
    "        'actual_users': actual,\n",
    "    })\n",
    "\n",
    "assets_df = pd.DataFrame(assets)\n",
    "print(f\"Generated {len(assets_df)} data assets\")\n",
    "print(f\"\\nSensitivity distribution:\")\n",
    "print(assets_df['sensitivity'].value_counts())\n",
    "print(f\"\\nPII assets: {assets_df['has_pii'].sum()} ({assets_df['has_pii'].mean()*100:.1f}%)\")\n",
    "print(f\"Encrypted:  {assets_df['has_encryption'].sum()} ({assets_df['has_encryption'].mean()*100:.1f}%)\")\n",
    "assets_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1.1: Governance Policies\n\nThe code below defines 6 governance policy check functions as executable rules. Each function takes an asset's metadata and returns whether it violates the policy. This \"policy-as-code\" approach ensures every asset is evaluated consistently and automatically."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def check_pii_encryption(asset):\n    \"\"\"POL-001: PII data must be encrypted.\"\"\"\n    if asset['has_pii'] and not asset['has_encryption']:\n        return True, f\"Asset contains PII but is NOT encrypted (sensitivity={asset['sensitivity']})\"\n    return False, \"\"\n\n\ndef check_retention_compliance(asset):\n    \"\"\"POL-002: Assets older than their retention period must be flagged.\"\"\"\n    if asset['days_since_creation'] > asset['retention_days']:\n        over = asset['days_since_creation'] - asset['retention_days']\n        return True, f\"Asset is {over} days past its {asset['retention_days']}-day retention period\"\n    return False, \"\"\n\n\ndef check_stale_data(asset):\n    \"\"\"POL-003: Assets not accessed in 180+ days should be reviewed.\"\"\"\n    if asset['last_access_days_ago'] >= 180:\n        return True, f\"Asset last accessed {asset['last_access_days_ago']} days ago (stale threshold: 180)\"\n    return False, \"\"\n\n\ndef check_owner_assigned(asset):\n    \"\"\"POL-004: Every asset must have an assigned owner.\"\"\"\n    if asset['owner'] is None or (isinstance(asset['owner'], float) and np.isnan(asset['owner'])):\n        return True, \"Asset has no owner assigned\"\n    return False, \"\"\n\n\ndef check_access_control(asset):\n    \"\"\"POL-005: Only approved users should access an asset.\"\"\"\n    approved = set(asset['approved_users'])\n    actual = set(asset['actual_users'])\n    unauthorized = actual - approved\n    if unauthorized:\n        return True, f\"{len(unauthorized)} unauthorized user(s): {', '.join(sorted(unauthorized))}\"\n    return False, \"\"\n\n\ndef check_sensitivity_review(asset):\n    \"\"\"POL-006: Restricted assets accessed by >5 users need review.\"\"\"\n    if asset['sensitivity'] == 'Restricted' and len(asset['actual_users']) > 5:\n        return True, (f\"Restricted asset accessed by {len(asset['actual_users'])} users \"\n                      f\"(max recommended: 5)\")\n    return False, \"\"\n\n\ndef check_classification(asset):\n    \"\"\"POL-007: All assets must have a sensitivity classification.\"\"\"\n    if not asset['sensitivity'] or asset['sensitivity'] == '':\n        return True, \"Asset has no sensitivity classification\"\n    return False, \"\"\n\n\ndef check_backup_compliance(asset):\n    \"\"\"POL-008: Confidential and Restricted assets must have backups.\"\"\"\n    if asset['sensitivity'] in ('Confidential', 'Restricted') and not asset['has_backup']:\n        return True, f\"No backup for {asset['sensitivity']} asset\"\n    return False, \"\"\n\n\nPOLICY_FUNCTIONS = {\n    'check_pii_encryption':      check_pii_encryption,\n    'check_retention_compliance': check_retention_compliance,\n    'check_stale_data':          check_stale_data,\n    'check_owner_assigned':      check_owner_assigned,\n    'check_access_control':      check_access_control,\n    'check_sensitivity_review':  check_sensitivity_review,\n    'check_classification':      check_classification,\n    'check_backup_compliance':   check_backup_compliance,\n}\n\nprint(f\"Defined {len(POLICY_FUNCTIONS)} policy check functions:\")\nfor name, fn in POLICY_FUNCTIONS.items():\n    print(f\"  - {name:35s}  {fn.__doc__}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3volsmxvulq",
   "source": "### Analysis Questions\n\n1. How would you prioritize remediation across Critical, High, and Medium violations?\n2. Which policy would be hardest to implement in a real enterprise? Why?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1.2: Automated Policy Scanning\n\nThe code below runs every policy against every data asset, collecting all violations into a structured DataFrame. This automated scanning can process hundreds of assets in seconds — compared to weeks of manual audit."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Build the policies metadata table (links policy IDs to check functions)\npolicies_df = pd.DataFrame([\n    {'policy_id': 'POL-001', 'name': 'PII Encryption', 'description': 'Assets containing PII must be encrypted', 'check_function_name': 'check_pii_encryption', 'severity': 'Critical'},\n    {'policy_id': 'POL-002', 'name': 'Retention Compliance', 'description': 'Assets past retention must be flagged', 'check_function_name': 'check_retention_compliance', 'severity': 'High'},\n    {'policy_id': 'POL-003', 'name': 'Stale Data', 'description': 'Assets not accessed in 180+ days need review', 'check_function_name': 'check_stale_data', 'severity': 'Medium'},\n    {'policy_id': 'POL-004', 'name': 'Owner Assigned', 'description': 'Every asset must have an owner', 'check_function_name': 'check_owner_assigned', 'severity': 'High'},\n    {'policy_id': 'POL-005', 'name': 'Access Control', 'description': 'Only approved users should access assets', 'check_function_name': 'check_access_control', 'severity': 'Critical'},\n    {'policy_id': 'POL-006', 'name': 'Sensitivity Review', 'description': 'Restricted assets with >5 users need review', 'check_function_name': 'check_sensitivity_review', 'severity': 'High'},\n    {'policy_id': 'POL-007', 'name': 'Classification Required', 'description': 'All assets must have sensitivity set', 'check_function_name': 'check_classification', 'severity': 'Medium'},\n    {'policy_id': 'POL-008', 'name': 'Backup Compliance', 'description': 'Confidential+ assets need backups', 'check_function_name': 'check_backup_compliance', 'severity': 'High'},\n])\n\ndef run_policy_scan(assets_df, policies_df, policy_functions):\n    \"\"\"Scan all assets against all policies and collect violations.\"\"\"\n    violations = []\n    for _, policy in policies_df.iterrows():\n        fn = policy_functions[policy['check_function_name']]\n        for _, asset in assets_df.iterrows():\n            violated, detail = fn(asset)\n            if violated:\n                violations.append({\n                    'asset_id': asset['asset_id'], 'asset_name': asset['name'],\n                    'category': asset['category'], 'sensitivity': asset['sensitivity'],\n                    'policy_id': policy['policy_id'], 'policy_name': policy['name'],\n                    'severity': policy['severity'], 'detail': detail,\n                })\n    return pd.DataFrame(violations)\n\nviolations_df = run_policy_scan(assets_df, policies_df, POLICY_FUNCTIONS)\n\nprint(f\"Total violations found: {len(violations_df)}\")\nprint(f\"Assets with at least one violation: {violations_df['asset_id'].nunique()} / {len(assets_df)}\")\nprint(f\"\\nViolations by severity:\")\nprint(violations_df['severity'].value_counts())\nprint(f\"\\nViolations by policy:\")\nprint(violations_df['policy_name'].value_counts())\nprint(f\"\\nViolations by category:\")\nprint(violations_df['category'].value_counts())\nprint(f\"\\nSample violations:\")\nviolations_df.head(10)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "xklya2i5pd",
   "source": "### Analysis Questions\n\n1. What percentage of assets have at least one violation? Is this realistic for a real organisation?\n2. Which department has the most violations? What structural factors cause this?\n3. Are certain policies violated together (co-occurrence)? What does this suggest?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2.1: Data Lineage Tracking\n\nThe code below builds a directed graph representing data lineage — how data flows from source systems (databases, APIs) through transformations (ETL jobs, ML pipelines) to outputs (dashboards, reports). The graph is analysed for bottlenecks and long dependency chains."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "np.random.seed(123)\nG = nx.DiGraph()\n\nsource_nodes = ['CRM_Database', 'ERP_System', 'HR_Portal', 'Web_Analytics',\n    'IoT_Sensors', 'Email_Server', 'Payment_Gateway', 'Social_API',\n    'Survey_Platform', 'Legacy_Mainframe', 'Cloud_Storage', 'Partner_Feed']\ntransform_nodes = ['ETL_Pipeline_A', 'ETL_Pipeline_B', 'Spark_Job_Clean',\n    'Spark_Job_Enrich', 'Python_Transform', 'dbt_Model_Stg',\n    'dbt_Model_Int', 'dbt_Model_Mart', 'ML_Feature_Eng',\n    'Anonymiser', 'Data_Quality_Check', 'Aggregator',\n    'Dedup_Service', 'Schema_Validator', 'Encryption_Layer']\noutput_nodes = ['DW_Finance_Mart', 'DW_HR_Mart', 'DW_Marketing_Mart',\n    'DW_Engineering_Mart', 'DW_Legal_Mart', 'BI_Dashboard',\n    'ML_Training_Set', 'Customer_360', 'Compliance_Report',\n    'Executive_Summary', 'Data_Catalogue', 'Archive_S3']\n\nfor s in source_nodes: G.add_node(s, node_type='source')\nfor t in transform_nodes: G.add_node(t, node_type='transform')\nfor o in output_nodes: G.add_node(o, node_type='output')\n\nfor _ in range(50):\n    src = np.random.choice(source_nodes)\n    chain = list(np.random.choice(transform_nodes, size=np.random.randint(1,4), replace=False))\n    out = np.random.choice(output_nodes)\n    G.add_edge(src, chain[0])\n    for j in range(len(chain)-1): G.add_edge(chain[j], chain[j+1])\n    G.add_edge(chain[-1], out)\n\nprint(f\"Lineage graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n\ncolor_map = {'source': '#3b82f6', 'transform': '#f59e0b', 'output': '#10b981'}\nnode_colors = [color_map[G.nodes[n].get('node_type','transform')] for n in G.nodes]\n\nfig, ax = plt.subplots(figsize=(18, 12))\npos = nx.spring_layout(G, seed=42, k=1.8)\nnx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=600, alpha=0.9, ax=ax)\nnx.draw_networkx_labels(G, pos, font_size=6, font_weight='bold', ax=ax)\nnx.draw_networkx_edges(G, pos, edge_color='#94a3b8', arrows=True, arrowsize=15, width=1.2, alpha=0.7, ax=ax)\n\nfrom matplotlib.lines import Line2D\nlegend_elements = [\n    Line2D([0],[0], marker='o', color='w', markerfacecolor='#3b82f6', markersize=12, label='Source'),\n    Line2D([0],[0], marker='o', color='w', markerfacecolor='#f59e0b', markersize=12, label='Transform'),\n    Line2D([0],[0], marker='o', color='w', markerfacecolor='#10b981', markersize=12, label='Output')]\nax.legend(handles=legend_elements, loc='upper left', fontsize=11)\nax.set_title('Data Lineage Graph', fontsize=16, fontweight='bold')\nplt.tight_layout(); plt.show()\n\nprint(\"\\n--- Lineage Analysis ---\")\ntry:\n    longest_path = nx.dag_longest_path(G)\n    print(f\"Longest lineage path ({len(longest_path)} nodes):\")\n    print(f\"  {' -> '.join(longest_path)}\")\nexcept nx.NetworkXUnfeasible:\n    print(\"Graph contains cycles. Finding longest simple path...\")\n    max_path = []\n    for s in source_nodes:\n        for o in output_nodes:\n            try:\n                for p in nx.all_simple_paths(G, s, o):\n                    if len(p) > len(max_path): max_path = p\n            except nx.NetworkXError: pass\n    if max_path: print(f\"Longest path ({len(max_path)} nodes): {' -> '.join(max_path)}\")\n\ndegree_sorted = sorted(G.degree(), key=lambda x: x[1], reverse=True)[:10]\nprint(f\"\\nTop 10 most-connected nodes:\")\nfor node, deg in degree_sorted:\n    print(f\"  {node:30s}  type={G.nodes[node].get('node_type','?'):10s}  degree={deg}\")\n\nbetweenness = nx.betweenness_centrality(G)\ntop_b = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)[:5]\nprint(f\"\\nTop 5 bottleneck nodes (betweenness centrality):\")\nfor node, bc in top_b:\n    print(f\"  {node:30s}  type={G.nodes[node].get('node_type','?'):10s}  centrality={bc:.4f}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Lineage analysis was performed in the cell above",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ixah6po4hso",
   "source": "### Analysis Questions\n\n1. Which transform nodes are bottlenecks (highest betweenness centrality)? What's the risk if they fail?\n2. What's the longest dependency chain? How does this affect debugging and auditing?\n3. If a source database goes offline, how many downstream outputs are affected?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2.2: RBAC Access Control\n\nThe code below simulates a role-based access control system with four roles (admin, analyst, engineer, viewer) and 2,000 access requests. Each role has a maximum sensitivity level it can access. The violation rates are visualised as a heatmap showing which role/sensitivity combinations cause denials."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "np.random.seed(99)\nroles = {\n    'admin':    {'allowed': ['Public', 'Internal', 'Confidential', 'Restricted']},\n    'analyst':  {'allowed': ['Public', 'Internal', 'Confidential']},\n    'engineer': {'allowed': ['Public', 'Internal']},\n    'viewer':   {'allowed': ['Public']},\n}\nuser_roles = {}\nfor user in all_users:\n    user_roles[user] = np.random.choice(list(roles.keys()), p=[0.10, 0.30, 0.35, 0.25])\n\nprint(\"User role assignments:\")\nfor user, role in sorted(user_roles.items()):\n    print(f\"  {user:10s} -> {role}\")\n\naccess_log = []\nfor _ in range(2000):\n    user = np.random.choice(all_users)\n    asset = assets_df.iloc[np.random.randint(0, len(assets_df))]\n    role = user_roles[user]\n    granted = asset['sensitivity'] in roles[role]['allowed']\n    access_log.append({'user': user, 'role': role, 'asset_id': asset['asset_id'],\n        'sensitivity': asset['sensitivity'], 'category': asset['category'], 'granted': granted})\n\naccess_df = pd.DataFrame(access_log)\nprint(f\"\\nSimulated {len(access_df)} access requests\")\nprint(f\"Granted: {access_df['granted'].sum()}  |  Denied: {(~access_df['granted']).sum()}\")\nprint(f\"Overall denial rate: {(~access_df['granted']).mean():.1%}\")\n\ndenial_matrix = access_df.groupby(['role','sensitivity'])['granted'].apply(\n    lambda x: (~x).mean()).unstack(fill_value=0)\nrole_order = ['admin', 'analyst', 'engineer', 'viewer']\nsens_order = ['Public', 'Internal', 'Confidential', 'Restricted']\ndenial_matrix = denial_matrix.reindex(index=role_order, columns=sens_order, fill_value=0)\nprint(f\"\\nDenial rate matrix:\")\nprint(denial_matrix.round(3))\n\nfig, ax = plt.subplots(figsize=(10, 6))\nim = ax.imshow(denial_matrix.values, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\nax.set_xticks(range(4)); ax.set_xticklabels(sens_order, fontsize=12)\nax.set_yticks(range(4)); ax.set_yticklabels(role_order, fontsize=12)\nfor i in range(4):\n    for j in range(4):\n        val = denial_matrix.values[i, j]\n        ax.text(j, i, f'{val:.0%}', ha='center', va='center',\n                fontsize=14, fontweight='bold', color='white' if val > 0.5 else 'black')\nplt.colorbar(im, ax=ax, label='Denial Rate')\nax.set_title('Access Denial Rate: Role vs Sensitivity Level', fontsize=14, fontweight='bold')\nax.set_xlabel('Asset Sensitivity', fontsize=12); ax.set_ylabel('User Role', fontsize=12)\nplt.tight_layout(); plt.show()\n\nprint(\"\\nPer-role access summary:\")\nprint(access_df.groupby('role').agg(\n    total=('granted','count'), granted=('granted','sum'),\n    denied=('granted', lambda x: (~x).sum()),\n    denial_rate=('granted', lambda x: f\"{(~x).mean():.1%}\")))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bwhil2e50ft",
   "source": "### Analysis Questions\n\n1. Which role has the highest access denial rate? Is the access model too restrictive or too permissive?\n2. Look at the heatmap — which role/sensitivity combination causes the most violations?\n3. How would you redesign the role hierarchy to reduce violations while maintaining security?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 3.1: Governance Dashboard\n\nThe code below creates a 6-panel governance dashboard showing: violations by severity, violations by category, compliance trend, top violated policies, sensitivity distribution of violating assets, and an overall compliance score gauge."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "violating_ids = violations_df['asset_id'].unique()\nn_compliant = len(assets_df) - len(violating_ids)\ncompliance_pct = n_compliant / len(assets_df) * 100\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\n\n# 1. Violations by severity\nax = axes[0, 0]\nsev_order = ['Critical', 'High', 'Medium', 'Low']\nsev_colors = {'Critical': '#ef4444', 'High': '#f59e0b', 'Medium': '#3b82f6', 'Low': '#10b981'}\nsev_counts = violations_df['severity'].value_counts().reindex(sev_order, fill_value=0)\nbars = ax.bar(sev_counts.index, sev_counts.values, color=[sev_colors[s] for s in sev_counts.index])\nfor bar, val in zip(bars, sev_counts.values):\n    ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+1, str(val), ha='center', fontweight='bold')\nax.set_title('Violations by Severity', fontsize=12, fontweight='bold'); ax.set_ylabel('Count')\n\n# 2. Violations by category (stacked)\nax = axes[0, 1]\ncat_sev = violations_df.groupby(['category','severity']).size().unstack(fill_value=0).reindex(columns=sev_order, fill_value=0)\ncat_sev.plot(kind='bar', stacked=True, ax=ax, color=[sev_colors[s] for s in sev_order])\nax.set_title('Violations by Category & Severity', fontsize=12, fontweight='bold')\nax.set_ylabel('Count'); ax.legend(title='Severity', fontsize=8); ax.tick_params(axis='x', rotation=45)\n\n# 3. Compliance trend\nax = axes[0, 2]\nmonths = pd.date_range('2024-01-01', periods=12, freq='MS')\nnp.random.seed(55)\ntrend = [min(60 + i*2.5 + np.random.normal(0, 1.5), 98) for i in range(12)]\nax.plot(months, trend, 'o-', color='#3b82f6', linewidth=2, markersize=6)\nax.fill_between(months, trend, alpha=0.15, color='#3b82f6')\nax.axhline(y=85, color='#ef4444', linestyle='--', alpha=0.7, label='Target (85%)')\nax.set_title('Compliance Rate Trend (Simulated)', fontsize=12, fontweight='bold')\nax.set_ylabel('Compliance %'); ax.set_ylim(50, 100); ax.legend(fontsize=9)\nax.tick_params(axis='x', rotation=45)\n\n# 4. Top violated policies\nax = axes[1, 0]\npc = violations_df['policy_name'].value_counts().head(10)\npsev = violations_df.drop_duplicates('policy_name').set_index('policy_name')['severity']\nbc = [sev_colors.get(psev.get(p, 'Medium'), '#3b82f6') for p in pc.index]\nax.barh(pc.index[::-1], pc.values[::-1], color=bc[::-1])\nax.set_title('Top Violated Policies', fontsize=12, fontweight='bold'); ax.set_xlabel('Violations')\n\n# 5. Sensitivity pie\nax = axes[1, 1]\nsd = violations_df.drop_duplicates('asset_id')['sensitivity'].value_counts()\nscm = {'Public':'#10b981','Internal':'#3b82f6','Confidential':'#f59e0b','Restricted':'#ef4444'}\nax.pie(sd.values, labels=sd.index, colors=[scm.get(s,'#94a3b8') for s in sd.index],\n       autopct='%1.1f%%', startangle=90, textprops={'fontsize': 10})\nax.set_title('Sensitivity of Violating Assets', fontsize=12, fontweight='bold')\n\n# 6. Gauge\nax = axes[1, 2]\ntheta = np.linspace(np.pi, 0, 100)\nax.plot(np.cos(theta), np.sin(theta), color='#e2e8f0', linewidth=25, solid_capstyle='round')\nscore = compliance_pct\ntf = np.linspace(np.pi, np.pi-(np.pi*score/100), max(int(score), 2))\ngc = '#10b981' if score >= 80 else '#f59e0b' if score >= 60 else '#ef4444'\nax.plot(np.cos(tf), np.sin(tf), color=gc, linewidth=25, solid_capstyle='round')\nax.text(0, 0.15, f'{score:.1f}%', ha='center', va='center', fontsize=36, fontweight='bold', color=gc)\nax.text(0, -0.15, 'Overall Compliance', ha='center', va='center', fontsize=12, color='#64748b')\nax.text(0, -0.35, f'{n_compliant} of {len(assets_df)} assets compliant', ha='center', va='center', fontsize=10, color='#94a3b8')\nax.set_xlim(-1.3, 1.3); ax.set_ylim(-0.5, 1.3); ax.set_aspect('equal'); ax.axis('off')\nax.set_title('Compliance Score', fontsize=12, fontweight='bold')\n\nplt.suptitle('Data Governance Dashboard', fontsize=18, fontweight='bold', y=1.01)\nplt.tight_layout(); plt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "mq8gb5b4gfb",
   "source": "### Analysis Questions\n\n1. What's the overall compliance score? Would you be comfortable presenting this to a regulator?\n2. Which single improvement would have the biggest impact on the score?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 3.2: Compliance Report\n\nThe code below generates a formatted compliance report with an executive summary, critical findings, per-department breakdown, and actionable recommendations. This is the kind of document that governance teams present to leadership and regulators."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def generate_compliance_report(assets_df, violations_df, policies_df):\n    \"\"\"Generate a formatted text compliance report.\"\"\"\n    total = len(assets_df)\n    vids = violations_df['asset_id'].unique()\n    n_v = len(vids); n_c = total - n_v\n    score = n_c / total * 100\n    sev_counts = violations_df['severity'].value_counts()\n\n    L = []\n    L.append('=' * 72)\n    L.append('        DATA GOVERNANCE COMPLIANCE REPORT')\n    L.append(f'        Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n    L.append('=' * 72); L.append('')\n\n    L.append('EXECUTIVE SUMMARY'); L.append('-' * 40)\n    L.append(f'  Overall Compliance Score:  {score:.1f}%')\n    L.append(f'  Total Assets Scanned:      {total}')\n    L.append(f'  Compliant Assets:          {n_c}')\n    L.append(f'  Non-Compliant Assets:      {n_v}')\n    L.append(f'  Total Violations Found:    {len(violations_df)}')\n    L.append(f'  Policies Evaluated:        {len(policies_df)}'); L.append('')\n\n    if score >= 90: rating = 'EXCELLENT - Governance posture is strong.'\n    elif score >= 75: rating = 'GOOD - Minor remediation needed.'\n    elif score >= 60: rating = 'FAIR - Significant gaps require attention.'\n    else: rating = 'POOR - Urgent remediation required.'\n    L.append(f'  Rating: {rating}'); L.append('')\n\n    L.append('VIOLATIONS BY SEVERITY'); L.append('-' * 40)\n    for sev in ['Critical', 'High', 'Medium', 'Low']:\n        cnt = sev_counts.get(sev, 0)\n        L.append(f'  {sev:10s}  {cnt:4d}  {\"#\" * (cnt // 3)}')\n    L.append('')\n\n    L.append('CRITICAL FINDINGS'); L.append('-' * 40)\n    crit = violations_df[violations_df['severity'] == 'Critical']\n    if len(crit) > 0:\n        for i, (pol, cnt) in enumerate(crit['policy_name'].value_counts().items(), 1):\n            L.append(f'  {i}. {pol}: {cnt} violations')\n            for _, r in crit[crit['policy_name']==pol].head(3).iterrows():\n                L.append(f'     - {r[\"asset_id\"]} ({r[\"category\"]}): {r[\"detail\"]}')\n    else:\n        L.append('  No critical violations found.')\n    L.append('')\n\n    L.append('RECOMMENDATIONS'); L.append('-' * 40)\n    recs = []\n    cc = sev_counts.get('Critical', 0); hc = sev_counts.get('High', 0)\n    if cc > 0:\n        recs.append(f'[URGENT] Address {cc} critical violations immediately, '\n                    f'focusing on PII encryption and access control gaps.')\n    if hc > 0:\n        recs.append(f'[HIGH] Remediate {hc} high-severity violations within 30 days, '\n                    f'including retention compliance and owner assignment.')\n    ov = violations_df[violations_df['policy_name']=='Owner Assigned']\n    if len(ov) > 0:\n        recs.append(f'[PROCESS] Assign owners to {len(ov)} orphaned assets. '\n                    f'Implement mandatory owner field in asset registration.')\n    sv = violations_df[violations_df['policy_name']=='Stale Data']\n    if len(sv) > 0:\n        recs.append(f'[HYGIENE] Review {len(sv)} stale assets for archival or deletion. '\n                    f'Consider automated lifecycle management.')\n    recs.append('[ONGOING] Schedule quarterly governance scans and track compliance trends.')\n    for i, rec in enumerate(recs, 1):\n        L.append(f'  {i}. {rec}')\n    L.append('')\n\n    L.append('PER-DEPARTMENT BREAKDOWN'); L.append('-' * 40)\n    for cat in sorted(assets_df['category'].unique()):\n        ca = assets_df[assets_df['category']==cat]\n        cv = violations_df[violations_df['category']==cat]\n        cv_n = cv['asset_id'].nunique(); ct = len(ca)\n        cc_ = ct - cv_n; cs = cc_/ct*100 if ct > 0 else 100\n        L.append(f'  {cat}')\n        L.append(f'    Assets:     {ct}')\n        L.append(f'    Compliant:  {cc_} ({cs:.1f}%)')\n        L.append(f'    Violations: {len(cv)}')\n        csev = cv['severity'].value_counts()\n        parts = [f'{s}: {csev.get(s,0)}' for s in ['Critical','High','Medium','Low'] if csev.get(s,0)>0]\n        if parts: L.append(f'    Breakdown:  {\", \".join(parts)}')\n        L.append('')\n\n    L.append('=' * 72)\n    L.append('  End of Report')\n    L.append('  Data Discovery: Harnessing AI, AGI & Vector Databases | AI Elevate')\n    L.append('=' * 72)\n    return '\\n'.join(L)\n\nreport = generate_compliance_report(assets_df, violations_df, policies_df)\nprint(report)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cs0uf44f9e",
   "source": "### Analysis Questions\n\n1. Does the executive summary effectively communicate the compliance posture?\n2. Are the recommendations actionable? What's missing?\n3. Who should receive this report, and how often should it be generated?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Define** governance policies as executable rule functions\n",
    "2. **Scan** data assets against policies to detect compliance violations\n",
    "3. **Model** data lineage as a directed graph and identify bottlenecks\n",
    "4. **Simulate** role-based access control to detect unauthorized access\n",
    "5. **Build** a comprehensive governance compliance dashboard\n",
    "6. **Generate** automated compliance reports with findings and recommendations\n",
    "\n",
    "---\n",
    "\n",
    "*Data Discovery: Harnessing AI, AGI & Vector Databases | AI Elevate*"
   ]
  }
 ]
}