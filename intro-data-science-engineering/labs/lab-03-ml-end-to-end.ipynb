{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Lab 3: End-to-End ML Project \u2014 Customer Churn Prediction\n",
    "**Introduction to Data Science & Engineering - Day 3**\n",
    "\n",
    "| Duration | Difficulty | Framework | Exercises |\n",
    "|---|---|---|---|\n",
    "| 120 min | Intermediate | scikit-learn, matplotlib | 5 |\n",
    "\n",
    "In this lab, you'll practice:\n",
    "- Generating and exploring a churn dataset\n",
    "- Feature engineering for ML\n",
    "- Encoding and scaling features\n",
    "- Training multiple models (Logistic Regression, Random Forest, Gradient Boosting)\n",
    "- Cross-validation and evaluation\n",
    "- Confusion matrices and feature importance\n",
    "- Saving and loading models for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, classification_report, roc_auc_score, roc_curve)\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "## Part 1: Generate and Explore the Dataset\n",
    "We'll create a synthetic subscription service dataset for churn prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_customers = 2000\n",
    "\n",
    "# Base features\n",
    "tenure = np.random.randint(1, 72, n_customers)\n",
    "monthly_charges = np.round(np.random.uniform(20, 120, n_customers), 2)\n",
    "total_charges = np.round(tenure * monthly_charges * np.random.uniform(0.8, 1.1, n_customers), 2)\n",
    "\n",
    "contract = np.random.choice(['Month-to-month', 'One year', 'Two year'], n_customers, p=[0.5, 0.3, 0.2])\n",
    "payment = np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'], n_customers)\n",
    "internet = np.random.choice(['DSL', 'Fiber optic', 'No'], n_customers, p=[0.35, 0.45, 0.20])\n",
    "\n",
    "support_tickets = np.random.poisson(2, n_customers)\n",
    "num_products = np.random.randint(1, 6, n_customers)\n",
    "\n",
    "# Churn based on realistic factors\n",
    "churn_prob = np.zeros(n_customers)\n",
    "churn_prob += (contract == 'Month-to-month') * 0.25\n",
    "churn_prob += (tenure < 12) * 0.15\n",
    "churn_prob += (monthly_charges > 80) * 0.1\n",
    "churn_prob += (support_tickets > 3) * 0.15\n",
    "churn_prob += (internet == 'Fiber optic') * 0.05\n",
    "churn_prob -= (contract == 'Two year') * 0.2\n",
    "churn_prob -= (num_products > 3) * 0.1\n",
    "churn_prob = np.clip(churn_prob, 0.05, 0.85)\n",
    "\n",
    "churn = np.random.binomial(1, churn_prob)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'tenure_months': tenure,\n",
    "    'monthly_charges': monthly_charges,\n",
    "    'total_charges': total_charges,\n",
    "    'contract_type': contract,\n",
    "    'payment_method': payment,\n",
    "    'internet_service': internet,\n",
    "    'support_tickets': support_tickets,\n",
    "    'num_products': num_products,\n",
    "    'age': np.random.randint(18, 75, n_customers),\n",
    "    'has_partner': np.random.choice([0, 1], n_customers, p=[0.45, 0.55]),\n",
    "    'has_dependents': np.random.choice([0, 1], n_customers, p=[0.6, 0.4]),\n",
    "    'churn': churn\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nChurn distribution:\")\n",
    "print(df['churn'].value_counts(normalize=True).round(3))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Explore the Data\n",
    "\n",
    "**Your Task:** Print the dataset shape and churn rate, display descriptive statistics, then create a 2x3 subplot visualizing churn patterns across contract type, tenure, monthly charges, support tickets, internet service, and number of products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print dataset shape and churn rate\n",
    "# TODO: Display descriptive statistics using df.describe()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_churn_analysis(df):\n",
    "    \"\"\"Visualize churn by key features in a 2x3 subplot.\n",
    "    \n",
    "    Plots:\n",
    "    1. Churn rate by contract type (bar)\n",
    "    2. Tenure distribution by churn (stacked histogram)\n",
    "    3. Monthly charges by churn (boxplot)\n",
    "    4. Support tickets by churn (boxplot)\n",
    "    5. Churn rate by internet service (bar)\n",
    "    6. Churn rate by number of products (bar)\n",
    "    \"\"\"\n",
    "    # TODO: Create 2x3 figure (18, 10)\n",
    "    # TODO: Implement each subplot\n",
    "    pass\n",
    "\n",
    "plot_churn_analysis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## Part 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Create New Features\n",
    "\n",
    "**Your Task:** Engineer derived features from the raw data including tenure-based, charges-based, support ratio, and a composite engagement score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Create new features from existing data.\n",
    "    \n",
    "    Features to create:\n",
    "    - tenure_years: tenure_months / 12\n",
    "    - is_new_customer: 1 if tenure <= 6 months\n",
    "    - avg_monthly_charge: total_charges / tenure_months\n",
    "    - charge_per_product: monthly_charges / num_products\n",
    "    - support_per_tenure: support_tickets / tenure_months\n",
    "    - engagement_score: weighted composite (products * 0.3 + tenure_norm * 0.3 + partner * 0.2 + dependents * 0.2)\n",
    "    \n",
    "    Returns: DataFrame with new features added\n",
    "    \"\"\"\n",
    "    # TODO: Create each derived feature\n",
    "    # TODO: Handle division by zero in tenure-based features\n",
    "    pass\n",
    "\n",
    "df_ml = engineer_features(df.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Encode Categorical Variables\n",
    "\n",
    "**Your Task:** One-hot encode the categorical columns and drop the customer_id column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(df_ml):\n",
    "    \"\"\"One-hot encode categorical variables.\n",
    "    \n",
    "    Encode: contract_type, payment_method, internet_service (drop_first=True)\n",
    "    Drop: customer_id\n",
    "    \n",
    "    Returns: encoded DataFrame\n",
    "    \"\"\"\n",
    "    # TODO: Apply pd.get_dummies with drop_first=True\n",
    "    # TODO: Drop customer_id column\n",
    "    pass\n",
    "\n",
    "df_encoded = encode_features(df_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Prepare Train/Test Split\n",
    "\n",
    "**Your Task:** Split the data 80/20 with stratification on the target, then scale numeric features using StandardScaler (fit on train only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_encoded):\n",
    "    \"\"\"Split data and scale numeric features.\n",
    "    \n",
    "    - Split 80/20 stratified on churn\n",
    "    - Scale numeric features with StandardScaler (fit on train only!)\n",
    "    \n",
    "    Returns: X_train, X_test, y_train, y_test, scaler\n",
    "    \"\"\"\n",
    "    # TODO: Separate features (X) and target (y = 'churn')\n",
    "    # TODO: train_test_split with test_size=0.2, stratify=y, random_state=42\n",
    "    # TODO: Define numeric_features list\n",
    "    # TODO: Fit scaler on X_train, transform both X_train and X_test\n",
    "    pass\n",
    "\n",
    "X_train, X_test, y_train, y_test, scaler = prepare_data(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9",
   "metadata": {},
   "source": [
    "## Part 3: Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Train Three Models\n",
    "\n",
    "**Your Task:** Train Logistic Regression, Random Forest, and Gradient Boosting classifiers. Compute accuracy, precision, recall, F1, and AUC for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train 3 models and compute metrics.\n",
    "    \n",
    "    Models: LogisticRegression, RandomForestClassifier, GradientBoostingClassifier\n",
    "    Metrics: accuracy, precision, recall, f1, auc\n",
    "    \n",
    "    Returns: dict of {model_name: {model, y_pred, y_prob, accuracy, precision, recall, f1, auc}}\n",
    "    \"\"\"\n",
    "    # TODO: Define models dict with 3 classifiers\n",
    "    # TODO: For each model: fit, predict, predict_proba, compute all metrics\n",
    "    # TODO: Print results for each model\n",
    "    pass\n",
    "\n",
    "results = train_and_evaluate(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Cross-Validation\n",
    "\n",
    "**Your Task:** Perform 5-fold stratified cross-validation using F1 scoring on all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_models(results, X_train, y_train):\n",
    "    \"\"\"Perform 5-fold stratified cross-validation on all models.\n",
    "    \n",
    "    Print mean and std of F1 scores for each model.\n",
    "    \"\"\"\n",
    "    # TODO: Create StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    # TODO: Run cross_val_score with scoring='f1' for each model\n",
    "    # TODO: Print formatted results\n",
    "    pass\n",
    "\n",
    "cross_validate_models(results, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Confusion Matrices\n",
    "\n",
    "**Your Task:** Plot confusion matrices for all three models side by side using seaborn heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(results, y_test):\n",
    "    \"\"\"Plot confusion matrices for all 3 models side by side.\n",
    "    \n",
    "    Use sns.heatmap with annot=True, fmt='d', cmap='Blues'.\n",
    "    Labels: ['Stayed', 'Churned']\n",
    "    \"\"\"\n",
    "    # TODO: Create 1x3 subplot figure (18, 5)\n",
    "    # TODO: For each model, compute confusion_matrix and plot heatmap\n",
    "    pass\n",
    "\n",
    "plot_confusion_matrices(results, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6",
   "metadata": {},
   "source": [
    "### Exercise 3.4: ROC Curves\n",
    "\n",
    "**Your Task:** Plot ROC curves for all models on a single figure with AUC values in the legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(results, y_test):\n",
    "    \"\"\"Plot ROC curves for all models on a single figure.\n",
    "    \n",
    "    Include diagonal reference line and legend with AUC values.\n",
    "    \"\"\"\n",
    "    # TODO: Create figure (10, 8)\n",
    "    # TODO: For each model, compute roc_curve and plot\n",
    "    # TODO: Add diagonal reference line\n",
    "    # TODO: Add legend, labels, title\n",
    "    pass\n",
    "\n",
    "plot_roc_curves(results, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8",
   "metadata": {},
   "source": [
    "## Part 4: Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7d8e9",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Analyze Feature Importance\n",
    "\n",
    "**Your Task:** Extract and visualize the top 15 features from Random Forest, then compare importance rankings between Random Forest and Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(results, X_train):\n",
    "    \"\"\"Plot top 15 features from Random Forest by importance.\n",
    "    \n",
    "    Also print the top 10 features with their importance values.\n",
    "    \"\"\"\n",
    "    # TODO: Get feature_importances_ from Random Forest model\n",
    "    # TODO: Create DataFrame, sort by importance\n",
    "    # TODO: Plot horizontal bar chart\n",
    "    # TODO: Print top 10\n",
    "    pass\n",
    "\n",
    "plot_feature_importance(results, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_importance(results, X_train):\n",
    "    \"\"\"Side-by-side comparison of Random Forest vs Gradient Boosting importance.\n",
    "    \n",
    "    Show top 10 features from each model.\n",
    "    \"\"\"\n",
    "    # TODO: Get importances from both models\n",
    "    # TODO: Create 1x2 subplot (18, 8)\n",
    "    pass\n",
    "\n",
    "compare_importance(results, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0a1b2",
   "metadata": {},
   "source": [
    "## Part 5: Model Deployment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1b2c3",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Save the Best Model\n",
    "\n",
    "**Your Task:** Identify the best model by F1 score and save both the model and scaler using joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(results, scaler):\n",
    "    \"\"\"Identify and save the best model (by F1 score).\n",
    "    \n",
    "    Save both the model and scaler using joblib.\n",
    "    \"\"\"\n",
    "    # TODO: Find model with highest F1 score\n",
    "    # TODO: Save model as 'churn_model.pkl'\n",
    "    # TODO: Save scaler as 'churn_scaler.pkl'\n",
    "    pass\n",
    "\n",
    "save_best_model(results, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e6",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Create Prediction Function\n",
    "\n",
    "**Your Task:** Build a function that loads the saved model and scaler, engineers the same features, encodes categoricals, aligns columns, scales, and predicts churn probability for a new customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_churn(customer_data, model_path='churn_model.pkl', scaler_path='churn_scaler.pkl'):\n",
    "    \"\"\"Predict churn probability for a new customer.\n",
    "    \n",
    "    Steps:\n",
    "    1. Load model and scaler\n",
    "    2. Engineer features (same as training)\n",
    "    3. One-hot encode categoricals\n",
    "    4. Align columns with training data\n",
    "    5. Scale numeric features\n",
    "    6. Predict probability\n",
    "    \n",
    "    Returns: dict with 'prediction' and 'churn_probability'\n",
    "    \"\"\"\n",
    "    # TODO: Load model and scaler with joblib\n",
    "    # TODO: Create DataFrame from customer_data\n",
    "    # TODO: Engineer same features as in training\n",
    "    # TODO: One-hot encode\n",
    "    # TODO: Align columns with X_train (fill missing with 0)\n",
    "    # TODO: Scale numeric features\n",
    "    # TODO: Predict probability and return result\n",
    "    pass\n",
    "\n",
    "# Test with a high-risk customer\n",
    "test_customer = {\n",
    "    'tenure_months': 3, 'monthly_charges': 95.00, 'total_charges': 285.00,\n",
    "    'contract_type': 'Month-to-month', 'payment_method': 'Electronic check',\n",
    "    'internet_service': 'Fiber optic', 'support_tickets': 5, 'num_products': 1,\n",
    "    'age': 32, 'has_partner': 0, 'has_dependents': 0\n",
    "}\n",
    "# result = predict_churn(test_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a loyal customer\n",
    "loyal_customer = {\n",
    "    'tenure_months': 48,\n",
    "    'monthly_charges': 55.00,\n",
    "    'total_charges': 2640.00,\n",
    "    'contract_type': 'Two year',\n",
    "    'payment_method': 'Bank transfer',\n",
    "    'internet_service': 'DSL',\n",
    "    'support_tickets': 1,\n",
    "    'num_products': 4,\n",
    "    'age': 45,\n",
    "    'has_partner': 1,\n",
    "    'has_dependents': 1\n",
    "}\n",
    "\n",
    "# result = predict_churn(loyal_customer)\n",
    "# print(f\"Prediction: {result['prediction']}\")\n",
    "# print(f\"Churn Probability: {result['churn_probability']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up saved files\n",
    "import os\n",
    "for f in ['churn_model.pkl', 'churn_scaler.pkl']:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8ca",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Generate and explore** a realistic churn dataset\n",
    "2. **Engineer features** from raw data (tenure-based, charge-based, engagement)\n",
    "3. **Encode and scale** features for ML\n",
    "4. **Train and compare** three models (Logistic Regression, Random Forest, Gradient Boosting)\n",
    "5. **Evaluate models** with cross-validation, confusion matrices, and ROC curves\n",
    "6. **Analyze feature importance** to understand model decisions\n",
    "7. **Save and deploy** models with joblib and build prediction functions\n",
    "\n",
    "---\n",
    "\n",
    "*Introduction to Data Science & Engineering | AI Elevate*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}