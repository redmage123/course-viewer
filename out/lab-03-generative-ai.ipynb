{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Generative AI with Ollama\n",
    "\n",
    "**Duration:** 90-120 minutes | **Difficulty:** Intermediate to Advanced\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lab teaches you how to work with Large Language Models (LLMs) using Ollama for local inference.\n",
    "\n",
    "### Lab Structure\n",
    "\n",
    "| Part | Topic | Key Concepts |\n",
    "|------|-------|---------------|\n",
    "| **Part 1** | Basic Generation | ollama.generate(), prompts |\n",
    "| **Part 2** | Prompt Engineering | System prompts, roles, structured output |\n",
    "| **Part 3** | Generation Parameters | Temperature, top_p, max_tokens |\n",
    "| **Part 4** | Chat Conversations | Multi-turn chat, message history |\n",
    "| **Part 5** | Building Applications | Summarization, sentiment, Q&A |\n",
    "| **Part 6** | RAG | Embeddings, retrieval, augmented generation |\n",
    "| **Part 7** | Fine-tuning Concepts | LoRA, QLoRA, training data |\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Ollama must be installed and running locally\n",
    "- Pull a model: `ollama pull llama3.1:8b-instruct-q4_K_M`\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Read each markdown cell carefully\n",
    "- Write your code in the empty code cells\n",
    "- Run cells with `Shift+Enter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cell below to import libraries and verify Ollama is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# Check connection to Ollama\n",
    "try:\n",
    "    models = ollama.list()\n",
    "    print(\"Connected to Ollama!\")\n",
    "    print(\"\\nAvailable models:\")\n",
    "    for model in models.get('models', []):\n",
    "        print(f\"  - {model.get('model', model.get('name', 'unknown'))}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama: {e}\")\n",
    "    print(\"\\nMake sure Ollama is running: ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Basic Text Generation\n",
    "\n",
    "Ollama provides a simple API for generating text with local LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Simple Generation\n",
    "\n",
    "Use `ollama.generate()` to generate text:\n",
    "\n",
    "```python\n",
    "response = ollama.generate(\n",
    "    model='llama3.1:8b-instruct-q4_K_M',  # or your available model\n",
    "    prompt='Your prompt here'\n",
    ")\n",
    "print(response['response'])\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Call `ollama.generate()` with model 'llama3.1:8b-instruct-q4_K_M' (or your available model)\n",
    "2. Use the prompt: \"What is machine learning in one sentence?\"\n",
    "3. Print the response\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Machine learning is a type of artificial intelligence that enables computers \n",
    "to learn from data and make predictions or decisions without being explicitly \n",
    "programmed.\n",
    "```\n",
    "(Output will vary slightly each time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.2 Exploring the Response\n\nThe response object contains useful metadata:\n\n```python\nresponse['response']      # The generated text\nresponse['model']         # Model used\nresponse['total_duration'] # Time in nanoseconds\nresponse['eval_count']    # Tokens generated\n```\n\n**Your Task:**\n1. Generate a response for: \"Explain neural networks briefly\"\n2. Print the response text\n3. Print the model name\n4. Calculate and print the generation time in seconds (total_duration / 1e9)\n\n**Expected Output:**\n```\nResponse: Neural networks are computing systems inspired by biological neural \nnetworks. They consist of layers of interconnected nodes that process and \nlearn from data...\n\nModel: llama3.1:8b-instruct-q4_K_M:latest\nGeneration time: 2.45 seconds\n```\n\n**Sample Code:**\n```python\n# Accessing response metadata\nresponse = ollama.generate(model='llama3.1:8b-instruct-q4_K_M', prompt='Hello')\ntext = response['response']\nmodel_name = response['model']\nduration_sec = response['total_duration'] / 1e9\nprint(f\"Text: {text}\")\nprint(f\"Model: {model_name}\")\nprint(f\"Time: {duration_sec:.2f}s\")\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Prompt Engineering\n",
    "\n",
    "Better prompts lead to better outputs. Learn techniques to guide the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.1 Role-Based Prompting\n\nGive the model a role to improve responses:\n\n```python\nprompt = \"\"\"You are an expert Python programmer.\n\nQuestion: How do I read a CSV file in Python?\n\nAnswer:\"\"\"\n```\n\n**Your Task:**\n1. Create a prompt that assigns the role of \"expert data scientist\"\n2. Ask: \"What are the top 3 things to check when debugging a machine learning model?\"\n3. Generate and print the response\n\n**Expected Output:**\n```\nAs an expert data scientist, here are the top 3 things to check:\n\n1. **Data Quality**: Check for missing values, outliers, and data leakage...\n2. **Model Overfitting/Underfitting**: Compare train vs validation metrics...\n3. **Feature Engineering**: Verify feature scaling and encoding...\n```\n\n**Sample Code:**\n```python\n# Using role-based prompting\nprompt = \"\"\"You are an expert chef specializing in Italian cuisine.\n\nQuestion: What makes a great pasta dish?\n\nAnswer:\"\"\"\n\nresponse = ollama.generate(model='llama3.1:8b-instruct-q4_K_M', prompt=prompt)\nprint(response['response'])\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Structured Output\n",
    "\n",
    "Request specific output formats in your prompt:\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"List 3 programming languages.\n",
    "\n",
    "Format your response as a JSON array like this:\n",
    "[\"language1\", \"language2\", \"language3\"]\n",
    "\n",
    "Only output the JSON, nothing else.\"\"\"\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Create a prompt asking for 3 benefits of AI\n",
    "2. Request the output in JSON format with keys: benefit1, benefit2, benefit3\n",
    "3. Generate and print the response\n",
    "4. Try parsing it with `json.loads()` (may need to extract just the JSON part)\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "{\n",
    "    \"benefit1\": \"Automation of repetitive tasks\",\n",
    "    \"benefit2\": \"Improved decision-making through data analysis\",\n",
    "    \"benefit3\": \"24/7 availability and scalability\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Generation Parameters\n",
    "\n",
    "Control the creativity and style of outputs with generation parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Temperature\n",
    "\n",
    "Temperature controls randomness:\n",
    "- **Low (0.1-0.3)**: More focused, deterministic\n",
    "- **Medium (0.5-0.7)**: Balanced creativity\n",
    "- **High (0.8-1.0)**: More random, creative\n",
    "\n",
    "```python\n",
    "response = ollama.generate(\n",
    "    model='llama3.1:8b-instruct-q4_K_M',\n",
    "    prompt='Write a creative story opening',\n",
    "    options={'temperature': 0.9}  # High creativity\n",
    ")\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Use the prompt: \"Write a one-sentence tagline for a coffee shop\"\n",
    "2. Generate with temperature=0.2 (low) and print the result\n",
    "3. Generate with temperature=0.9 (high) and print the result\n",
    "4. Compare the differences\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Temperature 0.2 (Low - More predictable):\n",
    "\"Start your day with the perfect cup.\"\n",
    "\n",
    "Temperature 0.9 (High - More creative):\n",
    "\"Where caffeine dreams dance in ceramic clouds.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Other Parameters\n",
    "\n",
    "Additional generation options:\n",
    "\n",
    "| Parameter | Description | Example |\n",
    "|-----------|-------------|----------|\n",
    "| `num_predict` | Max tokens to generate | `100` |\n",
    "| `top_p` | Nucleus sampling threshold | `0.9` |\n",
    "| `top_k` | Limit vocabulary choices | `40` |\n",
    "| `repeat_penalty` | Reduce repetition | `1.1` |\n",
    "\n",
    "```python\n",
    "options = {\n",
    "    'temperature': 0.7,\n",
    "    'num_predict': 50,\n",
    "    'top_p': 0.9\n",
    "}\n",
    "response = ollama.generate(model='llama3.1:8b-instruct-q4_K_M', prompt='...', options=options)\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Generate a response with `num_predict=30` to limit output length\n",
    "2. Use prompt: \"Explain the internet in simple terms\"\n",
    "3. Print the response\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "The internet is a global network of connected computers that allows people \n",
    "to share information and communicate with each other...\n",
    "(output truncated at ~30 tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Chat Conversations\n",
    "\n",
    "Build multi-turn conversations with message history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Chat API\n",
    "\n",
    "Use `ollama.chat()` for conversations:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "    {'role': 'user', 'content': 'Hello!'}\n",
    "]\n",
    "\n",
    "response = ollama.chat(model='llama3.1:8b-instruct-q4_K_M', messages=messages)\n",
    "print(response['message']['content'])\n",
    "```\n",
    "\n",
    "Roles:\n",
    "- `system`: Sets the assistant's behavior\n",
    "- `user`: User messages\n",
    "- `assistant`: Previous assistant responses\n",
    "\n",
    "**Your Task:**\n",
    "1. Create a messages list with a system message: \"You are a friendly cooking assistant\"\n",
    "2. Add a user message: \"What's a quick breakfast idea?\"\n",
    "3. Call `ollama.chat()` and print the response\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Here's a quick and delicious breakfast idea: scrambled eggs with toast!\n",
    "\n",
    "Ingredients: 2 eggs, butter, salt, pepper, 2 slices of bread\n",
    "Time: 5 minutes\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Multi-turn Conversation\n",
    "\n",
    "Continue a conversation by adding messages:\n",
    "\n",
    "```python\n",
    "# After getting a response, add it to history\n",
    "messages.append({'role': 'assistant', 'content': response['message']['content']})\n",
    "\n",
    "# Add next user message\n",
    "messages.append({'role': 'user', 'content': 'Tell me more'})\n",
    "\n",
    "# Get next response\n",
    "response = ollama.chat(model='llama3.1:8b-instruct-q4_K_M', messages=messages)\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Start a conversation about Python programming\n",
    "2. Ask a follow-up question that references the first response\n",
    "3. Print both responses showing the conversation flow\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "User: What is Python best used for?\n",
    "Assistant: Python is best used for web development, data science, \n",
    "machine learning, automation, and scripting...\n",
    "\n",
    "User: Can you give me an example of the first one you mentioned?\n",
    "Assistant: Sure! For web development, you can use frameworks like Django \n",
    "or Flask. Here's a simple Flask example...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Building Applications\n",
    "\n",
    "Apply LLMs to common NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Text Summarization\n",
    "\n",
    "Create a summarization function:\n",
    "\n",
    "```python\n",
    "def summarize(text, max_sentences=2):\n",
    "    prompt = f\"\"\"Summarize this text in {max_sentences} sentences:\n",
    "\n",
    "{text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "    response = ollama.generate(model='llama3.1:8b-instruct-q4_K_M', prompt=prompt)\n",
    "    return response['response']\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Create a `summarize(text, max_sentences)` function\n",
    "2. Test it with this text:\n",
    "   ```\n",
    "   Machine learning is a subset of artificial intelligence that enables systems to learn from data. \n",
    "   Instead of being explicitly programmed, these systems improve through experience. \n",
    "   ML is used in many applications including image recognition, natural language processing, and recommendation systems.\n",
    "   ```\n",
    "3. Print the summary\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Summary: Machine learning is a branch of AI that allows systems to learn \n",
    "from data rather than being explicitly programmed. It powers applications \n",
    "like image recognition and recommendation systems.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Sentiment Analysis\n",
    "\n",
    "Analyze the sentiment of text:\n",
    "\n",
    "```python\n",
    "def analyze_sentiment(text):\n",
    "    prompt = f\"\"\"Analyze the sentiment of this text.\n",
    "Respond with exactly one word: POSITIVE, NEGATIVE, or NEUTRAL.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Sentiment:\"\"\"\n",
    "    response = ollama.generate(model='llama3.1:8b-instruct-q4_K_M', prompt=prompt)\n",
    "    return response['response'].strip()\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Create an `analyze_sentiment(text)` function\n",
    "2. Test it with these examples:\n",
    "   - \"I love this product! It's amazing!\"\n",
    "   - \"This is the worst experience ever.\"\n",
    "   - \"The meeting is scheduled for 3pm.\"\n",
    "3. Print the sentiment for each\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Text: \"I love this product! It's amazing!\"\n",
    "Sentiment: POSITIVE\n",
    "\n",
    "Text: \"This is the worst experience ever.\"\n",
    "Sentiment: NEGATIVE\n",
    "\n",
    "Text: \"The meeting is scheduled for 3pm.\"\n",
    "Sentiment: NEUTRAL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "RAG combines retrieval with generation for knowledge-grounded responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Embeddings\n",
    "\n",
    "Get vector embeddings for semantic search:\n",
    "\n",
    "```python\n",
    "response = ollama.embeddings(\n",
    "    model='llama3.1:8b-instruct-q4_K_M',  # or embedding model\n",
    "    prompt='Your text here'\n",
    ")\n",
    "embedding = response['embedding']  # List of floats\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Get embeddings for two similar sentences:\n",
    "   - \"I love programming in Python\"\n",
    "   - \"Python is my favorite programming language\"\n",
    "2. Print the dimension of the embeddings\n",
    "3. Calculate cosine similarity between them using:\n",
    "   ```python\n",
    "   def cosine_similarity(a, b):\n",
    "       return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "   ```\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Embedding dimension: 4096\n",
    "Cosine similarity: 0.89 (high similarity for similar sentences)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Simple RAG Implementation\n",
    "\n",
    "Build a basic RAG system:\n",
    "\n",
    "1. **Index documents**: Create embeddings for your knowledge base\n",
    "2. **Retrieve**: Find relevant documents for a query\n",
    "3. **Generate**: Use retrieved context to answer\n",
    "\n",
    "```python\n",
    "class SimpleRAG:\n",
    "    def __init__(self, model='llama3.1:8b-instruct-q4_K_M'):\n",
    "        self.model = model\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def add_document(self, text):\n",
    "        self.documents.append(text)\n",
    "        emb = ollama.embeddings(model=self.model, prompt=text)['embedding']\n",
    "        self.embeddings.append(emb)\n",
    "    \n",
    "    def query(self, question, top_k=2):\n",
    "        # Get query embedding\n",
    "        q_emb = ollama.embeddings(model=self.model, prompt=question)['embedding']\n",
    "        \n",
    "        # Find most similar documents\n",
    "        similarities = [cosine_similarity(q_emb, e) for e in self.embeddings]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        # Build context\n",
    "        context = \"\\n\".join([self.documents[i] for i in top_indices])\n",
    "        \n",
    "        # Generate answer\n",
    "        prompt = f\"\"\"Context: {context}\n",
    "        \n",
    "Question: {question}\n",
    "\n",
    "Answer based on the context:\"\"\"\n",
    "        \n",
    "        response = ollama.generate(model=self.model, prompt=prompt)\n",
    "        return response['response']\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Implement the `SimpleRAG` class\n",
    "2. Add these documents:\n",
    "   - \"Python was created by Guido van Rossum in 1991.\"\n",
    "   - \"JavaScript is the language of the web browser.\"\n",
    "   - \"Machine learning uses algorithms to learn from data.\"\n",
    "3. Query: \"Who created Python?\"\n",
    "4. Print the answer\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Added 3 documents to RAG system\n",
    "\n",
    "Query: Who created Python?\n",
    "Answer: Based on the context, Python was created by Guido van Rossum in 1991.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Fine-tuning Concepts\n",
    "\n",
    "Understanding how to customize LLMs for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 LoRA (Low-Rank Adaptation)\n",
    "\n",
    "LoRA is an efficient fine-tuning technique:\n",
    "\n",
    "- Instead of updating all weights, adds small trainable matrices\n",
    "- Reduces memory requirements significantly\n",
    "- Key parameters:\n",
    "  - `r` (rank): Size of adaptation matrices (4-64)\n",
    "  - `alpha`: Scaling factor\n",
    "  - `target_modules`: Which layers to adapt\n",
    "\n",
    "```python\n",
    "# Conceptual LoRA configuration (not runnable without training setup)\n",
    "lora_config = {\n",
    "    'r': 16,\n",
    "    'lora_alpha': 32,\n",
    "    'target_modules': ['q_proj', 'v_proj'],\n",
    "    'lora_dropout': 0.05\n",
    "}\n",
    "```\n",
    "\n",
    "**Your Task:** (Conceptual - no code execution needed)\n",
    "1. Create a dictionary `lora_config` with:\n",
    "   - `r`: 8\n",
    "   - `lora_alpha`: 16\n",
    "   - `target_modules`: ['q_proj', 'k_proj', 'v_proj']\n",
    "   - `lora_dropout`: 0.1\n",
    "2. Print the configuration\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "LoRA Configuration:\n",
    "{\n",
    "    'r': 8,\n",
    "    'lora_alpha': 16,\n",
    "    'target_modules': ['q_proj', 'k_proj', 'v_proj'],\n",
    "    'lora_dropout': 0.1\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 QLoRA (Quantized LoRA)\n",
    "\n",
    "QLoRA combines quantization with LoRA:\n",
    "\n",
    "- Uses 4-bit quantization to reduce memory even further\n",
    "- Enables fine-tuning large models on consumer GPUs\n",
    "- Key concepts:\n",
    "  - **NF4**: Normalized Float 4-bit quantization\n",
    "  - **Double quantization**: Quantizes the quantization constants\n",
    "\n",
    "```python\n",
    "# Conceptual QLoRA configuration\n",
    "qlora_config = {\n",
    "    'load_in_4bit': True,\n",
    "    'bnb_4bit_quant_type': 'nf4',\n",
    "    'bnb_4bit_use_double_quant': True,\n",
    "    'bnb_4bit_compute_dtype': 'float16'\n",
    "}\n",
    "```\n",
    "\n",
    "**Your Task:** (Conceptual)\n",
    "1. Create a dictionary `qlora_config` with the 4-bit configuration shown above\n",
    "2. Print it and explain (in comments) what each parameter does\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "QLoRA Configuration:\n",
    "{\n",
    "    'load_in_4bit': True,           # Load model in 4-bit precision\n",
    "    'bnb_4bit_quant_type': 'nf4',   # Use NormalFloat4 quantization\n",
    "    'bnb_4bit_use_double_quant': True,  # Apply double quantization\n",
    "    'bnb_4bit_compute_dtype': 'float16'  # Compute in float16\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7.3 Training Data Format\n\nFine-tuning requires properly formatted training data:\n\n**Instruction format:**\n```json\n{\n    \"instruction\": \"What is the capital of France?\",\n    \"input\": \"\",\n    \"output\": \"The capital of France is Paris.\"\n}\n```\n\n**Chat format:**\n```json\n{\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n        {\"role\": \"assistant\", \"content\": \"2+2 equals 4.\"}\n    ]\n}\n```\n\n**Your Task:**\n1. Create a list of 3 training examples in instruction format for a customer support bot\n2. Print the examples as JSON\n\n**Expected Output:**\n```json\n[\n    {\n        \"instruction\": \"How do I reset my password?\",\n        \"input\": \"\",\n        \"output\": \"To reset your password, click 'Forgot Password' on the login page...\"\n    },\n    {\n        \"instruction\": \"What are your business hours?\",\n        \"input\": \"\",\n        \"output\": \"Our business hours are Monday-Friday, 9am-5pm EST.\"\n    },\n    {\n        \"instruction\": \"How do I track my order?\",\n        \"input\": \"\",\n        \"output\": \"You can track your order by logging into your account and clicking 'Order History'.\"\n    }\n]\n```\n\n**Sample Code:**\n```python\n# Creating training data in instruction format\ntraining_examples = [\n    {\n        \"instruction\": \"Translate hello to Spanish\",\n        \"input\": \"\",\n        \"output\": \"Hello in Spanish is 'Hola'.\"\n    },\n    {\n        \"instruction\": \"What is the square root of 16?\",\n        \"input\": \"\",\n        \"output\": \"The square root of 16 is 4.\"\n    }\n]\n\nprint(json.dumps(training_examples, indent=2))\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lab Complete!\n",
    "\n",
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- **Basic Generation**: ollama.generate() for text completion\n",
    "- **Prompt Engineering**: Roles, structured output, formatting\n",
    "- **Parameters**: Temperature, top_p, num_predict\n",
    "- **Chat**: Multi-turn conversations with message history\n",
    "- **Applications**: Summarization, sentiment analysis, Q&A\n",
    "- **RAG**: Embeddings, retrieval, augmented generation\n",
    "- **Fine-tuning**: LoRA, QLoRA, training data formats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}