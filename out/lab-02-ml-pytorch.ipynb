{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 2: Machine Learning with PyTorch\n\n**Duration:** 90-120 minutes | **Difficulty:** Intermediate\n\n---\n\n## Overview\n\nThis lab teaches PyTorch fundamentals through hands-on implementation of ML algorithms.\n\n### Lab Structure\n\n| Part | Topic | Key Concepts |\n|------|-------|---------------|\n| **Part 1** | PyTorch Tensors | Creating tensors, operations, autograd |\n| **Part 2** | Linear Regression | Training loop, MSE loss, nn.Module |\n| **Part 3** | Logistic Regression | Sigmoid, BCE loss, classification |\n| **Part 4** | Support Vector Machines | Kernels, margins, support vectors |\n| **Part 5** | Model Evaluation | Confusion matrix, precision, recall, F1 |\n| **Part 6** | Natural Language Processing | Tokenization, TF-IDF, Naive Bayes, sentiment analysis |\n\n### Instructions\n\n- Read each markdown cell carefully\n- Write your code in the empty code cells\n- Run cells with `Shift+Enter`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cell below to import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import make_classification, make_moons\n\n# NLP imports\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nimport re\n\nplt.rcParams['figure.figsize'] = [10, 6]\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nprint(\"Setup complete!\")\nprint(f\"PyTorch version: {torch.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: PyTorch Tensors\n",
    "\n",
    "Tensors are the fundamental data structure in PyTorch - similar to NumPy arrays but with GPU support and automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.1 Creating Tensors\n\nCreate tensors using these functions:\n\n| Function | Description | Example |\n|----------|-------------|----------|\n| `torch.tensor([...])` | From Python list | `torch.tensor([1, 2, 3])` |\n| `torch.zeros(shape)` | Zeros tensor | `torch.zeros(3, 4)` |\n| `torch.ones(shape)` | Ones tensor | `torch.ones(2, 3)` |\n| `torch.randn(shape)` | Random normal | `torch.randn(3, 3)` |\n| `torch.from_numpy(arr)` | From NumPy | `torch.from_numpy(np_array)` |\n\n**Your Task:** Create the following tensors:\n1. `t1`: A 1D tensor with values [1.0, 2.0, 3.0, 4.0, 5.0]\n2. `t2`: A 3×3 tensor of zeros\n3. `t3`: A 2×4 tensor of random values\n4. `t4`: Convert this NumPy array to a tensor: `np.array([[1, 2], [3, 4]])`\n\nPrint each tensor and its shape using `.shape`.\n\n**Expected Output:**\n```\nt1: tensor([1., 2., 3., 4., 5.])\nt1 shape: torch.Size([5])\n\nt2:\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\nt2 shape: torch.Size([3, 3])\n\nt3 shape: torch.Size([2, 4])\n\nt4:\ntensor([[1, 2],\n        [3, 4]])\nt4 shape: torch.Size([2, 2])\n```\n\n**Sample Code:**\n```python\n# Creating tensors in different ways\nmy_tensor = torch.tensor([10.0, 20.0, 30.0])\nmy_ones = torch.ones(2, 2)\nnp_arr = np.array([5, 6, 7])\nfrom_np = torch.from_numpy(np_arr)\nprint(\"Tensor:\", my_tensor)\nprint(\"Shape:\", my_tensor.shape)\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.2 Tensor Operations\n\nCommon tensor operations:\n\n| Operation | Description | Example |\n|-----------|-------------|----------|\n| `a + b` | Element-wise addition | `t1 + t2` |\n| `a * b` | Element-wise multiplication | `t1 * t2` |\n| `a @ b` or `torch.matmul(a, b)` | Matrix multiplication | `t1 @ t2` |\n| `t.sum()` | Sum all elements | `tensor.sum()` |\n| `t.mean()` | Mean of elements | `tensor.mean()` |\n| `t.reshape(shape)` | Reshape tensor | `t.reshape(2, 3)` |\n\n**Your Task:** \n1. Create two tensors: `a = torch.tensor([[1., 2.], [3., 4.]])` and `b = torch.tensor([[5., 6.], [7., 8.]])`\n2. Calculate and print:\n   - `add_result`: Element-wise addition of a and b\n   - `mult_result`: Element-wise multiplication of a and b\n   - `matmul_result`: Matrix multiplication of a and b\n   - `sum_a`: Sum of all elements in a\n   - `mean_b`: Mean of all elements in b\n\n**Expected Output:**\n```\nAddition:\ntensor([[ 6.,  8.],\n        [10., 12.]])\n\nMultiplication:\ntensor([[ 5., 12.],\n        [21., 32.]])\n\nMatrix Multiplication:\ntensor([[19., 22.],\n        [43., 50.]])\n\nSum of a: tensor(10.)\nMean of b: tensor(6.5000)\n```\n\n**Sample Code:**\n```python\n# Tensor operations\nx = torch.tensor([[1., 2.], [3., 4.]])\ny = torch.tensor([[2., 0.], [1., 3.]])\nadded = x + y                    # Element-wise add\nproduct = x * y                  # Element-wise multiply\nmatrix_prod = x @ y              # Matrix multiply\ntotal = x.sum()                  # Sum all\nprint(\"Sum:\", total)\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.3 Automatic Differentiation (Autograd)\n\n### This is NOT Machine Learning (Yet!)\n\nIn this section, we're **not** training a model. We're demonstrating something simpler: **PyTorch can compute derivatives automatically**.\n\nThis is just calculus:\n- Given: y = x³\n- Question: What is dy/dx when x = 3?\n- Answer: dy/dx = 3x² = 3(9) = 27\n\nYou could solve this with pen and paper. But PyTorch can do it automatically with `autograd`.\n\n### Why Does This Matter for ML?\n\nHere's the connection - in machine learning training:\n\n| What we have | What we need |\n|--------------|--------------|\n| Loss = f(weights, data) | d(Loss)/d(weights) |\n| A number (how wrong we are) | Which direction to adjust weights |\n\nComputing `d(Loss)/d(weights)` by hand for a neural network with millions of weights? Impossible. But PyTorch's autograd does it automatically using the **same mechanism** we're demonstrating here.\n\n### The Simple Demo\n\n```python\n# This is just calculus, not ML!\nx = torch.tensor([3.0], requires_grad=True)  # Our variable\ny = x ** 3                                     # y = x³ = 27\n\n# Ask PyTorch: \"What is dy/dx?\"\ny.backward()\n\n# PyTorch computed: dy/dx = 3x² = 3(9) = 27\nprint(x.grad)  # tensor([27.])\n```\n\n**There's no model, no data, no loss function** - just a math expression and its derivative.\n\n### How This Becomes ML (Preview of Part 2)\n\nIn Part 2, we'll use the same `.backward()` but with actual ML components:\n\n```python\n# PART 2 (coming up) - Actual ML training:\nmodel = LinearRegression()           # Model with weights\nprediction = model(X_train)          # Forward pass with real data\nloss = criterion(prediction, y_train) # Compute loss (how wrong are we?)\nloss.backward()                       # Compute d(loss)/d(weights) ← Same autograd!\noptimizer.step()                      # Update weights using gradients\n```\n\nThe `loss.backward()` in Part 2 uses the **exact same autograd engine** as `y.backward()` here - just with more operations in the chain.\n\n### Key Concepts\n\n| Concept | What it does |\n|---------|--------------|\n| `requires_grad=True` | Tell PyTorch: \"I'll want the derivative with respect to this\" |\n| `.backward()` | Compute the derivative |\n| `.grad` | Access the computed derivative |\n\n---\n\n**Your Task:** Let PyTorch compute a derivative for you.\n\n1. Create `x = torch.tensor([3.0], requires_grad=True)`\n2. Compute `y = x ** 3`\n3. Call `y.backward()` to compute dy/dx\n4. Print `x.grad` (should be 27.0, since d/dx(x³) = 3x² = 27)\n\n**Expected Output:**\n```\nx = tensor([3.], requires_grad=True)\ny = x^3 = tensor([27.], grad_fn=<PowBackward0>)\nGradient (dy/dx) = tensor([27.])\n```\n\nThis is PyTorch doing calculus for you. In Part 2, we'll use this same tool for actual model training!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Linear Regression\n",
    "\n",
    "Linear regression finds the best-fit line: `y = wx + b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Generate Data\n",
    "\n",
    "Run the cell below to create training data with a known relationship: `y = 3x + 1 + noise`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linear data: y = 3x + 1 + noise\n",
    "np.random.seed(42)\n",
    "X_np = np.random.rand(100, 1) * 10  # 100 samples, values 0-10\n",
    "y_np = 3 * X_np + 1 + np.random.randn(100, 1) * 2  # y = 3x + 1 + noise\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.FloatTensor(X_np)\n",
    "y = torch.FloatTensor(y_np)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(X_np, y_np, alpha=0.6)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Training Data (y = 3x + 1 + noise)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define a Linear Model\n",
    "\n",
    "Create a model using `nn.Module`:\n",
    "\n",
    "```python\n",
    "class ModelName(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "```\n",
    "\n",
    "**Your Task:** Create a `LinearRegression` class:\n",
    "1. Define `__init__` with `nn.Linear(1, 1)` (1 input feature, 1 output)\n",
    "2. Define `forward` that returns `self.linear(x)`\n",
    "3. Create an instance: `model = LinearRegression()`\n",
    "4. Print the model\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "LinearRegression(\n",
    "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Train the Model\n",
    "\n",
    "The PyTorch training loop:\n",
    "\n",
    "```python\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update weights\n",
    "```\n",
    "\n",
    "**Your Task:** Train your linear regression model:\n",
    "1. Create `criterion = nn.MSELoss()`\n",
    "2. Create `optimizer = optim.SGD(model.parameters(), lr=0.01)`\n",
    "3. Train for 100 epochs using the loop pattern above\n",
    "4. Print the loss every 20 epochs\n",
    "5. After training, print the learned weight and bias:\n",
    "   - `model.linear.weight.item()` (should be close to 3)\n",
    "   - `model.linear.bias.item()` (should be close to 1)\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Epoch 0, Loss: XX.XXXX\n",
    "Epoch 20, Loss: X.XXXX\n",
    "Epoch 40, Loss: X.XXXX\n",
    "Epoch 60, Loss: X.XXXX\n",
    "Epoch 80, Loss: X.XXXX\n",
    "\n",
    "Learned weight: ~3.0 (close to 3)\n",
    "Learned bias: ~1.0 (close to 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Logistic Regression\n",
    "\n",
    "Logistic regression is used for binary classification. It uses the sigmoid function to output probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Generate Classification Data\n",
    "\n",
    "Run the cell below to create a binary classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=200, n_features=2, n_redundant=0, \n",
    "    n_informative=2, n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_t = torch.FloatTensor(X_train)\n",
    "y_train_t = torch.FloatTensor(y_train).reshape(-1, 1)\n",
    "X_test_t = torch.FloatTensor(X_test)\n",
    "y_test_t = torch.FloatTensor(y_test).reshape(-1, 1)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='coolwarm', alpha=0.6)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Binary Classification Data')\n",
    "plt.colorbar(label='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define Logistic Regression Model\n",
    "\n",
    "Logistic regression applies sigmoid to a linear transformation:\n",
    "\n",
    "```python\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Create a `LogisticRegression` class with 2 input features (matching our data)\n",
    "2. Use `torch.sigmoid()` in the forward method\n",
    "3. Create an instance: `log_model = LogisticRegression(2)`\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "LogisticRegression(\n",
    "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Train Logistic Regression\n",
    "\n",
    "For binary classification, use Binary Cross Entropy loss:\n",
    "\n",
    "```python\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Create BCELoss criterion and SGD optimizer (lr=0.1)\n",
    "2. Train for 200 epochs\n",
    "3. Print loss every 40 epochs\n",
    "4. After training, evaluate on test set:\n",
    "   - Get predictions: `y_pred = (log_model(X_test_t) >= 0.5).float()`\n",
    "   - Calculate accuracy: `(y_pred == y_test_t).float().mean()`\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Epoch 0, Loss: 0.XXXX\n",
    "Epoch 40, Loss: 0.XXXX\n",
    "Epoch 80, Loss: 0.XXXX\n",
    "Epoch 120, Loss: 0.XXXX\n",
    "Epoch 160, Loss: 0.XXXX\n",
    "\n",
    "Test Accuracy: ~0.95 (around 95%)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Support Vector Machines\n",
    "\n",
    "SVMs find the optimal hyperplane that separates classes with maximum margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 SVM with Different Kernels\n",
    "\n",
    "scikit-learn's SVC (Support Vector Classifier):\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear')  # or 'rbf', 'poly'\n",
    "svm.fit(X_train, y_train)\n",
    "predictions = svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "```\n",
    "\n",
    "Available kernels:\n",
    "- `'linear'`: Linear decision boundary\n",
    "- `'rbf'`: Radial Basis Function (good for non-linear data)\n",
    "- `'poly'`: Polynomial kernel\n",
    "\n",
    "**Your Task:**\n",
    "1. Train an SVM with `kernel='linear'` on X_train, y_train\n",
    "2. Predict on X_test\n",
    "3. Calculate and print the accuracy\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Linear SVM Accuracy: ~0.95 (around 95%)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Compare Kernels on Non-Linear Data\n",
    "\n",
    "Run the cell below to create moon-shaped data that's not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-linear (moon-shaped) data\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='coolwarm', alpha=0.6)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Moon-shaped Data (Non-linear)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4.3 Train and Compare Kernels\n\n**Your Task:** Train SVMs with different kernels and compare accuracies:\n1. Train an SVM with `kernel='linear'`\n2. Train an SVM with `kernel='rbf'`\n3. Train an SVM with `kernel='poly'`\n4. Print the accuracy for each kernel\n\nWhich kernel works best for this non-linear data?\n\n**Expected Output:**\n```\nLinear kernel accuracy: ~0.88\nRBF kernel accuracy: ~1.00 (best for non-linear data)\nPoly kernel accuracy: ~0.93\n```\n\n**Sample Code:**\n```python\n# Training SVMs with different kernels\nsvm_linear = SVC(kernel='linear')\nsvm_linear.fit(X_train_m, y_train_m)\npred_linear = svm_linear.predict(X_test_m)\nacc_linear = accuracy_score(y_test_m, pred_linear)\nprint(f\"Linear accuracy: {acc_linear:.2f}\")\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Model Evaluation\n",
    "\n",
    "Beyond accuracy: precision, recall, F1-score, and confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Confusion Matrix\n",
    "\n",
    "A confusion matrix shows the breakdown of predictions:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "# [[TN, FP],\n",
    "#  [FN, TP]]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- TN: True Negatives (correctly predicted 0)\n",
    "- FP: False Positives (incorrectly predicted 1)\n",
    "- FN: False Negatives (incorrectly predicted 0)\n",
    "- TP: True Positives (correctly predicted 1)\n",
    "\n",
    "**Your Task:**\n",
    "1. Use your best SVM model from Part 4 to get predictions on the moon test data\n",
    "2. Create a confusion matrix\n",
    "3. Print the confusion matrix\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Confusion Matrix:\n",
    "[[29  0]\n",
    " [ 0 31]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Classification Report\n",
    "\n",
    "Get precision, recall, and F1-score in one report:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)\n",
    "```\n",
    "\n",
    "Metrics:\n",
    "- **Precision**: Of predicted positives, how many are actually positive?\n",
    "- **Recall**: Of actual positives, how many did we predict correctly?\n",
    "- **F1-score**: Harmonic mean of precision and recall\n",
    "\n",
    "**Your Task:**\n",
    "1. Generate a classification report for your SVM predictions\n",
    "2. Print the report\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00        29\n",
    "           1       1.00      1.00      1.00        31\n",
    "\n",
    "    accuracy                           1.00        60\n",
    "   macro avg       1.00      1.00      1.00        60\n",
    "weighted avg       1.00      1.00      1.00        60\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Part 6: Natural Language Processing\n\nLearn to process text data for machine learning: tokenization, vectorization, and text classification."
  },
  {
   "cell_type": "markdown",
   "source": "## 6.1 Text Preprocessing\n\nBefore feeding text to ML models, we must clean and normalize it.\n\nCommon preprocessing steps:\n\n| Step | Description | Example |\n|------|-------------|---------|\n| Lowercase | Convert to lowercase | \"Hello World\" → \"hello world\" |\n| Remove punctuation | Strip special characters | \"Hello!\" → \"Hello\" |\n| Tokenization | Split into words | \"hello world\" → [\"hello\", \"world\"] |\n| Remove stopwords | Remove common words | [\"the\", \"is\", \"a\"] removed |\n\n**Your Task:** Write a function `preprocess_text(text)` that:\n1. Converts text to lowercase\n2. Removes all non-alphanumeric characters (keep spaces)\n3. Returns the cleaned text\n\nTest it on: `\"Hello, World! This is NLP 101.\"`\n\n**Expected Output:**\n```\nOriginal: Hello, World! This is NLP 101.\nCleaned: hello world this is nlp 101\n```\n\n**Sample Code:**\n```python\nimport re\n\ndef clean_example(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Keep only alphanumeric and spaces\n    return text\n\nresult = clean_example(\"Test! 123\")\nprint(result)  # \"test 123\"\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Your code here\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.2 Bag of Words with CountVectorizer\n\nBag of Words converts text to numerical vectors by counting word occurrences.\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(documents)  # Returns sparse matrix\nprint(vectorizer.get_feature_names_out())  # Vocabulary\nprint(X.toarray())  # Dense matrix\n```\n\n**Your Task:**\n1. Create a list of 3 documents:\n   - \"I love machine learning\"\n   - \"Machine learning is great\"\n   - \"I love programming\"\n2. Create a `CountVectorizer` and fit_transform the documents\n3. Print the vocabulary (feature names)\n4. Print the document-term matrix as an array\n\n**Expected Output:**\n```\nVocabulary: ['great' 'is' 'learning' 'love' 'machine' 'programming']\n\nDocument-Term Matrix:\n[[0 0 1 1 1 0]\n [1 1 1 0 1 0]\n [0 0 0 1 0 1]]\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Your code here\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.3 TF-IDF Vectorization\n\nTF-IDF (Term Frequency - Inverse Document Frequency) weights words by importance:\n- **TF**: How often a word appears in a document\n- **IDF**: How rare a word is across all documents\n- Words common in one doc but rare overall get high scores\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer()\nX = tfidf.fit_transform(documents)\nprint(X.toarray())  # TF-IDF weighted matrix\n```\n\n**Your Task:**\n1. Use the same 3 documents from 6.2\n2. Create a `TfidfVectorizer` and fit_transform the documents\n3. Print the TF-IDF matrix (rounded to 2 decimals)\n4. Observe: Which words have higher weights? Why?\n\n**Expected Output:**\n```\nTF-IDF Matrix:\n[[0.   0.   0.52 0.68 0.52 0.  ]\n [0.55 0.55 0.42 0.   0.42 0.  ]\n [0.   0.   0.   0.61 0.   0.79]]\n\nNote: 'programming' has high weight in doc 3 (unique to it)\n      'love' has lower weight (appears in docs 1 and 3)\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Your code here\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.4 Sentiment Analysis Dataset\n\nRun the cell below to create a simple sentiment analysis dataset.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sentiment Analysis Dataset\nreviews = [\n    \"This movie was amazing and wonderful\",\n    \"I loved this film, it was great\",\n    \"Excellent movie, highly recommended\",\n    \"Best film I have ever seen\",\n    \"Wonderful story and great acting\",\n    \"This movie was terrible and boring\",\n    \"I hated this film, it was awful\",\n    \"Worst movie ever, do not watch\",\n    \"Boring and disappointing film\",\n    \"Terrible acting and bad story\",\n    \"The movie was okay, nothing special\",\n    \"It was an average film\",\n    \"Not bad but not great either\",\n    \"Mediocre movie with some good moments\",\n    \"Fantastic cinematography and brilliant performances\",\n    \"Absolutely dreadful, waste of time\",\n]\n\n# Labels: 1 = positive, 0 = negative\nlabels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0]\n\n# Split into train and test\nX_train_text, X_test_text, y_train_nlp, y_test_nlp = train_test_split(\n    reviews, labels, test_size=0.25, random_state=42\n)\n\nprint(f\"Training samples: {len(X_train_text)}\")\nprint(f\"Test samples: {len(X_test_text)}\")\nprint(f\"\\nSample reviews:\")\nfor i in range(3):\n    sentiment = \"Positive\" if y_train_nlp[i] == 1 else \"Negative\"\n    print(f\"  [{sentiment}] {X_train_text[i]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.5 Text Classification with Naive Bayes\n\nNaive Bayes is a probabilistic classifier that works well with text data.\n\n```python\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Vectorize text\nvectorizer = TfidfVectorizer()\nX_train_vec = vectorizer.fit_transform(X_train_text)\nX_test_vec = vectorizer.transform(X_test_text)\n\n# Train classifier\nclf = MultinomialNB()\nclf.fit(X_train_vec, y_train)\n\n# Predict\npredictions = clf.predict(X_test_vec)\n```\n\n**Your Task:**\n1. Create a `TfidfVectorizer` and vectorize the training and test text\n2. Create a `MultinomialNB` classifier and train it\n3. Make predictions on the test set\n4. Calculate and print the accuracy\n5. Print the classification report\n\n**Expected Output:**\n```\nAccuracy: ~0.75 (75%)\n\nClassification Report:\n              precision    recall  f1-score   support\n           0       X.XX      X.XX      X.XX         X\n           1       X.XX      X.XX      X.XX         X\n```\n\n**Sample Code:**\n```python\n# Complete pipeline example\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('clf', MultinomialNB())\n])\npipeline.fit(X_train_text, y_train_nlp)\naccuracy = pipeline.score(X_test_text, y_test_nlp)\nprint(f\"Accuracy: {accuracy:.2f}\")\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Your code here\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6.6 Predict on New Text\n\nUse your trained model to classify new reviews.\n\n**Your Task:**\n1. Create a list of 3 new reviews (make up your own!)\n2. Vectorize them using the same vectorizer (use `.transform()`, not `.fit_transform()`)\n3. Use your classifier to predict the sentiment\n4. Print each review with its predicted sentiment\n\n**Expected Output:**\n```\nReview: \"This was the best experience ever!\"\nPredicted: Positive\n\nReview: \"Horrible waste of my time\"\nPredicted: Negative\n\nReview: \"It was pretty good overall\"\nPredicted: Positive\n```\n\n**Sample Code:**\n```python\nnew_reviews = [\"Your review here\", \"Another review\"]\nnew_vectors = vectorizer.transform(new_reviews)\npredictions = clf.predict(new_vectors)\n\nfor review, pred in zip(new_reviews, predictions):\n    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n    print(f\"Review: {review}\")\n    print(f\"Predicted: {sentiment}\\n\")\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Your code here\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# Lab Complete!\n\n## Summary\n\nYou learned:\n- **PyTorch Tensors**: Create, manipulate, and use autograd\n- **Linear Regression**: nn.Module, training loop, MSE loss\n- **Logistic Regression**: Sigmoid, BCE loss, classification\n- **SVMs**: Different kernels for linear/non-linear data\n- **Evaluation**: Confusion matrix, precision, recall, F1-score\n- **NLP**: Text preprocessing, Bag of Words, TF-IDF, Naive Bayes classification",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}