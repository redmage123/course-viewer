{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 3: Semantic Search & Data Catalogue RAG\n\n**Data Discovery: Harnessing AI, AGI & Vector Databases - Day 2**\n\n| Duration | Framework | Sections |\n|---|---|---|\n| 90 min | pandas, sentence-transformers, chromadb, rank_bm25, scikit-learn | 6 |\n\nIn this lab, you'll explore:\n- Comparing embedding models for domain-specific retrieval\n- Building a BM25 keyword search index\n- Combining keyword and semantic search with hybrid fusion\n- Evaluating search quality with precision@k\n- Building a RAG pipeline over a data catalogue\n- Expanding queries with embedding similarity\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "qp0n2qotpb9",
   "source": "## Student Notes & Background\n\n### Why Semantic Search Matters for Data Discovery\n\nTraditional data catalogues rely on **keyword matching** — if you search for \"salary data,\" you only find assets whose metadata literally contains the word \"salary.\" But what about assets described as \"compensation benchmarking\" or \"payroll deductions\"? These are clearly relevant, yet a keyword search misses them entirely.\n\n**Semantic search** solves this by converting text into dense numerical vectors (embeddings) that capture *meaning*, not just surface words. Two descriptions that are conceptually similar will have vectors that are close together in embedding space, even if they share no words in common.\n\n### Key Concepts\n\n#### 1. Embedding Models\nAn **embedding model** (e.g., `all-MiniLM-L6-v2` from Sentence-Transformers) takes a piece of text and maps it to a fixed-length vector, typically 384 or 768 dimensions. These models are pre-trained on large text corpora and fine-tuned so that semantically similar texts produce similar vectors. Different models have different strengths — some excel at short queries, others at long documents, and some are tuned for specific domains.\n\n**Cosine similarity** is the standard metric for comparing embeddings. It measures the angle between two vectors:\n- **1.0** = identical direction (maximum similarity)\n- **0.0** = orthogonal (no similarity)\n- **-1.0** = opposite direction (though rare with modern embeddings)\n\n#### 2. BM25 Keyword Search\n**BM25 (Best Matching 25)** is a classical information retrieval algorithm that improves on simple TF-IDF. It scores documents by:\n- **Term Frequency (TF):** How often the query term appears in the document (with diminishing returns)\n- **Inverse Document Frequency (IDF):** Terms that appear in fewer documents are weighted higher\n- **Document length normalisation:** Longer documents are penalised slightly to avoid bias\n\nBM25 excels at finding exact keyword matches and is extremely fast, but it cannot understand synonyms, paraphrases, or conceptual relationships.\n\n#### 3. Hybrid Search (Fusion)\nNeither keyword nor semantic search is universally superior. **Hybrid search** combines both using weighted fusion:\n\n```\nfinal_score = α × BM25_normalised + (1 - α) × cosine_similarity\n```\n\n- **α = 0.0:** Pure semantic search\n- **α = 0.5:** Equal weight (good default)\n- **α = 1.0:** Pure keyword search\n\nThe optimal α depends on your data and use case. In practice, hybrid search consistently outperforms either strategy alone.\n\n#### 4. Precision@k\n**Precision@k** measures what fraction of the top-k results are truly relevant:\n\n```\nPrecision@k = (number of relevant results in top k) / k\n```\n\nFor example, if you search for \"employee data\" and 3 of your top-5 results are from the HR category, your Precision@5 = 3/5 = 0.60.\n\n#### 5. Retrieval-Augmented Generation (RAG)\n**RAG** is a pattern that combines retrieval with language model generation:\n1. **Retrieve** relevant documents using search (keyword, semantic, or hybrid)\n2. **Augment** a prompt with the retrieved context\n3. **Generate** an answer grounded in the retrieved facts\n\nIn this lab, we simulate the generation step with structured extraction since we don't have a live LLM, but the retrieval pipeline is identical to what you'd use in production.\n\n#### 6. Query Expansion\n**Query expansion** improves recall by adding semantically related terms to the original query before searching. For example, expanding \"salary\" might add \"compensation,\" \"payroll,\" \"remuneration,\" and \"wages.\" This helps bridge vocabulary gaps between the user's query and the catalogue descriptions.\n\n### What You'll Build\n\nIn this lab, you will:\n1. **Compare** two embedding models (`all-MiniLM-L6-v2` vs `all-mpnet-base-v2`) by measuring how well each clusters descriptions from the same department\n2. **Build** a BM25 keyword search index and test it with domain queries\n3. **Implement** hybrid search that fuses BM25 and semantic scores with a tuneable α parameter\n4. **Evaluate** search quality using Precision@k against known ground-truth category labels\n5. **Build** a RAG-style pipeline that retrieves relevant catalogue entries and produces structured answers\n6. **Implement** query expansion using embedding similarity over the corpus vocabulary\n\n### Prerequisites\n- Familiarity with pandas DataFrames and numpy arrays\n- Basic understanding of cosine similarity (from Lab 1)\n- Concepts from Lab 1: TF-IDF vectorisation, ChromaDB basics\n\n### Tips\n- The synthetic data uses a fixed random seed (`np.random.seed(42)`), so your results should be reproducible\n- When comparing search strategies, pay attention to which categories each strategy retrieves — this reveals their strengths and weaknesses\n- The α parameter in hybrid search is powerful — experiment with different values to build intuition\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Embeddings & vector DB\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Keyword search\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Synthetic Data Catalogue\n",
    "\n",
    "We'll reuse the same 500-asset catalogue from Lab 1 so results are directly comparable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "categories = ['HR', 'Finance', 'Marketing', 'Engineering', 'Legal']\n",
    "sources = ['PostgreSQL', 'S3 Bucket', 'SharePoint', 'Salesforce', 'MongoDB']\n",
    "data_types = ['Table', 'Document', 'Spreadsheet', 'Log File', 'Report']\n",
    "sensitivity_levels = ['Public', 'Internal', 'Confidential', 'Restricted']\n",
    "\n",
    "descriptions_pool = {\n",
    "    'HR': [\n",
    "        'Employee personal records including name address and date of birth',\n",
    "        'Annual performance review scores and manager feedback',\n",
    "        'Payroll data with salary deductions and tax withholdings',\n",
    "        'Recruitment pipeline tracking applicant status and interview notes',\n",
    "        'Benefits enrollment records for health dental and vision plans',\n",
    "        'Employee onboarding documentation and training completion',\n",
    "        'Workforce diversity and inclusion metrics by department',\n",
    "        'Time and attendance records with overtime calculations',\n",
    "        'Employee termination records and exit interview summaries',\n",
    "        'Compensation benchmarking data across industry roles',\n",
    "    ],\n",
    "    'Finance': [\n",
    "        'Quarterly revenue reports broken down by business unit',\n",
    "        'Accounts payable invoices and payment processing records',\n",
    "        'Annual budget forecasts with departmental allocations',\n",
    "        'Customer billing records including credit card transactions',\n",
    "        'Expense reimbursement claims with receipt attachments',\n",
    "        'General ledger entries and journal adjustments',\n",
    "        'Tax filing documents and regulatory compliance records',\n",
    "        'Cash flow projections and working capital analysis',\n",
    "        'Vendor payment terms and contract financial summaries',\n",
    "        'Audit trail logs for financial transaction approvals',\n",
    "    ],\n",
    "    'Marketing': [\n",
    "        'Campaign performance metrics including click rates and conversions',\n",
    "        'Customer segmentation profiles based on purchase behaviour',\n",
    "        'Social media analytics with engagement and reach data',\n",
    "        'Email marketing subscriber lists with opt-in preferences',\n",
    "        'Brand sentiment analysis from customer reviews and surveys',\n",
    "        'Website traffic analytics and user journey tracking',\n",
    "        'Lead scoring models and marketing qualified lead reports',\n",
    "        'Content calendar and editorial planning documents',\n",
    "        'Competitive intelligence reports and market research data',\n",
    "        'Event registration lists with attendee contact information',\n",
    "    ],\n",
    "    'Engineering': [\n",
    "        'Application server logs with error traces and stack dumps',\n",
    "        'CI/CD pipeline metrics including build times and failure rates',\n",
    "        'Infrastructure monitoring data from cloud resources',\n",
    "        'API usage statistics and rate limiting configurations',\n",
    "        'Database schema documentation and migration scripts',\n",
    "        'Code repository commit history and pull request reviews',\n",
    "        'Load testing results and performance benchmarks',\n",
    "        'Security vulnerability scan reports and remediation tracking',\n",
    "        'Microservice dependency maps and architecture diagrams',\n",
    "        'Incident response logs and post-mortem analysis documents',\n",
    "    ],\n",
    "    'Legal': [\n",
    "        'Active contract repository with vendor agreements and SLAs',\n",
    "        'Intellectual property filings including patents and trademarks',\n",
    "        'Regulatory compliance audit findings and remediation plans',\n",
    "        'Data processing agreements under GDPR Article 28',\n",
    "        'Litigation case files and legal correspondence records',\n",
    "        'Corporate governance meeting minutes and board resolutions',\n",
    "        'Privacy impact assessments for new data processing activities',\n",
    "        'Non-disclosure agreement tracking and expiration dates',\n",
    "        'Employment law compliance documentation by jurisdiction',\n",
    "        'Insurance policy records and claims history',\n",
    "    ],\n",
    "}\n",
    "\n",
    "n_assets = 500\n",
    "records = []\n",
    "\n",
    "for i in range(n_assets):\n",
    "    cat = np.random.choice(categories)\n",
    "    desc = np.random.choice(descriptions_pool[cat])\n",
    "    if np.random.random() < 0.3:\n",
    "        desc += ' updated ' + np.random.choice(['weekly', 'monthly', 'quarterly', 'annually'])\n",
    "    records.append({\n",
    "        'asset_id': f'ASSET-{i+1:04d}',\n",
    "        'name': f'{cat.lower()}_{np.random.choice([\"report\", \"dataset\", \"log\", \"file\", \"table\"])}_{i+1:04d}',\n",
    "        'description': desc,\n",
    "        'category': cat,\n",
    "        'source': np.random.choice(sources),\n",
    "        'data_type': np.random.choice(data_types),\n",
    "        'sensitivity': np.random.choice(sensitivity_levels, p=[0.15, 0.35, 0.30, 0.20]),\n",
    "        'owner': np.random.choice(['alice', 'bob', 'carol', 'dave', 'eve', None], p=[0.2, 0.2, 0.2, 0.2, 0.15, 0.05]),\n",
    "        'row_count': np.random.randint(100, 1_000_000) if np.random.random() > 0.2 else None,\n",
    "        'last_updated': pd.Timestamp('2023-01-01') + pd.Timedelta(days=int(np.random.randint(0, 730))),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(f\"Generated {len(df)} data asset records\")\n",
    "df.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1.1: Embedding Model Comparison\n\nThe code below loads two embedding models — the lightweight MiniLM (384 dimensions) and the higher-quality MPNet (768 dimensions) — encodes all catalogue descriptions with both, and compares their **intra-class cosine similarity** to see which produces tighter clusters per category."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load both embedding models\nmodel_mini = SentenceTransformer('all-MiniLM-L6-v2')\nmodel_mpnet = SentenceTransformer('all-mpnet-base-v2')\n\ndescriptions = df['description'].tolist()\n\nprint(\"Encoding descriptions with all-MiniLM-L6-v2...\")\nemb_mini = model_mini.encode(descriptions, show_progress_bar=True)\n\nprint(\"Encoding descriptions with all-mpnet-base-v2...\")\nemb_mpnet = model_mpnet.encode(descriptions, show_progress_bar=True)\n\nprint(f\"\\nMiniLM embeddings shape:  {emb_mini.shape}\")\nprint(f\"MPNet embeddings shape:   {emb_mpnet.shape}\")\n\n# Compute intra-class similarity for each model\ndef compute_intra_class_similarity(embeddings, labels):\n    \"\"\"Compute average cosine similarity among items in the same class.\"\"\"\n    unique_labels = sorted(set(labels))\n    results = {}\n    for label in unique_labels:\n        mask = [i for i, l in enumerate(labels) if l == label]\n        class_emb = embeddings[mask]\n        sim_matrix = cosine_similarity(class_emb)\n        # Take upper triangle (exclude diagonal)\n        n = len(mask)\n        upper_tri = sim_matrix[np.triu_indices(n, k=1)]\n        results[label] = float(np.mean(upper_tri))\n    return results\n\nlabels = df['category'].tolist()\nsim_mini = compute_intra_class_similarity(emb_mini, labels)\nsim_mpnet = compute_intra_class_similarity(emb_mpnet, labels)\n\nprint(\"\\nIntra-class cosine similarity (higher = tighter clusters):\")\nprint(f\"{'Category':<15} {'MiniLM':>10} {'MPNet':>10}\")\nprint(\"-\" * 37)\nfor cat in categories:\n    print(f\"{cat:<15} {sim_mini[cat]:>10.4f} {sim_mpnet[cat]:>10.4f}\")\nprint(f\"{'Average':<15} {np.mean(list(sim_mini.values())):>10.4f} {np.mean(list(sim_mpnet.values())):>10.4f}\")\n\n# Bar chart comparison\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(categories))\nwidth = 0.35\nbars1 = ax.bar(x - width/2, [sim_mini[c] for c in categories], width,\n               label='all-MiniLM-L6-v2', color='#3b82f6')\nbars2 = ax.bar(x + width/2, [sim_mpnet[c] for c in categories], width,\n               label='all-mpnet-base-v2', color='#10b981')\nax.set_xlabel('Category')\nax.set_ylabel('Intra-class Cosine Similarity')\nax.set_title('Embedding Model Comparison: Intra-class Similarity')\nax.set_xticks(x)\nax.set_xticklabels(categories)\nax.legend()\nax.set_ylim(0, 1)\nplt.tight_layout()\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ey96qqo6rvf",
   "source": "### Analysis Questions\n\n1. Which model (MiniLM vs MPNet) produces higher intra-category similarity? What does this mean for search quality?\n2. Consider the speed vs accuracy trade-off — when would you choose the smaller model?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1.2: BM25 Keyword Search\n\nThe code below builds a BM25 keyword search index over the catalogue descriptions. BM25 is a classical information retrieval algorithm that excels at finding exact keyword matches — fast and effective for queries containing specific terminology."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Tokenize descriptions for BM25\ntokenized_corpus = [desc.lower().split() for desc in descriptions]\n\n# Build BM25 index\nbm25 = BM25Okapi(tokenized_corpus)\nprint(f\"BM25 index built over {len(tokenized_corpus)} documents\")\nprint(f\"Average document length: {np.mean([len(d) for d in tokenized_corpus]):.1f} tokens\")\n\ndef bm25_search(query, top_k=10):\n    \"\"\"Search the catalogue using BM25 keyword matching.\"\"\"\n    tokenized_query = query.lower().split()\n    scores = bm25.get_scores(tokenized_query)\n    top_indices = np.argsort(scores)[::-1][:top_k]\n    results = []\n    for idx in top_indices:\n        results.append({\n            'rank': len(results) + 1,\n            'asset_id': df.iloc[idx]['asset_id'],\n            'category': df.iloc[idx]['category'],\n            'description': df.iloc[idx]['description'],\n            'score': scores[idx],\n        })\n    return results\n\n# Test queries\ntest_queries = [\n    \"employee salary payroll\",\n    \"revenue budget financial\",\n    \"server logs monitoring\",\n    \"contract compliance GDPR\",\n    \"campaign click conversion\",\n]\n\nfor query in test_queries:\n    results = bm25_search(query, top_k=5)\n    print(f\"\\nQuery: '{query}'\")\n    print(\"-\" * 70)\n    for r in results:\n        print(f\"  {r['rank']}. [{r['category']:<12}] {r['description'][:65]}... (score: {r['score']:.3f})\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "n0cjn50p6ss",
   "source": "### Analysis Questions\n\n1. For which queries does BM25 work well? Where does it fail?\n2. Why does BM25 miss semantically related results that don't share exact keywords?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2.1: Hybrid Search\n\nThe code below combines BM25 keyword scores with semantic cosine similarity using weighted fusion. The `alpha` parameter controls the balance: alpha=0.0 is pure BM25, alpha=1.0 is pure semantic, and values in between blend both signals."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Use MiniLM embeddings for the hybrid search (faster, good quality)\ncatalogue_embeddings = emb_mini\n\ndef hybrid_search(query, alpha=0.5, top_k=10):\n    \"\"\"Combine BM25 keyword scores with semantic cosine similarity.\n    \n    Args:\n        query: Natural language search query\n        alpha: Weight for semantic score (0=pure BM25, 1=pure semantic)\n        top_k: Number of results to return\n    \n    Returns:\n        List of result dicts with hybrid scores\n    \"\"\"\n    # BM25 scores\n    tokenized_query = query.lower().split()\n    bm25_scores = bm25.get_scores(tokenized_query)\n    \n    # Normalise BM25 scores to [0, 1]\n    bm25_max = bm25_scores.max()\n    if bm25_max > 0:\n        bm25_norm = bm25_scores / bm25_max\n    else:\n        bm25_norm = bm25_scores\n    \n    # Semantic cosine similarity scores\n    query_emb = model_mini.encode([query])\n    semantic_scores = cosine_similarity(query_emb, catalogue_embeddings)[0]\n    \n    # Normalise semantic scores to [0, 1]\n    sem_min = semantic_scores.min()\n    sem_max = semantic_scores.max()\n    if sem_max > sem_min:\n        semantic_norm = (semantic_scores - sem_min) / (sem_max - sem_min)\n    else:\n        semantic_norm = semantic_scores\n    \n    # Weighted fusion\n    hybrid_scores = alpha * semantic_norm + (1 - alpha) * bm25_norm\n    \n    # Rank by hybrid score\n    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n    results = []\n    for idx in top_indices:\n        results.append({\n            'rank': len(results) + 1,\n            'asset_id': df.iloc[idx]['asset_id'],\n            'category': df.iloc[idx]['category'],\n            'description': df.iloc[idx]['description'],\n            'hybrid_score': hybrid_scores[idx],\n            'semantic_score': semantic_scores[idx],\n            'bm25_score': bm25_scores[idx],\n        })\n    return results\n\n# Demonstrate hybrid search with different alpha values\ndemo_query = \"employee salary compensation data\"\n\nfor alpha in [0.0, 0.5, 1.0]:\n    label = {0.0: 'Pure BM25', 0.5: 'Hybrid (50/50)', 1.0: 'Pure Semantic'}[alpha]\n    results = hybrid_search(demo_query, alpha=alpha, top_k=5)\n    print(f\"\\n{'='*70}\")\n    print(f\"Query: '{demo_query}' | Strategy: {label} (alpha={alpha})\")\n    print(f\"{'='*70}\")\n    for r in results:\n        print(f\"  {r['rank']}. [{r['category']:<12}] {r['description'][:55]}...\")\n        print(f\"     hybrid={r['hybrid_score']:.3f}  sem={r['semantic_score']:.3f}  bm25={r['bm25_score']:.3f}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "w0y2jkmg6ec",
   "source": "### Analysis Questions\n\n1. As alpha changes from 0 (BM25) to 1 (semantic), how do the results shift?\n2. What alpha value seems to produce the best results? Why might a 50/50 blend not always be optimal?\n3. Can you identify a query where pure semantic beats hybrid, or vice versa?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2.2: Search Quality Evaluation\n\nThe code below evaluates BM25, semantic, and hybrid search using **Precision@k** — the fraction of top-k results that belong to the expected category. It tests 15 ground-truth queries across all 5 departments and compares strategies at k=5 and k=10."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Ground truth: queries mapped to their expected relevant category\nground_truth = {\n    \"employee salary payroll\": \"HR\",\n    \"performance review feedback\": \"HR\",\n    \"recruitment hiring interview\": \"HR\",\n    \"revenue budget quarterly\": \"Finance\",\n    \"invoice payment accounts\": \"Finance\",\n    \"tax filing compliance\": \"Finance\",\n    \"campaign click conversion\": \"Marketing\",\n    \"customer segmentation purchase\": \"Marketing\",\n    \"social media engagement\": \"Marketing\",\n    \"server logs error monitoring\": \"Engineering\",\n    \"CI/CD pipeline build deployment\": \"Engineering\",\n    \"API usage rate limiting\": \"Engineering\",\n    \"contract vendor agreement SLA\": \"Legal\",\n    \"GDPR data processing privacy\": \"Legal\",\n    \"patent trademark intellectual property\": \"Legal\",\n}\n\ndef precision_at_k(results, relevant_category, k):\n    \"\"\"Compute precision@k: fraction of top-k results in the relevant category.\"\"\"\n    top_k_results = results[:k]\n    relevant_count = sum(1 for r in top_k_results if r['category'] == relevant_category)\n    return relevant_count / k\n\ndef semantic_search_as_results(query, top_k=10):\n    \"\"\"Pure semantic search returning results in the same format.\"\"\"\n    query_emb = model_mini.encode([query])\n    scores = cosine_similarity(query_emb, catalogue_embeddings)[0]\n    top_indices = np.argsort(scores)[::-1][:top_k]\n    results = []\n    for idx in top_indices:\n        results.append({\n            'rank': len(results) + 1,\n            'asset_id': df.iloc[idx]['asset_id'],\n            'category': df.iloc[idx]['category'],\n            'description': df.iloc[idx]['description'],\n            'score': scores[idx],\n        })\n    return results\n\n# Evaluate all three strategies\nstrategies = {\n    'BM25': lambda q, k: bm25_search(q, top_k=k),\n    'Semantic': lambda q, k: semantic_search_as_results(q, top_k=k),\n    'Hybrid': lambda q, k: hybrid_search(q, alpha=0.5, top_k=k),\n}\n\nk_values = [5, 10]\neval_results = {strat: {f'P@{k}': [] for k in k_values} for strat in strategies}\n\nfor query, relevant_cat in ground_truth.items():\n    for strat_name, search_fn in strategies.items():\n        results = search_fn(query, max(k_values))\n        for k in k_values:\n            p_at_k = precision_at_k(results, relevant_cat, k)\n            eval_results[strat_name][f'P@{k}'].append(p_at_k)\n\n# Compute averages\nprint(\"Search Quality Evaluation (averaged over 15 queries)\")\nprint(\"=\" * 50)\nprint(f\"{'Strategy':<12} {'P@5':>8} {'P@10':>8}\")\nprint(\"-\" * 30)\navg_scores = {}\nfor strat_name in strategies:\n    p5 = np.mean(eval_results[strat_name]['P@5'])\n    p10 = np.mean(eval_results[strat_name]['P@10'])\n    avg_scores[strat_name] = {'P@5': p5, 'P@10': p10}\n    print(f\"{strat_name:<12} {p5:>8.3f} {p10:>8.3f}\")\n\n# Plot comparison\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(strategies))\nwidth = 0.35\nstrat_names = list(strategies.keys())\nbars1 = ax.bar(x - width/2, [avg_scores[s]['P@5'] for s in strat_names], width,\n               label='Precision@5', color='#3b82f6')\nbars2 = ax.bar(x + width/2, [avg_scores[s]['P@10'] for s in strat_names], width,\n               label='Precision@10', color='#10b981')\n\nax.set_xlabel('Search Strategy')\nax.set_ylabel('Precision')\nax.set_title('Search Quality: Precision@K Comparison')\nax.set_xticks(x)\nax.set_xticklabels(strat_names)\nax.legend()\nax.set_ylim(0, 1.05)\n\n# Add value labels on bars\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        ax.annotate(f'{height:.2f}',\n                    xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3), textcoords=\"offset points\",\n                    ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "znrvtdr4i2",
   "source": "### Analysis Questions\n\n1. Which strategy achieves the highest Precision@5? Does this hold for Precision@10?\n2. Which category is the hardest to retrieve accurately? Why?\n3. How would you validate these results with a real user study?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 3.1: RAG Pipeline\n\nThe code below implements a Retrieval-Augmented Generation pipeline that accepts a natural-language question, retrieves the top-5 relevant assets via hybrid search, and produces a structured answer summarising the findings. In production, the generation step would use an LLM; here we simulate it with structured extraction."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def format_context(results):\n    \"\"\"Format retrieved results into a context string for the RAG pipeline.\"\"\"\n    context_parts = []\n    for r in results:\n        context_parts.append(\n            f\"[{r['asset_id']}] Category: {r['category']} | \"\n            f\"Description: {r['description']}\"\n        )\n    return \"\\n\".join(context_parts)\n\n\ndef generate_answer(question, results):\n    \"\"\"Simulate LLM generation by summarising retrieved documents.\n    \n    In production, replace this with an actual LLM call:\n        prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n        answer = llm.generate(prompt)\n    \"\"\"\n    # Extract statistics from retrieved results\n    categories_found = Counter(r['category'] for r in results)\n    unique_descriptions = list(dict.fromkeys(r['description'] for r in results))\n    asset_ids = [r['asset_id'] for r in results]\n    avg_score = np.mean([r.get('hybrid_score', 0) for r in results])\n    \n    # Build structured answer\n    answer_parts = [\n        f\"Based on {len(results)} retrieved data assets, here is a summary:\\n\",\n        f\"**Relevant Categories:** {', '.join(f'{cat} ({cnt})' for cat, cnt in categories_found.most_common())}\\n\",\n        f\"**Matching Assets:** {', '.join(asset_ids)}\\n\",\n        f\"**Key Findings:**\",\n    ]\n    for i, desc in enumerate(unique_descriptions, 1):\n        answer_parts.append(f\"  {i}. {desc}\")\n    \n    answer_parts.append(f\"\\n**Average Relevance Score:** {avg_score:.3f}\")\n    answer_parts.append(f\"**Recommendation:** Focus on the {categories_found.most_common(1)[0][0]} \"\n                        f\"category assets which dominate the results for this query.\")\n    \n    return \"\\n\".join(answer_parts)\n\n\ndef rag_query(question, alpha=0.5, top_k=5):\n    \"\"\"Full RAG pipeline: retrieve -> format context -> generate answer.\"\"\"\n    # Step 1: Retrieve\n    results = hybrid_search(question, alpha=alpha, top_k=top_k)\n    \n    # Step 2: Format context\n    context = format_context(results)\n    \n    # Step 3: Generate answer\n    answer = generate_answer(question, results)\n    \n    return {\n        'question': question,\n        'context': context,\n        'answer': answer,\n        'results': results,\n    }\n\n\n# Test the RAG pipeline with several questions\nquestions = [\n    \"What data assets contain employee salary information?\",\n    \"Which datasets track marketing campaign performance?\",\n    \"Are there any assets related to regulatory compliance or auditing?\",\n    \"What engineering monitoring and observability data do we have?\",\n]\n\nfor question in questions:\n    result = rag_query(question)\n    print(f\"\\n{'='*80}\")\n    print(f\"QUESTION: {result['question']}\")\n    print(f\"{'='*80}\")\n    print(f\"\\n--- Retrieved Context ---\")\n    print(result['context'])\n    print(f\"\\n--- Generated Answer ---\")\n    print(result['answer'])\n    print()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a94yr49qi2",
   "source": "### Analysis Questions\n\n1. Look at the structured answers — do they accurately reflect the retrieved context?\n2. Where would the RAG pipeline's answers improve with a real LLM instead of structured extraction?\n3. What happens to answer quality if the retrieval step returns poor results?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 3.2: Query Expansion\n\nThe code below improves retrieval by expanding the original query with semantically related terms from the corpus vocabulary. For example, expanding \"salary\" might add \"compensation\", \"payroll\", and \"deductions\" — bridging vocabulary gaps between the user's query and the catalogue descriptions."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Build vocabulary from corpus\nall_words = set()\nfor desc in descriptions:\n    for word in desc.lower().split():\n        # Keep only alphabetic tokens >= 3 chars\n        cleaned = ''.join(c for c in word if c.isalpha())\n        if len(cleaned) >= 3:\n            all_words.add(cleaned)\n\nvocab_list = sorted(all_words)\nprint(f\"Vocabulary size: {len(vocab_list)} unique terms\")\n\n# Embed the vocabulary\nprint(\"Encoding vocabulary...\")\nvocab_embeddings = model_mini.encode(vocab_list, show_progress_bar=True)\nprint(f\"Vocabulary embeddings shape: {vocab_embeddings.shape}\")\n\n\ndef expand_query(query, n_expansion_terms=5):\n    \"\"\"Expand a query with semantically related terms from the corpus vocabulary.\"\"\"\n    # Encode the query\n    query_emb = model_mini.encode([query])\n    \n    # Compute similarity to all vocab terms\n    similarities = cosine_similarity(query_emb, vocab_embeddings)[0]\n    \n    # Get query words to exclude them from expansion\n    query_words = set(query.lower().split())\n    \n    # Rank vocab by similarity, skip words already in query\n    ranked_indices = np.argsort(similarities)[::-1]\n    expansion_terms = []\n    for idx in ranked_indices:\n        term = vocab_list[idx]\n        if term not in query_words and similarities[idx] > 0.1:\n            expansion_terms.append((term, similarities[idx]))\n        if len(expansion_terms) >= n_expansion_terms:\n            break\n    \n    return expansion_terms\n\n\ndef expanded_hybrid_search(query, alpha=0.5, top_k=10, n_expansion=5):\n    \"\"\"Perform hybrid search with query expansion.\"\"\"\n    # Get expansion terms\n    expansion = expand_query(query, n_expansion_terms=n_expansion)\n    expansion_words = [term for term, score in expansion]\n    expanded_query = query + \" \" + \" \".join(expansion_words)\n    \n    return expanded_query, expansion, hybrid_search(expanded_query, alpha=alpha, top_k=top_k)\n\n\n# Demonstrate query expansion and improved retrieval\ndemo_queries = [\n    \"employee records\",\n    \"financial audit\",\n    \"cloud infrastructure\",\n]\n\nfor query in demo_queries:\n    print(f\"\\n{'='*80}\")\n    print(f\"Original query: '{query}'\")\n    \n    # Get expansion terms\n    expansion = expand_query(query, n_expansion_terms=5)\n    print(f\"Expansion terms: {', '.join(f'{t} ({s:.3f})' for t, s in expansion)}\")\n    \n    expanded_query, _, expanded_results = expanded_hybrid_search(query, top_k=5)\n    print(f\"Expanded query:  '{expanded_query}'\")\n    \n    # Compare original vs expanded results\n    original_results = hybrid_search(query, alpha=0.5, top_k=5)\n    \n    print(f\"\\n  {'Original Results':<40} | {'Expanded Results':<40}\")\n    print(f\"  {'-'*40} | {'-'*40}\")\n    for orig, exp in zip(original_results, expanded_results):\n        orig_str = f\"[{orig['category']:<10}] {orig['description'][:25]}...\"\n        exp_str = f\"[{exp['category']:<10}] {exp['description'][:25]}...\"\n        print(f\"  {orig_str:<40} | {exp_str:<40}\")\n\n# Quantitative evaluation: compare precision with and without expansion\nprint(f\"\\n\\n{'='*80}\")\nprint(\"Precision@5 Comparison: Standard vs Query-Expanded Hybrid Search\")\nprint(f\"{'='*80}\")\n\np5_standard = []\np5_expanded = []\n\nfor query, relevant_cat in ground_truth.items():\n    # Standard hybrid\n    std_results = hybrid_search(query, alpha=0.5, top_k=5)\n    p5_std = precision_at_k(std_results, relevant_cat, 5)\n    p5_standard.append(p5_std)\n    \n    # Expanded hybrid\n    _, _, exp_results = expanded_hybrid_search(query, alpha=0.5, top_k=5)\n    p5_exp = precision_at_k(exp_results, relevant_cat, 5)\n    p5_expanded.append(p5_exp)\n\nprint(f\"\\nAverage P@5 (Standard Hybrid): {np.mean(p5_standard):.3f}\")\nprint(f\"Average P@5 (Expanded Hybrid): {np.mean(p5_expanded):.3f}\")\nimprovement = np.mean(p5_expanded) - np.mean(p5_standard)\nprint(f\"Improvement: {improvement:+.3f} ({improvement/max(np.mean(p5_standard), 1e-9)*100:+.1f}%)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "sjh3zimjfi",
   "source": "### Analysis Questions\n\n1. Which expanded terms make the most sense semantically? Are any surprising?\n2. How much does expansion improve precision? Are there cases where it hurts?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Compare** embedding models to find the best fit for your domain\n",
    "2. **Build** a BM25 keyword search index for exact term matching\n",
    "3. **Combine** keyword and semantic search with weighted hybrid fusion\n",
    "4. **Evaluate** search quality using precision@k against ground truth\n",
    "5. **Build** a RAG pipeline that retrieves context to answer catalogue questions\n",
    "6. **Expand** queries with semantically related terms for better recall\n",
    "\n",
    "---\n",
    "\n",
    "*Data Discovery: Harnessing AI, AGI & Vector Databases | AI Elevate*"
   ]
  }
 ]
}