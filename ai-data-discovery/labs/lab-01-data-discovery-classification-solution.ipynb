{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Data Discovery & Classification - SOLUTIONS\n",
    "\n",
    "**Data Discovery: Harnessing AI, AGI & Vector Databases - Day 1**\n",
    "\n",
    "| Duration | Difficulty | Framework | Exercises |\n",
    "|---|---|---|---|\n",
    "| 90 min | Beginner | pandas, scikit-learn, sentence-transformers, chromadb | 5 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Vector database\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "categories = ['HR', 'Finance', 'Marketing', 'Engineering', 'Legal']\n",
    "sources = ['PostgreSQL', 'S3 Bucket', 'SharePoint', 'Salesforce', 'MongoDB']\n",
    "data_types = ['Table', 'Document', 'Spreadsheet', 'Log File', 'Report']\n",
    "sensitivity_levels = ['Public', 'Internal', 'Confidential', 'Restricted']\n",
    "\n",
    "descriptions_pool = {\n",
    "    'HR': [\n",
    "        'Employee personal records including name address and date of birth',\n",
    "        'Annual performance review scores and manager feedback',\n",
    "        'Payroll data with salary deductions and tax withholdings',\n",
    "        'Recruitment pipeline tracking applicant status and interview notes',\n",
    "        'Benefits enrollment records for health dental and vision plans',\n",
    "        'Employee onboarding documentation and training completion',\n",
    "        'Workforce diversity and inclusion metrics by department',\n",
    "        'Time and attendance records with overtime calculations',\n",
    "        'Employee termination records and exit interview summaries',\n",
    "        'Compensation benchmarking data across industry roles',\n",
    "    ],\n",
    "    'Finance': [\n",
    "        'Quarterly revenue reports broken down by business unit',\n",
    "        'Accounts payable invoices and payment processing records',\n",
    "        'Annual budget forecasts with departmental allocations',\n",
    "        'Customer billing records including credit card transactions',\n",
    "        'Expense reimbursement claims with receipt attachments',\n",
    "        'General ledger entries and journal adjustments',\n",
    "        'Tax filing documents and regulatory compliance records',\n",
    "        'Cash flow projections and working capital analysis',\n",
    "        'Vendor payment terms and contract financial summaries',\n",
    "        'Audit trail logs for financial transaction approvals',\n",
    "    ],\n",
    "    'Marketing': [\n",
    "        'Campaign performance metrics including click rates and conversions',\n",
    "        'Customer segmentation profiles based on purchase behaviour',\n",
    "        'Social media analytics with engagement and reach data',\n",
    "        'Email marketing subscriber lists with opt-in preferences',\n",
    "        'Brand sentiment analysis from customer reviews and surveys',\n",
    "        'Website traffic analytics and user journey tracking',\n",
    "        'Lead scoring models and marketing qualified lead reports',\n",
    "        'Content calendar and editorial planning documents',\n",
    "        'Competitive intelligence reports and market research data',\n",
    "        'Event registration lists with attendee contact information',\n",
    "    ],\n",
    "    'Engineering': [\n",
    "        'Application server logs with error traces and stack dumps',\n",
    "        'CI/CD pipeline metrics including build times and failure rates',\n",
    "        'Infrastructure monitoring data from cloud resources',\n",
    "        'API usage statistics and rate limiting configurations',\n",
    "        'Database schema documentation and migration scripts',\n",
    "        'Code repository commit history and pull request reviews',\n",
    "        'Load testing results and performance benchmarks',\n",
    "        'Security vulnerability scan reports and remediation tracking',\n",
    "        'Microservice dependency maps and architecture diagrams',\n",
    "        'Incident response logs and post-mortem analysis documents',\n",
    "    ],\n",
    "    'Legal': [\n",
    "        'Active contract repository with vendor agreements and SLAs',\n",
    "        'Intellectual property filings including patents and trademarks',\n",
    "        'Regulatory compliance audit findings and remediation plans',\n",
    "        'Data processing agreements under GDPR Article 28',\n",
    "        'Litigation case files and legal correspondence records',\n",
    "        'Corporate governance meeting minutes and board resolutions',\n",
    "        'Privacy impact assessments for new data processing activities',\n",
    "        'Non-disclosure agreement tracking and expiration dates',\n",
    "        'Employment law compliance documentation by jurisdiction',\n",
    "        'Insurance policy records and claims history',\n",
    "    ],\n",
    "}\n",
    "\n",
    "n_assets = 500\n",
    "records = []\n",
    "\n",
    "for i in range(n_assets):\n",
    "    cat = np.random.choice(categories)\n",
    "    desc = np.random.choice(descriptions_pool[cat])\n",
    "    if np.random.random() < 0.3:\n",
    "        desc += ' updated ' + np.random.choice(['weekly', 'monthly', 'quarterly', 'annually'])\n",
    "    records.append({\n",
    "        'asset_id': f'ASSET-{i+1:04d}',\n",
    "        'name': f'{cat.lower()}_{np.random.choice([\"report\", \"dataset\", \"log\", \"file\", \"table\"])}_{i+1:04d}',\n",
    "        'description': desc,\n",
    "        'category': cat,\n",
    "        'source': np.random.choice(sources),\n",
    "        'data_type': np.random.choice(data_types),\n",
    "        'sensitivity': np.random.choice(sensitivity_levels, p=[0.15, 0.35, 0.30, 0.20]),\n",
    "        'owner': np.random.choice(['alice', 'bob', 'carol', 'dave', 'eve', None], p=[0.2, 0.2, 0.2, 0.2, 0.15, 0.05]),\n",
    "        'row_count': np.random.randint(100, 1_000_000) if np.random.random() > 0.2 else None,\n",
    "        'last_updated': pd.Timestamp('2023-01-01') + pd.Timedelta(days=int(np.random.randint(0, 730))),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(f\"Generated {len(df)} data asset records\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Data Profiling - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape and data types\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMemory Usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts\n",
    "print(\"Category distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "print(\"\\nSource distribution:\")\n",
    "print(df['source'].value_counts())\n",
    "print(\"\\nData type distribution:\")\n",
    "print(df['data_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(1)\n",
    "print(\"Missing values:\")\n",
    "print(pd.DataFrame({'Missing': missing, 'Percent': missing_pct}).query('Missing > 0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "df['sensitivity'].value_counts().plot(kind='bar', ax=ax, color=['#10b981', '#3b82f6', '#f59e0b', '#ef4444'])\n",
    "ax.set_title('Data Asset Sensitivity Distribution')\n",
    "ax.set_xlabel('Sensitivity Level')\n",
    "ax.set_ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2: Text Classification - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(df):\n",
    "    \"\"\"Build a TF-IDF + RandomForest text classifier.\"\"\"\n",
    "    # Vectorise descriptions\n",
    "    tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    X_tfidf = tfidf.fit_transform(df['description'])\n",
    "    y = df['category']\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_tfidf, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Train classifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return tfidf, clf, X_tfidf\n",
    "\n",
    "tfidf, clf, X_tfidf = build_classifier(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Metadata Extraction - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_business_terms(text):\n",
    "    \"\"\"Extract business terms from a data asset description.\"\"\"\n",
    "    terms = [\n",
    "        'salary', 'revenue', 'customer', 'employee', 'invoice',\n",
    "        'compliance', 'contract', 'performance', 'billing', 'payroll',\n",
    "        'budget', 'marketing', 'security', 'legal', 'audit'\n",
    "    ]\n",
    "    found = []\n",
    "    for term in terms:\n",
    "        if re.search(rf'\\b{term}\\b', text, re.IGNORECASE):\n",
    "            found.append(term)\n",
    "    return found\n",
    "\n",
    "# Apply to all descriptions\n",
    "df['business_terms'] = df['description'].apply(extract_business_terms)\n",
    "\n",
    "# Count most common terms\n",
    "all_terms = [term for terms_list in df['business_terms'] for term in terms_list]\n",
    "term_counts = Counter(all_terms).most_common(10)\n",
    "print(\"Top 10 business terms:\")\n",
    "for term, count in term_counts:\n",
    "    print(f\"  {term:15} {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Clustering - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_visualise(X_tfidf, df, n_clusters=5):\n",
    "    \"\"\"Cluster data assets using KMeans and visualise with PCA.\"\"\"\n",
    "    # Fit KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "    # Reduce to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    coords = pca.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    scatter = ax.scatter(coords[:, 0], coords[:, 1], c=clusters, cmap='viridis', alpha=0.6, s=30)\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    ax.set_title('Data Asset Clusters (PCA Projection)')\n",
    "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print cluster composition\n",
    "    df_temp = df.copy()\n",
    "    df_temp['cluster'] = clusters\n",
    "    print(\"\\nCluster composition (actual categories):\")\n",
    "    print(pd.crosstab(df_temp['cluster'], df_temp['category']))\n",
    "\n",
    "    return clusters\n",
    "\n",
    "clusters = cluster_and_visualise(X_tfidf, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3: Vector Catalogue - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vector_catalogue(df):\n",
    "    \"\"\"Build a vector catalogue with ChromaDB.\"\"\"\n",
    "    # Load model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Encode descriptions\n",
    "    descriptions = df['description'].tolist()\n",
    "    embeddings = model.encode(descriptions)\n",
    "\n",
    "    # Create ChromaDB collection\n",
    "    client = chromadb.Client()\n",
    "    collection = client.create_collection(\"data_catalogue\")\n",
    "\n",
    "    # Add to collection\n",
    "    collection.add(\n",
    "        embeddings=embeddings.tolist(),\n",
    "        documents=descriptions,\n",
    "        ids=df['asset_id'].tolist(),\n",
    "        metadatas=[{'category': cat, 'sensitivity': sens}\n",
    "                   for cat, sens in zip(df['category'], df['sensitivity'])]\n",
    "    )\n",
    "\n",
    "    print(f\"Vector catalogue built with {collection.count()} assets\")\n",
    "    return collection, model\n",
    "\n",
    "collection, model = build_vector_catalogue(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(collection, queries, n_results=5):\n",
    "    \"\"\"Perform semantic searches against the vector catalogue.\"\"\"\n",
    "    for query in queries:\n",
    "        results = collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=n_results\n",
    "        )\n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        print(\"-\" * 60)\n",
    "        for i, (doc, dist, meta) in enumerate(zip(\n",
    "            results['documents'][0],\n",
    "            results['distances'][0],\n",
    "            results['metadatas'][0]\n",
    "        )):\n",
    "            print(f\"  {i+1}. [{meta['category']:12}] {doc[:70]}... (dist: {dist:.3f})\")\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"customer financial transactions\",\n",
    "    \"employee personal information\",\n",
    "    \"software development metrics\",\n",
    "]\n",
    "\n",
    "semantic_search(collection, test_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Profile** synthetic data asset catalogues and identify quality issues\n",
    "2. **Classify** data assets using TF-IDF + RandomForest text classification\n",
    "3. **Extract** business metadata from descriptions using regex patterns\n",
    "4. **Cluster** data assets with KMeans to discover natural groupings\n",
    "5. **Build** a vector catalogue with semantic search using ChromaDB\n",
    "\n",
    "---\n",
    "\n",
    "*Data Discovery: Harnessing AI, AGI & Vector Databases | AI Elevate*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
