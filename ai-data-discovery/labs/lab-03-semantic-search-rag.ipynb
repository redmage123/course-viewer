{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Semantic Search & Data Catalogue RAG\n",
    "\n",
    "**Data Discovery: Harnessing AI, AGI & Vector Databases - Day 2**\n",
    "\n",
    "| Duration | Difficulty | Framework | Exercises |\n",
    "|---|---|---|---|\n",
    "| 90 min | Intermediate | pandas, sentence-transformers, chromadb, rank_bm25, scikit-learn | 6 |\n",
    "\n",
    "In this lab, you'll practice:\n",
    "- Comparing embedding models for domain-specific retrieval\n",
    "- Building a BM25 keyword search index\n",
    "- Combining keyword and semantic search with hybrid fusion\n",
    "- Evaluating search quality with precision@k\n",
    "- Building a RAG pipeline over a data catalogue\n",
    "- Expanding queries with embedding similarity\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qp0n2qotpb9",
   "source": "## Student Notes & Background\n\n### Why Semantic Search Matters for Data Discovery\n\nTraditional data catalogues rely on **keyword matching** — if you search for \"salary data,\" you only find assets whose metadata literally contains the word \"salary.\" But what about assets described as \"compensation benchmarking\" or \"payroll deductions\"? These are clearly relevant, yet a keyword search misses them entirely.\n\n**Semantic search** solves this by converting text into dense numerical vectors (embeddings) that capture *meaning*, not just surface words. Two descriptions that are conceptually similar will have vectors that are close together in embedding space, even if they share no words in common.\n\n### Key Concepts\n\n#### 1. Embedding Models\nAn **embedding model** (e.g., `all-MiniLM-L6-v2` from Sentence-Transformers) takes a piece of text and maps it to a fixed-length vector, typically 384 or 768 dimensions. These models are pre-trained on large text corpora and fine-tuned so that semantically similar texts produce similar vectors. Different models have different strengths — some excel at short queries, others at long documents, and some are tuned for specific domains.\n\n**Cosine similarity** is the standard metric for comparing embeddings. It measures the angle between two vectors:\n- **1.0** = identical direction (maximum similarity)\n- **0.0** = orthogonal (no similarity)\n- **-1.0** = opposite direction (though rare with modern embeddings)\n\n#### 2. BM25 Keyword Search\n**BM25 (Best Matching 25)** is a classical information retrieval algorithm that improves on simple TF-IDF. It scores documents by:\n- **Term Frequency (TF):** How often the query term appears in the document (with diminishing returns)\n- **Inverse Document Frequency (IDF):** Terms that appear in fewer documents are weighted higher\n- **Document length normalisation:** Longer documents are penalised slightly to avoid bias\n\nBM25 excels at finding exact keyword matches and is extremely fast, but it cannot understand synonyms, paraphrases, or conceptual relationships.\n\n#### 3. Hybrid Search (Fusion)\nNeither keyword nor semantic search is universally superior. **Hybrid search** combines both using weighted fusion:\n\n```\nfinal_score = α × BM25_normalised + (1 - α) × cosine_similarity\n```\n\n- **α = 0.0:** Pure semantic search\n- **α = 0.5:** Equal weight (good default)\n- **α = 1.0:** Pure keyword search\n\nThe optimal α depends on your data and use case. In practice, hybrid search consistently outperforms either strategy alone.\n\n#### 4. Precision@k\n**Precision@k** measures what fraction of the top-k results are truly relevant:\n\n```\nPrecision@k = (number of relevant results in top k) / k\n```\n\nFor example, if you search for \"employee data\" and 3 of your top-5 results are from the HR category, your Precision@5 = 3/5 = 0.60.\n\n#### 5. Retrieval-Augmented Generation (RAG)\n**RAG** is a pattern that combines retrieval with language model generation:\n1. **Retrieve** relevant documents using search (keyword, semantic, or hybrid)\n2. **Augment** a prompt with the retrieved context\n3. **Generate** an answer grounded in the retrieved facts\n\nIn this lab, we simulate the generation step with structured extraction since we don't have a live LLM, but the retrieval pipeline is identical to what you'd use in production.\n\n#### 6. Query Expansion\n**Query expansion** improves recall by adding semantically related terms to the original query before searching. For example, expanding \"salary\" might add \"compensation,\" \"payroll,\" \"remuneration,\" and \"wages.\" This helps bridge vocabulary gaps between the user's query and the catalogue descriptions.\n\n### What You'll Build\n\nIn this lab, you will:\n1. **Compare** two embedding models (`all-MiniLM-L6-v2` vs `all-mpnet-base-v2`) by measuring how well each clusters descriptions from the same department\n2. **Build** a BM25 keyword search index and test it with domain queries\n3. **Implement** hybrid search that fuses BM25 and semantic scores with a tuneable α parameter\n4. **Evaluate** search quality using Precision@k against known ground-truth category labels\n5. **Build** a RAG-style pipeline that retrieves relevant catalogue entries and produces structured answers\n6. **Implement** query expansion using embedding similarity over the corpus vocabulary\n\n### Prerequisites\n- Familiarity with pandas DataFrames and numpy arrays\n- Basic understanding of cosine similarity (from Lab 1)\n- Concepts from Lab 1: TF-IDF vectorisation, ChromaDB basics\n\n### Tips\n- The synthetic data uses a fixed random seed (`np.random.seed(42)`), so your results should be reproducible\n- When comparing search strategies, pay attention to which categories each strategy retrieves — this reveals their strengths and weaknesses\n- The α parameter in hybrid search is powerful — experiment with different values to build intuition\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Embeddings & vector DB\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Keyword search\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Synthetic Data Catalogue\n",
    "\n",
    "We'll reuse the same 500-asset catalogue from Lab 1 so results are directly comparable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "categories = ['HR', 'Finance', 'Marketing', 'Engineering', 'Legal']\n",
    "sources = ['PostgreSQL', 'S3 Bucket', 'SharePoint', 'Salesforce', 'MongoDB']\n",
    "data_types = ['Table', 'Document', 'Spreadsheet', 'Log File', 'Report']\n",
    "sensitivity_levels = ['Public', 'Internal', 'Confidential', 'Restricted']\n",
    "\n",
    "descriptions_pool = {\n",
    "    'HR': [\n",
    "        'Employee personal records including name address and date of birth',\n",
    "        'Annual performance review scores and manager feedback',\n",
    "        'Payroll data with salary deductions and tax withholdings',\n",
    "        'Recruitment pipeline tracking applicant status and interview notes',\n",
    "        'Benefits enrollment records for health dental and vision plans',\n",
    "        'Employee onboarding documentation and training completion',\n",
    "        'Workforce diversity and inclusion metrics by department',\n",
    "        'Time and attendance records with overtime calculations',\n",
    "        'Employee termination records and exit interview summaries',\n",
    "        'Compensation benchmarking data across industry roles',\n",
    "    ],\n",
    "    'Finance': [\n",
    "        'Quarterly revenue reports broken down by business unit',\n",
    "        'Accounts payable invoices and payment processing records',\n",
    "        'Annual budget forecasts with departmental allocations',\n",
    "        'Customer billing records including credit card transactions',\n",
    "        'Expense reimbursement claims with receipt attachments',\n",
    "        'General ledger entries and journal adjustments',\n",
    "        'Tax filing documents and regulatory compliance records',\n",
    "        'Cash flow projections and working capital analysis',\n",
    "        'Vendor payment terms and contract financial summaries',\n",
    "        'Audit trail logs for financial transaction approvals',\n",
    "    ],\n",
    "    'Marketing': [\n",
    "        'Campaign performance metrics including click rates and conversions',\n",
    "        'Customer segmentation profiles based on purchase behaviour',\n",
    "        'Social media analytics with engagement and reach data',\n",
    "        'Email marketing subscriber lists with opt-in preferences',\n",
    "        'Brand sentiment analysis from customer reviews and surveys',\n",
    "        'Website traffic analytics and user journey tracking',\n",
    "        'Lead scoring models and marketing qualified lead reports',\n",
    "        'Content calendar and editorial planning documents',\n",
    "        'Competitive intelligence reports and market research data',\n",
    "        'Event registration lists with attendee contact information',\n",
    "    ],\n",
    "    'Engineering': [\n",
    "        'Application server logs with error traces and stack dumps',\n",
    "        'CI/CD pipeline metrics including build times and failure rates',\n",
    "        'Infrastructure monitoring data from cloud resources',\n",
    "        'API usage statistics and rate limiting configurations',\n",
    "        'Database schema documentation and migration scripts',\n",
    "        'Code repository commit history and pull request reviews',\n",
    "        'Load testing results and performance benchmarks',\n",
    "        'Security vulnerability scan reports and remediation tracking',\n",
    "        'Microservice dependency maps and architecture diagrams',\n",
    "        'Incident response logs and post-mortem analysis documents',\n",
    "    ],\n",
    "    'Legal': [\n",
    "        'Active contract repository with vendor agreements and SLAs',\n",
    "        'Intellectual property filings including patents and trademarks',\n",
    "        'Regulatory compliance audit findings and remediation plans',\n",
    "        'Data processing agreements under GDPR Article 28',\n",
    "        'Litigation case files and legal correspondence records',\n",
    "        'Corporate governance meeting minutes and board resolutions',\n",
    "        'Privacy impact assessments for new data processing activities',\n",
    "        'Non-disclosure agreement tracking and expiration dates',\n",
    "        'Employment law compliance documentation by jurisdiction',\n",
    "        'Insurance policy records and claims history',\n",
    "    ],\n",
    "}\n",
    "\n",
    "n_assets = 500\n",
    "records = []\n",
    "\n",
    "for i in range(n_assets):\n",
    "    cat = np.random.choice(categories)\n",
    "    desc = np.random.choice(descriptions_pool[cat])\n",
    "    if np.random.random() < 0.3:\n",
    "        desc += ' updated ' + np.random.choice(['weekly', 'monthly', 'quarterly', 'annually'])\n",
    "    records.append({\n",
    "        'asset_id': f'ASSET-{i+1:04d}',\n",
    "        'name': f'{cat.lower()}_{np.random.choice([\"report\", \"dataset\", \"log\", \"file\", \"table\"])}_{i+1:04d}',\n",
    "        'description': desc,\n",
    "        'category': cat,\n",
    "        'source': np.random.choice(sources),\n",
    "        'data_type': np.random.choice(data_types),\n",
    "        'sensitivity': np.random.choice(sensitivity_levels, p=[0.15, 0.35, 0.30, 0.20]),\n",
    "        'owner': np.random.choice(['alice', 'bob', 'carol', 'dave', 'eve', None], p=[0.2, 0.2, 0.2, 0.2, 0.15, 0.05]),\n",
    "        'row_count': np.random.randint(100, 1_000_000) if np.random.random() > 0.2 else None,\n",
    "        'last_updated': pd.Timestamp('2023-01-01') + pd.Timedelta(days=int(np.random.randint(0, 730))),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(f\"Generated {len(df)} data asset records\")\n",
    "df.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Multi-Strategy Embeddings\n",
    "\n",
    "Different embedding models capture different aspects of text similarity. Compare two models on our data catalogue descriptions to see which produces better intra-category clustering.\n",
    "\n",
    "**Your Task:** Load two SentenceTransformer models, encode all descriptions, and compare their intra-class similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compare_embedding_models(df, model_names=['all-MiniLM-L6-v2', 'all-mpnet-base-v2']):\n",
    "    \"\"\"Compare embedding models by measuring intra-category cosine similarity.\n",
    "    \n",
    "    Steps:\n",
    "    1. Load each SentenceTransformer model\n",
    "    2. Encode all descriptions\n",
    "    3. For each category, compute the mean cosine similarity between all pairs\n",
    "    4. Plot a grouped bar chart comparing models across categories\n",
    "    \n",
    "    Returns: dict of {model_name: {category: mean_similarity}}\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "results = compare_embedding_models(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2: BM25 Keyword Search\n",
    "\n",
    "BM25 is a classical information retrieval algorithm that ranks documents by term frequency and inverse document frequency. Build a BM25 index over the data catalogue.\n",
    "\n",
    "**Your Task:** Tokenize descriptions, build a BM25 index, and implement a search function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_bm25_index(df):\n",
    "    \"\"\"Build a BM25 index over data asset descriptions.\n",
    "    \n",
    "    Steps:\n",
    "    1. Tokenize each description (lowercase, split on whitespace)\n",
    "    2. Build BM25Okapi index from tokenized corpus\n",
    "    \n",
    "    Returns: (bm25, tokenized_corpus)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def bm25_search(query, bm25, df, top_k=5):\n",
    "    \"\"\"Search the BM25 index and return top-k results.\n",
    "    \n",
    "    Steps:\n",
    "    1. Tokenize the query\n",
    "    2. Get BM25 scores for all documents\n",
    "    3. Return top-k results with asset_id, description, category, and score\n",
    "    \n",
    "    Returns: list of (asset_id, description, category, score) tuples\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "bm25_result = build_bm25_index(df)\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"employee salary payroll\",\n",
    "    \"customer billing credit card\",\n",
    "    \"server logs monitoring\",\n",
    "]\n",
    "\n",
    "if bm25_result:\n",
    "    bm25, tokenized_corpus = bm25_result\n",
    "    for q in test_queries:\n",
    "        print(f\"\\nQuery: '{q}'\")\n",
    "        results = bm25_search(q, bm25, df)\n",
    "        if results:\n",
    "            for asset_id, desc, cat, score in results:\n",
    "                print(f\"  [{cat}] {asset_id}: {desc[:80]}... (score: {score:.3f})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Hybrid Search\n",
    "\n",
    "Combine BM25 keyword scores with semantic cosine similarity using weighted fusion. This hybrid approach captures both exact keyword matches and semantic meaning.\n",
    "\n",
    "**Your Task:** Implement a hybrid search function that fuses BM25 and semantic scores."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_semantic_index(df, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"Build a semantic search index.\n",
    "    \n",
    "    Steps:\n",
    "    1. Load SentenceTransformer model\n",
    "    2. Encode all descriptions\n",
    "    \n",
    "    Returns: (model, embeddings)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def hybrid_search(query, bm25, model, embeddings, df, alpha=0.5, top_k=5):\n",
    "    \"\"\"Perform hybrid search combining BM25 and semantic similarity.\n",
    "    \n",
    "    Steps:\n",
    "    1. Get BM25 scores and normalize to [0, 1]\n",
    "    2. Get cosine similarity scores between query embedding and all doc embeddings\n",
    "    3. Combine: final_score = alpha * bm25_norm + (1 - alpha) * cosine_sim\n",
    "    4. Return top-k results\n",
    "    \n",
    "    Args:\n",
    "        alpha: Weight for BM25 scores (0 = pure semantic, 1 = pure keyword)\n",
    "    \n",
    "    Returns: list of (asset_id, description, category, score) tuples\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "semantic_result = build_semantic_index(df)\n",
    "\n",
    "# Test hybrid search with different alpha values\n",
    "if bm25_result and semantic_result:\n",
    "    model, embeddings = semantic_result\n",
    "    query = \"employee compensation and benefits\"\n",
    "    \n",
    "    for alpha in [0.0, 0.3, 0.5, 0.7, 1.0]:\n",
    "        print(f\"\\n--- alpha={alpha} ({'pure semantic' if alpha == 0 else 'pure BM25' if alpha == 1 else 'hybrid'}) ---\")\n",
    "        results = hybrid_search(query, bm25, model, embeddings, df, alpha=alpha)\n",
    "        if results:\n",
    "            for asset_id, desc, cat, score in results:\n",
    "                print(f\"  [{cat}] {desc[:70]}... (score: {score:.3f})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Search Quality Evaluation\n",
    "\n",
    "Measure search quality using precision@k with known category relevance as ground truth.\n",
    "\n",
    "**Your Task:** Define ground truth mappings, compute precision@k for each search strategy, and compare them."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_search_quality(bm25, model, embeddings, df, top_k=5):\n",
    "    \"\"\"Evaluate BM25, semantic, and hybrid search using precision@k.\n",
    "    \n",
    "    Ground truth queries (query -> expected category):\n",
    "    - 'employee salary payroll tax' -> HR\n",
    "    - 'revenue budget financial reports' -> Finance\n",
    "    - 'campaign marketing customer engagement' -> Marketing\n",
    "    - 'server logs CI/CD pipeline deployment' -> Engineering\n",
    "    - 'contract compliance legal agreement' -> Legal\n",
    "    \n",
    "    Steps:\n",
    "    1. For each query, run BM25, semantic, and hybrid search\n",
    "    2. Compute precision@k = (# results in expected category) / k\n",
    "    3. Plot grouped bar chart comparing strategies\n",
    "    \n",
    "    Returns: dict of {strategy: {query: precision}}\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "if bm25_result and semantic_result:\n",
    "    eval_results = evaluate_search_quality(bm25, model, embeddings, df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1: Build a Data Catalogue RAG Pipeline\n",
    "\n",
    "Use retrieved context from hybrid search to answer natural language questions about the data catalogue. Since we don't have a live LLM, we'll build a structured extraction pipeline that summarizes the retrieved assets.\n",
    "\n",
    "**Your Task:** Implement a RAG-style query function that retrieves relevant assets and produces a structured answer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def rag_query(question, bm25, model, embeddings, df, top_k=5):\n",
    "    \"\"\"Answer a natural language question using retrieved data catalogue context.\n",
    "    \n",
    "    Steps:\n",
    "    1. Run hybrid_search to retrieve top-k relevant assets\n",
    "    2. Extract structured information from results:\n",
    "       - Categories represented\n",
    "       - Sensitivity levels\n",
    "       - Data sources\n",
    "       - Owners\n",
    "    3. Format a structured answer summarizing findings\n",
    "    \n",
    "    Returns: formatted string answer\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test RAG queries\n",
    "test_questions = [\n",
    "    \"What employee data assets contain personal information?\",\n",
    "    \"Which financial assets involve customer payment data?\",\n",
    "    \"What engineering assets are available for monitoring?\",\n",
    "    \"Are there any legal compliance documents in the catalogue?\",\n",
    "]\n",
    "\n",
    "if bm25_result and semantic_result:\n",
    "    for q in test_questions:\n",
    "        print(f\"\\nQ: {q}\")\n",
    "        answer = rag_query(q, bm25, model, embeddings, df)\n",
    "        if answer:\n",
    "            print(answer)\n",
    "        print(\"-\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2: Query Expansion\n",
    "\n",
    "Improve retrieval by expanding the original query with semantically related terms from the corpus vocabulary.\n",
    "\n",
    "**Your Task:** Implement query expansion using embedding similarity and show improved results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def expand_query(query, model, corpus_terms, term_embeddings, n_expand=5):\n",
    "    \"\"\"Expand a query with semantically related terms from the corpus.\n",
    "    \n",
    "    Steps:\n",
    "    1. Encode the query\n",
    "    2. Compute cosine similarity between query and all corpus term embeddings\n",
    "    3. Select top-n most similar terms not already in the query\n",
    "    4. Append to the original query\n",
    "    \n",
    "    Returns: expanded query string\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def build_term_index(df, model):\n",
    "    \"\"\"Build an index of unique terms and their embeddings.\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract all unique words from descriptions (lowercase, len > 3)\n",
    "    2. Encode each term with the model\n",
    "    \n",
    "    Returns: (terms_list, term_embeddings)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Build term index and test expansion\n",
    "if semantic_result:\n",
    "    term_result = build_term_index(df, model)\n",
    "    if term_result:\n",
    "        corpus_terms, term_embeddings = term_result\n",
    "        \n",
    "        test_queries = [\"salary\", \"server monitoring\", \"contract\"]\n",
    "        for q in test_queries:\n",
    "            expanded = expand_query(q, model, corpus_terms, term_embeddings)\n",
    "            print(f\"Original:  '{q}'\")\n",
    "            print(f\"Expanded:  '{expanded}'\")\n",
    "            print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Compare** embedding models to find the best fit for your domain\n",
    "2. **Build** a BM25 keyword search index for exact term matching\n",
    "3. **Combine** keyword and semantic search with weighted hybrid fusion\n",
    "4. **Evaluate** search quality using precision@k against ground truth\n",
    "5. **Build** a RAG pipeline that retrieves context to answer catalogue questions\n",
    "6. **Expand** queries with semantically related terms for better recall\n",
    "\n",
    "---\n",
    "\n",
    "*Data Discovery: Harnessing AI, AGI & Vector Databases | AI Elevate*"
   ]
  }
 ]
}