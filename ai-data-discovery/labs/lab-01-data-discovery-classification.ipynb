{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Data Discovery & Classification\n",
    "\n",
    "**Data Discovery: Harnessing AI, AGI & Vector Databases - Day 1**\n",
    "\n",
    "| Duration | Difficulty | Framework | Exercises |\n",
    "|---|---|---|---|\n",
    "| 90 min | Beginner | pandas, scikit-learn, sentence-transformers, chromadb | 5 |\n",
    "\n",
    "In this lab, you'll practice:\n",
    "- Profiling synthetic data assets and identifying quality issues\n",
    "- Building a text classifier with TF-IDF and RandomForest\n",
    "- Extracting metadata using regex patterns\n",
    "- Discovering data clusters with KMeans and PCA\n",
    "- Building a vector catalogue with semantic search\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Vector database\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Synthetic Data Assets\n",
    "\n",
    "We'll create a synthetic catalogue of ~500 data assets representing a typical enterprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "categories = ['HR', 'Finance', 'Marketing', 'Engineering', 'Legal']\n",
    "sources = ['PostgreSQL', 'S3 Bucket', 'SharePoint', 'Salesforce', 'MongoDB']\n",
    "data_types = ['Table', 'Document', 'Spreadsheet', 'Log File', 'Report']\n",
    "sensitivity_levels = ['Public', 'Internal', 'Confidential', 'Restricted']\n",
    "\n",
    "descriptions_pool = {\n",
    "    'HR': [\n",
    "        'Employee personal records including name address and date of birth',\n",
    "        'Annual performance review scores and manager feedback',\n",
    "        'Payroll data with salary deductions and tax withholdings',\n",
    "        'Recruitment pipeline tracking applicant status and interview notes',\n",
    "        'Benefits enrollment records for health dental and vision plans',\n",
    "        'Employee onboarding documentation and training completion',\n",
    "        'Workforce diversity and inclusion metrics by department',\n",
    "        'Time and attendance records with overtime calculations',\n",
    "        'Employee termination records and exit interview summaries',\n",
    "        'Compensation benchmarking data across industry roles',\n",
    "    ],\n",
    "    'Finance': [\n",
    "        'Quarterly revenue reports broken down by business unit',\n",
    "        'Accounts payable invoices and payment processing records',\n",
    "        'Annual budget forecasts with departmental allocations',\n",
    "        'Customer billing records including credit card transactions',\n",
    "        'Expense reimbursement claims with receipt attachments',\n",
    "        'General ledger entries and journal adjustments',\n",
    "        'Tax filing documents and regulatory compliance records',\n",
    "        'Cash flow projections and working capital analysis',\n",
    "        'Vendor payment terms and contract financial summaries',\n",
    "        'Audit trail logs for financial transaction approvals',\n",
    "    ],\n",
    "    'Marketing': [\n",
    "        'Campaign performance metrics including click rates and conversions',\n",
    "        'Customer segmentation profiles based on purchase behaviour',\n",
    "        'Social media analytics with engagement and reach data',\n",
    "        'Email marketing subscriber lists with opt-in preferences',\n",
    "        'Brand sentiment analysis from customer reviews and surveys',\n",
    "        'Website traffic analytics and user journey tracking',\n",
    "        'Lead scoring models and marketing qualified lead reports',\n",
    "        'Content calendar and editorial planning documents',\n",
    "        'Competitive intelligence reports and market research data',\n",
    "        'Event registration lists with attendee contact information',\n",
    "    ],\n",
    "    'Engineering': [\n",
    "        'Application server logs with error traces and stack dumps',\n",
    "        'CI/CD pipeline metrics including build times and failure rates',\n",
    "        'Infrastructure monitoring data from cloud resources',\n",
    "        'API usage statistics and rate limiting configurations',\n",
    "        'Database schema documentation and migration scripts',\n",
    "        'Code repository commit history and pull request reviews',\n",
    "        'Load testing results and performance benchmarks',\n",
    "        'Security vulnerability scan reports and remediation tracking',\n",
    "        'Microservice dependency maps and architecture diagrams',\n",
    "        'Incident response logs and post-mortem analysis documents',\n",
    "    ],\n",
    "    'Legal': [\n",
    "        'Active contract repository with vendor agreements and SLAs',\n",
    "        'Intellectual property filings including patents and trademarks',\n",
    "        'Regulatory compliance audit findings and remediation plans',\n",
    "        'Data processing agreements under GDPR Article 28',\n",
    "        'Litigation case files and legal correspondence records',\n",
    "        'Corporate governance meeting minutes and board resolutions',\n",
    "        'Privacy impact assessments for new data processing activities',\n",
    "        'Non-disclosure agreement tracking and expiration dates',\n",
    "        'Employment law compliance documentation by jurisdiction',\n",
    "        'Insurance policy records and claims history',\n",
    "    ],\n",
    "}\n",
    "\n",
    "n_assets = 500\n",
    "records = []\n",
    "\n",
    "for i in range(n_assets):\n",
    "    cat = np.random.choice(categories)\n",
    "    desc = np.random.choice(descriptions_pool[cat])\n",
    "    # Add slight variation\n",
    "    if np.random.random() < 0.3:\n",
    "        desc += ' updated ' + np.random.choice(['weekly', 'monthly', 'quarterly', 'annually'])\n",
    "    records.append({\n",
    "        'asset_id': f'ASSET-{i+1:04d}',\n",
    "        'name': f'{cat.lower()}_{np.random.choice([\"report\", \"dataset\", \"log\", \"file\", \"table\"])}_{i+1:04d}',\n",
    "        'description': desc,\n",
    "        'category': cat,\n",
    "        'source': np.random.choice(sources),\n",
    "        'data_type': np.random.choice(data_types),\n",
    "        'sensitivity': np.random.choice(sensitivity_levels, p=[0.15, 0.35, 0.30, 0.20]),\n",
    "        'owner': np.random.choice(['alice', 'bob', 'carol', 'dave', 'eve', None], p=[0.2, 0.2, 0.2, 0.2, 0.15, 0.05]),\n",
    "        'row_count': np.random.randint(100, 1_000_000) if np.random.random() > 0.2 else None,\n",
    "        'last_updated': pd.Timestamp('2023-01-01') + pd.Timedelta(days=int(np.random.randint(0, 730))),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(f\"Generated {len(df)} data asset records\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Data Profiling\n",
    "\n",
    "Explore the data asset catalogue. Compute summary statistics, counts by source/type/category, and identify missing values.\n",
    "\n",
    "**Your Task:** Profile the dataset to understand its structure and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print the shape and data types of the dataset\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Show value counts for category, source, and data_type columns\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check for missing values and compute the percentage missing per column\n",
    "# Hint: Use df.isnull().sum() and divide by len(df)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Show the distribution of sensitivity levels\n",
    "# Hint: Use df['sensitivity'].value_counts() and plot as a bar chart\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2: Text Classification with TF-IDF + RandomForest\n",
    "\n",
    "Build a classifier that predicts the category of a data asset based on its text description.\n",
    "\n",
    "**Your Task:** Implement the TF-IDF + RandomForest pipeline and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(df):\n",
    "    \"\"\"Build a TF-IDF + RandomForest text classifier.\n",
    "    \n",
    "    Steps:\n",
    "    1. Vectorise descriptions with TfidfVectorizer (max_features=1000, stop_words='english')\n",
    "    2. Train/test split (test_size=0.2, random_state=42)\n",
    "    3. Train RandomForestClassifier (n_estimators=100, random_state=42)\n",
    "    4. Print classification_report on test set\n",
    "    \n",
    "    Returns: (tfidf, clf, X_tfidf) tuple\n",
    "    \"\"\"\n",
    "    # TODO: Create TfidfVectorizer and fit_transform on descriptions\n",
    "    # TODO: Split into train/test\n",
    "    # TODO: Train RandomForestClassifier\n",
    "    # TODO: Print classification_report\n",
    "    # TODO: Return (tfidf, clf, X_tfidf)\n",
    "    pass\n",
    "\n",
    "result = build_classifier(df)\n",
    "if result:\n",
    "    tfidf, clf, X_tfidf = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Metadata Extraction\n",
    "\n",
    "Extract business terms from data asset descriptions using regex patterns.\n",
    "\n",
    "**Your Task:** Write a function that extracts key business terms (e.g., 'salary', 'revenue', 'customer', 'compliance') from descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_business_terms(text):\n",
    "    \"\"\"Extract business terms from a data asset description.\n",
    "    \n",
    "    Use regex to find known business terms in the text.\n",
    "    Terms to detect: salary, revenue, customer, employee, invoice,\n",
    "    compliance, contract, performance, billing, payroll, budget,\n",
    "    marketing, security, legal, audit\n",
    "    \n",
    "    Returns: list of matched terms (lowercase)\n",
    "    \"\"\"\n",
    "    # TODO: Define a list of business terms\n",
    "    # TODO: Use regex to find each term in the text (case-insensitive)\n",
    "    # TODO: Return list of found terms\n",
    "    pass\n",
    "\n",
    "# TODO: Apply to all descriptions and add as a new column 'business_terms'\n",
    "# TODO: Show the 10 most common business terms across all assets\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Unsupervised Discovery with Clustering\n",
    "\n",
    "Use KMeans clustering on TF-IDF vectors to discover natural groupings, then visualise with PCA.\n",
    "\n",
    "**Your Task:** Cluster the data assets and create a 2D scatter plot coloured by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_visualise(X_tfidf, df, n_clusters=5):\n",
    "    \"\"\"Cluster data assets using KMeans and visualise with PCA.\n",
    "    \n",
    "    Steps:\n",
    "    1. Fit KMeans with n_clusters on X_tfidf\n",
    "    2. Reduce to 2D with PCA\n",
    "    3. Create scatter plot coloured by cluster\n",
    "    4. Print top terms per cluster\n",
    "    \n",
    "    Returns: cluster labels array\n",
    "    \"\"\"\n",
    "    # TODO: Fit KMeans on X_tfidf\n",
    "    # TODO: Reduce to 2D with PCA\n",
    "    # TODO: Create scatter plot (figsize 12, 8)\n",
    "    # TODO: Return cluster labels\n",
    "    pass\n",
    "\n",
    "if result:\n",
    "    clusters = cluster_and_visualise(X_tfidf, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3: Vector Catalogue with Semantic Search\n",
    "\n",
    "Build a vector catalogue using SentenceTransformer embeddings and ChromaDB, then perform semantic searches.\n",
    "\n",
    "**Your Task:** Create embeddings, store in ChromaDB, and query with natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vector_catalogue(df):\n",
    "    \"\"\"Build a vector catalogue with ChromaDB.\n",
    "    \n",
    "    Steps:\n",
    "    1. Load SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    2. Encode all descriptions\n",
    "    3. Create a ChromaDB collection called 'data_catalogue'\n",
    "    4. Add embeddings with documents and IDs\n",
    "    \n",
    "    Returns: (collection, model) tuple\n",
    "    \"\"\"\n",
    "    # TODO: Load SentenceTransformer model\n",
    "    # TODO: Encode descriptions\n",
    "    # TODO: Create ChromaDB client and collection\n",
    "    # TODO: Add embeddings, documents, and IDs\n",
    "    # TODO: Return (collection, model)\n",
    "    pass\n",
    "\n",
    "catalogue_result = build_vector_catalogue(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(collection, queries, n_results=5):\n",
    "    \"\"\"Perform semantic searches against the vector catalogue.\n",
    "    \n",
    "    For each query string, retrieve the top n_results matches\n",
    "    and print them with their distances.\n",
    "    \"\"\"\n",
    "    # TODO: For each query, call collection.query()\n",
    "    # TODO: Print the query and top results with distances\n",
    "    pass\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"customer financial transactions\",\n",
    "    \"employee personal information\",\n",
    "    \"software development metrics\",\n",
    "]\n",
    "\n",
    "if catalogue_result:\n",
    "    collection, model = catalogue_result\n",
    "    semantic_search(collection, test_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Profile** synthetic data asset catalogues and identify quality issues\n",
    "2. **Classify** data assets using TF-IDF + RandomForest text classification\n",
    "3. **Extract** business metadata from descriptions using regex patterns\n",
    "4. **Cluster** data assets with KMeans to discover natural groupings\n",
    "5. **Build** a vector catalogue with semantic search using ChromaDB\n",
    "\n",
    "---\n",
    "\n",
    "*Data Discovery: Harnessing AI, AGI & Vector Databases | AI Elevate*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
