{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Sensitive Data Detection & AI Cataloguing - SOLUTIONS\n",
    "\n",
    "**Data Discovery: Harnessing AI, AGI & Vector Databases - Day 2**\n",
    "\n",
    "| Duration | Difficulty | Framework | Exercises |\n",
    "|---|---|---|---|\n",
    "| 90 min | Intermediate | pandas, re, spacy, scikit-learn, chromadb, matplotlib | 5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Student Notes & Background\n\n### Why Sensitive Data Detection Matters\n\nEvery organisation handles data that, if exposed, could harm individuals or violate regulations. **Personally Identifiable Information (PII)** — Social Security numbers, credit card numbers, email addresses, phone numbers, medical records — is scattered across documents, databases, and emails, often without anyone knowing exactly where it lives.\n\nSensitive data detection is the process of **automatically scanning** data assets to find PII and other regulated content. This is a critical building block for:\n- **GDPR compliance** — knowing where EU personal data resides\n- **HIPAA compliance** — protecting patient health information\n- **PCI-DSS compliance** — securing credit card data\n- **CCPA compliance** — enabling data subject access and deletion requests\n\nManual detection doesn't scale. An enterprise with thousands of documents needs automated scanning that combines **pattern matching** (regex) with **AI-based entity recognition** (NER) for comprehensive coverage.\n\n### Key Concepts\n\n#### 1. Regex-Based PII Scanning\n**Regular expressions** are the first line of defence for PII detection. Common patterns include:\n\n| PII Type | Pattern | Example |\n|---|---|---|\n| **SSN** | `\\d{3}-\\d{2}-\\d{4}` | 123-45-6789 |\n| **Credit Card** | `\\d{4}-\\d{4}-\\d{4}-\\d{4}` | 4532-1234-5678-9012 |\n| **Email** | `[\\w.+-]+@[\\w-]+\\.[\\w.]+` | john.smith@company.com |\n| **Phone** | `\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}` | (555) 123-4567 |\n\n**Strengths:** Fast, deterministic, high precision for well-formatted data.\n**Weaknesses:** Cannot detect names, organisations, or locations. Misses non-standard formats. Produces false positives on data that matches the pattern but isn't PII (e.g., a product ID that looks like an SSN).\n\n#### 2. Named Entity Recognition (NER) with spaCy\n**NER** uses a trained neural network to identify and classify entities in text. spaCy's `en_core_web_sm` model recognises:\n\n| Entity Type | Description | Examples |\n|---|---|---|\n| **PERSON** | Named individuals | \"John Smith\", \"Dr. Garcia\" |\n| **ORG** | Organisations | \"Acme Corp\", \"MedPlus Health\" |\n| **GPE** | Geopolitical entities (cities, countries) | \"New York\", \"Seattle\" |\n\nNER complements regex by detecting PII types that have no fixed format — you can't write a regex for every possible person's name. The combination of regex + NER is called **hybrid detection** and achieves much higher recall than either method alone.\n\n#### 3. Risk Scoring\nA **risk score** (0–100) quantifies how sensitive a document is based on the types and volume of PII found. Typical scoring weights reflect the severity of potential harm:\n\n| Factor | Points | Rationale |\n|---|---|---|\n| SSN found | +30 | Direct identity theft risk |\n| Credit card found | +25 | Financial fraud risk |\n| Email found | +10 | Phishing/spam risk |\n| Phone found | +10 | Social engineering risk |\n| PERSON entities | +5 each (max 15) | Identity linkage risk |\n| Medical document | +15 | HIPAA regulatory exposure |\n| Financial document | +10 | PCI-DSS/SOX exposure |\n\nScores are capped at 100 and mapped to **risk tiers**: Critical (76–100), High (51–75), Medium (26–50), Low (0–25). Risk tiers drive prioritisation — Critical assets get remediated first.\n\n#### 4. Compliance Dashboards\nA **compliance dashboard** visualises the PII landscape across your document corpus. Effective dashboards answer four questions at a glance:\n1. **What PII do we have?** — Distribution of PII types (SSN, credit card, email, etc.)\n2. **How risky is our data?** — Risk tier breakdown (Critical/High/Medium/Low)\n3. **Which departments are most exposed?** — Average risk by document type\n4. **Which regulations apply?** — Count of documents subject to GDPR, HIPAA, PCI-DSS, CCPA\n\n#### 5. Vector Catalogue with Risk Metadata\nBuilding on Lab 1's vector catalogue, this lab adds **risk metadata** to each document's embedding entry. This enables **filtered semantic search** — for example, \"find employee personal data\" filtered to only Critical-risk documents. ChromaDB supports `where` clauses that filter on metadata fields before computing similarity, making these queries efficient even on large catalogues.\n\n### What You'll Build\n\nIn this lab, you will:\n1. **Scan** 200 synthetic enterprise documents for PII using regex patterns for SSNs, credit cards, emails, and phone numbers\n2. **Extract** named entities (PERSON, ORG, GPE) using spaCy NER and combine with regex findings for hybrid detection\n3. **Compute** a 0–100 risk score for each document based on PII types, volume, and document category\n4. **Build** a 2×2 compliance dashboard showing PII distribution, risk tiers, department risk, and regulatory exposure\n5. **Create** a ChromaDB vector catalogue with risk metadata and perform filtered semantic queries\n\n### Prerequisites\n- Completion of Lab 1 (or familiarity with pandas, TF-IDF, and ChromaDB basics)\n- Understanding of regular expressions (basic pattern matching)\n- No prior NLP experience required — spaCy usage is introduced step by step\n\n### Tips\n- When writing regex patterns, use `\\b` word boundaries to avoid matching substrings (e.g., matching \"123-45-6789\" inside a longer number)\n- spaCy's small model (`en_core_web_sm`) is fast but less accurate than larger models — expect some missed entities\n- The risk scoring formula is deliberately simple; in production you'd weight factors based on your organisation's specific regulatory exposure\n- For the compliance dashboard, use `plt.suptitle()` for an overall title and `plt.tight_layout()` to prevent label overlap\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "\n",
    "# ML & Vector DB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "first_names = ['John', 'Jane', 'Robert', 'Maria', 'David', 'Sarah', 'Michael', 'Emily', 'James', 'Lisa']\n",
    "last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis', 'Rodriguez', 'Wilson']\n",
    "companies = ['Acme Corp', 'GlobalTech', 'MedPlus Health', 'FinanceFirst', 'DataDriven Inc']\n",
    "cities = ['New York', 'San Francisco', 'Chicago', 'Boston', 'Seattle', 'Austin', 'Denver', 'Atlanta']\n",
    "\n",
    "def random_ssn():\n",
    "    return f\"{np.random.randint(100,999)}-{np.random.randint(10,99)}-{np.random.randint(1000,9999)}\"\n",
    "\n",
    "def random_cc():\n",
    "    return f\"{np.random.randint(4000,4999)}-{np.random.randint(1000,9999)}-{np.random.randint(1000,9999)}-{np.random.randint(1000,9999)}\"\n",
    "\n",
    "def random_email(first, last):\n",
    "    domains = ['company.com', 'email.org', 'corp.net', 'enterprise.io']\n",
    "    return f\"{first.lower()}.{last.lower()}@{np.random.choice(domains)}\"\n",
    "\n",
    "def random_phone():\n",
    "    return f\"({np.random.randint(200,999)}) {np.random.randint(200,999)}-{np.random.randint(1000,9999)}\"\n",
    "\n",
    "templates = {\n",
    "    'hr_memo': [\n",
    "        \"Employee {name} (SSN: {ssn}) has been promoted to Senior Analyst effective March 2024. Contact: {email}, Phone: {phone}. Based in {city}.\",\n",
    "        \"Termination notice for {name}, SSN: {ssn}. Final paycheck to be sent to address on file. HR contact: {email}. Processed by {company}.\",\n",
    "        \"{name} from {company} submitted a leave request. Employee ID: EMP-{emp_id}. Emergency contact phone: {phone}. Location: {city}.\",\n",
    "        \"Salary adjustment memo: {name} (SSN: {ssn}) annual compensation increased to ${salary:,}. Effective date: January 2024. Department: {company}.\",\n",
    "    ],\n",
    "    'financial_report': [\n",
    "        \"Invoice #INV-{inv_id} for {company}: Payment of ${amount:,.2f} via credit card {cc}. Approved by {name}. Contact: {email}.\",\n",
    "        \"Expense report submitted by {name} ({email}) for ${amount:,.2f}. Corporate card ending {cc_last4}. Reimbursement approved by finance team at {company}.\",\n",
    "        \"Quarterly financial summary for {company}: Revenue ${amount:,.2f}. Prepared by {name}, CFO. Confidential. Phone: {phone}.\",\n",
    "        \"Wire transfer confirmation: ${amount:,.2f} sent to account ending {acct_last4} for {name} at {company}. Reference: TXN-{txn_id}.\",\n",
    "    ],\n",
    "    'medical_form': [\n",
    "        \"Patient: {name}, DOB: {dob}, SSN: {ssn}. Diagnosis: Type 2 Diabetes. Prescribed Metformin 500mg. Dr. {doctor} at {city} Medical Center.\",\n",
    "        \"Insurance claim for {name} (Member ID: MED-{med_id}). Procedure: Annual physical exam. Provider: {company} Health. Phone: {phone}.\",\n",
    "        \"Medical records request for {name}, DOB: {dob}. Records to be sent to {doctor} at {city} General Hospital. Patient email: {email}.\",\n",
    "    ],\n",
    "    'marketing_data': [\n",
    "        \"Campaign analytics report for {company}: {impressions:,} impressions, {clicks:,} clicks, {conversions} conversions. Manager: {name}, {email}.\",\n",
    "        \"Customer profile: {name}, {city}. Purchase history includes {purchases} orders. Email: {email}. Phone: {phone}. Loyalty tier: Gold.\",\n",
    "        \"Event registration: {name} from {company} registered for AI Summit 2024 in {city}. Contact: {email}. Dietary: vegetarian.\",\n",
    "    ],\n",
    "    'legal_document': [\n",
    "        \"Non-disclosure agreement between {name} and {company}. Effective date: January 2024. Jurisdiction: {city}. Contact: {email}.\",\n",
    "        \"Data processing agreement: {company} processes personal data of EU residents per GDPR Art. 28. DPO: {name}, {email}, {phone}.\",\n",
    "        \"Contract #CTR-{ctr_id} between {name} and {company}. Value: ${amount:,.2f}. Signed in {city}. Witness: {witness}.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "documents = []\n",
    "for i in range(200):\n",
    "    doc_type = np.random.choice(list(templates.keys()))\n",
    "    template = np.random.choice(templates[doc_type])\n",
    "    first = np.random.choice(first_names)\n",
    "    last = np.random.choice(last_names)\n",
    "    name = f\"{first} {last}\"\n",
    "    \n",
    "    doc_text = template.format(\n",
    "        name=name,\n",
    "        ssn=random_ssn(),\n",
    "        cc=random_cc(),\n",
    "        cc_last4=f\"{np.random.randint(1000,9999)}\",\n",
    "        email=random_email(first, last),\n",
    "        phone=random_phone(),\n",
    "        city=np.random.choice(cities),\n",
    "        company=np.random.choice(companies),\n",
    "        salary=np.random.randint(50000, 200000),\n",
    "        amount=np.random.uniform(100, 500000),\n",
    "        emp_id=np.random.randint(10000, 99999),\n",
    "        inv_id=np.random.randint(10000, 99999),\n",
    "        txn_id=np.random.randint(100000, 999999),\n",
    "        acct_last4=f\"{np.random.randint(1000,9999)}\",\n",
    "        med_id=np.random.randint(100000, 999999),\n",
    "        dob=f\"{np.random.randint(1,12):02d}/{np.random.randint(1,28):02d}/{np.random.randint(1950,2000)}\",\n",
    "        doctor=f\"Dr. {np.random.choice(last_names)}\",\n",
    "        impressions=np.random.randint(10000, 1000000),\n",
    "        clicks=np.random.randint(100, 50000),\n",
    "        conversions=np.random.randint(10, 1000),\n",
    "        purchases=np.random.randint(1, 50),\n",
    "        ctr_id=np.random.randint(10000, 99999),\n",
    "        witness=f\"{np.random.choice(first_names)} {np.random.choice(last_names)}\",\n",
    "    )\n",
    "    \n",
    "    documents.append({\n",
    "        'doc_id': f'DOC-{i+1:04d}',\n",
    "        'doc_type': doc_type,\n",
    "        'text': doc_text,\n",
    "        'department': doc_type.replace('_', ' ').title().split()[0],\n",
    "    })\n",
    "\n",
    "docs_df = pd.DataFrame(documents)\n",
    "print(f\"Generated {len(docs_df)} documents\")\n",
    "print(f\"\\nDocument type distribution:\")\n",
    "print(docs_df['doc_type'].value_counts())\n",
    "print(f\"\\nSample document:\")\n",
    "print(docs_df.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Regex PII Scanning - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_pii_regex(text):\n",
    "    \"\"\"Scan text for PII using regex patterns.\"\"\"\n",
    "    patterns = {\n",
    "        'ssn': re.compile(r'\\b\\d{3}-\\d{2}-\\d{4}\\b'),\n",
    "        'credit_card': re.compile(r'\\b\\d{4}-\\d{4}-\\d{4}-\\d{4}\\b'),\n",
    "        'email': re.compile(r'\\b[\\w.+-]+@[\\w-]+\\.[\\w.]+\\b'),\n",
    "        'phone': re.compile(r'\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}'),\n",
    "    }\n",
    "    findings = {}\n",
    "    for name, pat in patterns.items():\n",
    "        matches = pat.findall(text)\n",
    "        if matches:\n",
    "            findings[name] = matches\n",
    "    return findings\n",
    "\n",
    "# Apply to all documents\n",
    "docs_df['pii_findings'] = docs_df['text'].apply(scan_pii_regex)\n",
    "docs_df['ssn_count'] = docs_df['pii_findings'].apply(lambda x: len(x.get('ssn', [])))\n",
    "docs_df['cc_count'] = docs_df['pii_findings'].apply(lambda x: len(x.get('credit_card', [])))\n",
    "docs_df['email_count'] = docs_df['pii_findings'].apply(lambda x: len(x.get('email', [])))\n",
    "docs_df['phone_count'] = docs_df['pii_findings'].apply(lambda x: len(x.get('phone', [])))\n",
    "docs_df['total_pii'] = docs_df['ssn_count'] + docs_df['cc_count'] + docs_df['email_count'] + docs_df['phone_count']\n",
    "\n",
    "print(\"PII Detection Summary:\")\n",
    "print(f\"  Documents with SSN:         {(docs_df['ssn_count'] > 0).sum()}\")\n",
    "print(f\"  Documents with Credit Card: {(docs_df['cc_count'] > 0).sum()}\")\n",
    "print(f\"  Documents with Email:       {(docs_df['email_count'] > 0).sum()}\")\n",
    "print(f\"  Documents with Phone:       {(docs_df['phone_count'] > 0).sum()}\")\n",
    "print(f\"  Documents with any PII:     {(docs_df['total_pii'] > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2: NER with spaCy - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ner_entities(text):\n",
    "    \"\"\"Extract named entities using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = {'PERSON': [], 'ORG': [], 'GPE': []}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in entities:\n",
    "            entities[ent.label_].append(ent.text)\n",
    "    return entities\n",
    "\n",
    "# Apply to all documents\n",
    "docs_df['ner_entities'] = docs_df['text'].apply(extract_ner_entities)\n",
    "docs_df['person_count'] = docs_df['ner_entities'].apply(lambda x: len(x['PERSON']))\n",
    "docs_df['org_count'] = docs_df['ner_entities'].apply(lambda x: len(x['ORG']))\n",
    "docs_df['gpe_count'] = docs_df['ner_entities'].apply(lambda x: len(x['GPE']))\n",
    "\n",
    "# Combined PII types count\n",
    "def count_pii_types(row):\n",
    "    types = 0\n",
    "    if row['ssn_count'] > 0: types += 1\n",
    "    if row['cc_count'] > 0: types += 1\n",
    "    if row['email_count'] > 0: types += 1\n",
    "    if row['phone_count'] > 0: types += 1\n",
    "    if row['person_count'] > 0: types += 1\n",
    "    if row['org_count'] > 0: types += 1\n",
    "    if row['gpe_count'] > 0: types += 1\n",
    "    return types\n",
    "\n",
    "docs_df['total_pii_types'] = docs_df.apply(count_pii_types, axis=1)\n",
    "\n",
    "print(\"NER Detection Summary:\")\n",
    "print(f\"  Documents with PERSON:  {(docs_df['person_count'] > 0).sum()}\")\n",
    "print(f\"  Documents with ORG:     {(docs_df['org_count'] > 0).sum()}\")\n",
    "print(f\"  Documents with GPE:     {(docs_df['gpe_count'] > 0).sum()}\")\n",
    "print(f\"\\nHybrid Detection (regex + NER):\")\n",
    "print(f\"  Avg PII types per doc:  {docs_df['total_pii_types'].mean():.1f}\")\n",
    "print(f\"  Max PII types in a doc: {docs_df['total_pii_types'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Risk Scoring - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_risk_score(row):\n",
    "    \"\"\"Compute a 0-100 risk score for a document.\"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # PII type scores\n",
    "    if row['ssn_count'] > 0:\n",
    "        score += 30\n",
    "    if row['cc_count'] > 0:\n",
    "        score += 25\n",
    "    if row['email_count'] > 0:\n",
    "        score += 10\n",
    "    if row['phone_count'] > 0:\n",
    "        score += 10\n",
    "    \n",
    "    # NER entity scores\n",
    "    score += min(row['person_count'] * 5, 15)\n",
    "    \n",
    "    # Document type bonus\n",
    "    if row['doc_type'] == 'medical_form':\n",
    "        score += 15\n",
    "    elif row['doc_type'] == 'financial_report':\n",
    "        score += 10\n",
    "    \n",
    "    return min(score, 100)\n",
    "\n",
    "# Apply risk scoring\n",
    "docs_df['risk_score'] = docs_df.apply(compute_risk_score, axis=1)\n",
    "\n",
    "# Assign risk tiers\n",
    "def assign_tier(score):\n",
    "    if score >= 76:\n",
    "        return 'Critical'\n",
    "    elif score >= 51:\n",
    "        return 'High'\n",
    "    elif score >= 26:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "docs_df['risk_tier'] = docs_df['risk_score'].apply(assign_tier)\n",
    "\n",
    "print(\"Risk Tier Distribution:\")\n",
    "print(docs_df['risk_tier'].value_counts())\n",
    "print(f\"\\nAverage risk score: {docs_df['risk_score'].mean():.1f}\")\n",
    "print(f\"Median risk score:  {docs_df['risk_score'].median():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Compliance Dashboard - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_compliance_dashboard(docs_df):\n",
    "    \"\"\"Build a 2x2 compliance dashboard.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    # 1. PII type distribution\n",
    "    pii_counts = {\n",
    "        'SSN': (docs_df['ssn_count'] > 0).sum(),\n",
    "        'Credit Card': (docs_df['cc_count'] > 0).sum(),\n",
    "        'Email': (docs_df['email_count'] > 0).sum(),\n",
    "        'Phone': (docs_df['phone_count'] > 0).sum(),\n",
    "        'Person (NER)': (docs_df['person_count'] > 0).sum(),\n",
    "        'Org (NER)': (docs_df['org_count'] > 0).sum(),\n",
    "        'Location (NER)': (docs_df['gpe_count'] > 0).sum(),\n",
    "    }\n",
    "    colors = ['#ef4444', '#f59e0b', '#3b82f6', '#10b981', '#8b5cf6', '#ec4899', '#06b6d4']\n",
    "    axes[0, 0].barh(list(pii_counts.keys()), list(pii_counts.values()), color=colors)\n",
    "    axes[0, 0].set_title('PII Type Distribution (docs containing each type)', fontsize=12)\n",
    "    axes[0, 0].set_xlabel('Number of Documents')\n",
    "\n",
    "    # 2. Risk tier distribution\n",
    "    tier_counts = docs_df['risk_tier'].value_counts()\n",
    "    tier_order = ['Critical', 'High', 'Medium', 'Low']\n",
    "    tier_colors = ['#ef4444', '#f59e0b', '#3b82f6', '#10b981']\n",
    "    tier_vals = [tier_counts.get(t, 0) for t in tier_order]\n",
    "    axes[0, 1].pie(tier_vals, labels=tier_order, colors=tier_colors,\n",
    "                   autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 1].set_title('Risk Tier Distribution', fontsize=12)\n",
    "\n",
    "    # 3. Average risk by doc type\n",
    "    avg_risk = docs_df.groupby('doc_type')['risk_score'].mean().sort_values()\n",
    "    avg_risk.plot(kind='barh', ax=axes[1, 0], color='#8b5cf6')\n",
    "    axes[1, 0].set_title('Average Risk Score by Document Type', fontsize=12)\n",
    "    axes[1, 0].set_xlabel('Average Risk Score')\n",
    "\n",
    "    # 4. Regulation applicability\n",
    "    regulations = {\n",
    "        'GDPR': (docs_df['person_count'] > 0).sum(),\n",
    "        'HIPAA': (docs_df['doc_type'] == 'medical_form').sum(),\n",
    "        'PCI-DSS': (docs_df['cc_count'] > 0).sum(),\n",
    "        'CCPA': ((docs_df['email_count'] > 0) & (docs_df['phone_count'] > 0)).sum(),\n",
    "    }\n",
    "    axes[1, 1].bar(regulations.keys(), regulations.values(),\n",
    "                   color=['#3b82f6', '#10b981', '#f59e0b', '#ef4444'])\n",
    "    axes[1, 1].set_title('Documents Subject to Regulation', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Number of Documents')\n",
    "\n",
    "    plt.suptitle('Compliance Dashboard', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "build_compliance_dashboard(docs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3: Catalogue Integration - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_risk_catalogue(docs_df):\n",
    "    \"\"\"Build a vector catalogue with risk metadata.\"\"\"\n",
    "    # Load model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Encode texts\n",
    "    texts = docs_df['text'].tolist()\n",
    "    embeddings = model.encode(texts)\n",
    "\n",
    "    # Create ChromaDB collection\n",
    "    client = chromadb.Client()\n",
    "    collection = client.create_collection(\"risk_catalogue\")\n",
    "\n",
    "    # Add with metadata\n",
    "    collection.add(\n",
    "        embeddings=embeddings.tolist(),\n",
    "        documents=texts,\n",
    "        ids=docs_df['doc_id'].tolist(),\n",
    "        metadatas=[\n",
    "            {\n",
    "                'doc_type': row['doc_type'],\n",
    "                'risk_score': int(row['risk_score']),\n",
    "                'risk_tier': row['risk_tier'],\n",
    "            }\n",
    "            for _, row in docs_df.iterrows()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(f\"Risk catalogue built with {collection.count()} documents\")\n",
    "    return collection, model\n",
    "\n",
    "collection, model = build_risk_catalogue(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_risk_search(collection, query, risk_tier=None, n_results=5):\n",
    "    \"\"\"Search the risk catalogue with optional risk tier filter.\"\"\"\n",
    "    where_clause = None\n",
    "    if risk_tier:\n",
    "        where_clause = {'risk_tier': risk_tier}\n",
    "\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        where=where_clause\n",
    "    )\n",
    "\n",
    "    filter_str = f\" [filtered: {risk_tier}]\" if risk_tier else \"\"\n",
    "    print(f\"\\nQuery: '{query}'{filter_str}\")\n",
    "    print(\"-\" * 70)\n",
    "    for i, (doc, dist, meta) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['distances'][0],\n",
    "        results['metadatas'][0]\n",
    "    )):\n",
    "        print(f\"  {i+1}. [{meta['risk_tier']:8} | {meta['doc_type']:18} | risk:{meta['risk_score']:3}]\")\n",
    "        print(f\"     {doc[:80]}...\")\n",
    "        print(f\"     (distance: {dist:.3f})\")\n",
    "\n",
    "# Test queries\n",
    "filtered_risk_search(collection, \"employee personal data\", risk_tier=\"Critical\")\n",
    "filtered_risk_search(collection, \"financial transactions and payments\")\n",
    "filtered_risk_search(collection, \"medical patient records\", risk_tier=\"Critical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Scan** documents for PII using regex patterns with precision/recall awareness\n",
    "2. **Extract** named entities with spaCy NER for hybrid PII detection\n",
    "3. **Score** data assets for compliance risk on a 0-100 scale\n",
    "4. **Visualise** compliance posture with a multi-chart dashboard\n",
    "5. **Integrate** risk metadata into a vector catalogue for filtered semantic search\n",
    "\n",
    "---\n",
    "\n",
    "*Data Discovery: Harnessing AI, AGI & Vector Databases | AI Elevate*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}