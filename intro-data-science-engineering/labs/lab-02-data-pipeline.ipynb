{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Lab 2: Building a Data Pipeline\n",
    "\n",
    "**Introduction to Data Science & Engineering - Day 2**\n",
    "\n",
    "| Duration | Difficulty | Framework | Exercises |\n",
    "|---|---|---|---|\n",
    "| 120 min | Intermediate | pandas, sqlite3 | 6 |\n",
    "\n",
    "In this lab, you'll practice:\n",
    "\n",
    "- Extracting data from multiple sources\n",
    "- Implementing data quality assertions\n",
    "- Building ETL transformations\n",
    "- Modeling data into a star schema\n",
    "- Loading into SQLite and Parquet\n",
    "- Running analytical SQL queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "## Part 1: Extract — Loading Data from Multiple Sources\n",
    "\n",
    "In real pipelines, data comes from many sources. We'll simulate three: CSV, JSON, and a Python dictionary (representing an API response)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Create and Extract Source Data\n",
    "\n",
    "Run the four cells below to generate the source data you will work with throughout this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 1: Orders data (simulating CSV)\n",
    "np.random.seed(42)\n",
    "n_orders = 1500\n",
    "\n",
    "start_date = datetime(2024, 1, 1)\n",
    "order_dates = [start_date + timedelta(days=np.random.randint(0, 365)) for _ in range(n_orders)]\n",
    "\n",
    "orders_data = {\n",
    "    'order_id': range(1001, 1001 + n_orders),\n",
    "    'customer_id': np.random.randint(1, 201, n_orders),\n",
    "    'product_id': np.random.randint(1, 51, n_orders),\n",
    "    'order_date': order_dates,\n",
    "    'quantity': np.random.randint(1, 8, n_orders),\n",
    "    'unit_price': np.round(np.random.uniform(10, 300, n_orders), 2),\n",
    "    'discount_pct': np.random.choice([0, 5, 10, 15, 20], n_orders, p=[0.5, 0.2, 0.15, 0.1, 0.05]),\n",
    "    'store_id': np.random.randint(1, 6, n_orders)\n",
    "}\n",
    "\n",
    "orders_df = pd.DataFrame(orders_data)\n",
    "\n",
    "# Inject some quality issues\n",
    "orders_df.loc[np.random.choice(orders_df.index, 30, replace=False), 'unit_price'] = np.nan\n",
    "orders_df.loc[np.random.choice(orders_df.index, 10, replace=False), 'quantity'] = -1\n",
    "orders_df.loc[np.random.choice(orders_df.index, 5, replace=False), 'customer_id'] = 999  # Invalid customer\n",
    "\n",
    "print(f\"Orders extracted: {len(orders_df)} rows\")\n",
    "orders_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 2: Customer data (simulating JSON API response)\n",
    "customers_json = {\n",
    "    \"customers\": [\n",
    "        {\n",
    "            \"customer_id\": i,\n",
    "            \"name\": f\"Customer_{i:03d}\",\n",
    "            \"email\": f\"customer{i}@email.com\",\n",
    "            \"segment\": np.random.choice(['Premium', 'Standard', 'Basic']),\n",
    "            \"join_date\": (datetime(2020, 1, 1) + timedelta(days=np.random.randint(0, 1460))).strftime('%Y-%m-%d'),\n",
    "            \"city\": np.random.choice(['Dublin', 'Cork', 'Galway', 'Limerick', 'Waterford']),\n",
    "            \"country\": \"Ireland\"\n",
    "        }\n",
    "        for i in range(1, 201)\n",
    "    ]\n",
    "}\n",
    "\n",
    "customers_df = pd.DataFrame(customers_json['customers'])\n",
    "customers_df['join_date'] = pd.to_datetime(customers_df['join_date'])\n",
    "\n",
    "print(f\"Customers extracted: {len(customers_df)} rows\")\n",
    "customers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 3: Product catalog (simulating dict/API)\n",
    "categories = ['Electronics', 'Clothing', 'Home & Garden', 'Books', 'Sports']\n",
    "products = []\n",
    "for i in range(1, 51):\n",
    "    cat = categories[(i - 1) % len(categories)]\n",
    "    products.append({\n",
    "        'product_id': i,\n",
    "        'product_name': f'{cat}_Item_{i:03d}',\n",
    "        'category': cat,\n",
    "        'brand': np.random.choice(['BrandA', 'BrandB', 'BrandC', 'BrandD']),\n",
    "        'cost_price': round(np.random.uniform(5, 150), 2),\n",
    "        'weight_kg': round(np.random.uniform(0.1, 20), 1)\n",
    "    })\n",
    "\n",
    "products_df = pd.DataFrame(products)\n",
    "print(f\"Products extracted: {len(products_df)} rows\")\n",
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 4: Store reference data\n",
    "stores_data = {\n",
    "    'store_id': [1, 2, 3, 4, 5],\n",
    "    'store_name': ['Dublin Central', 'Cork Main', 'Galway West', 'Online', 'Limerick Hub'],\n",
    "    'store_type': ['Physical', 'Physical', 'Physical', 'Online', 'Physical'],\n",
    "    'region': ['East', 'South', 'West', 'National', 'South']\n",
    "}\n",
    "stores_df = pd.DataFrame(stores_data)\n",
    "print(f\"Stores extracted: {len(stores_df)} rows\")\n",
    "stores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3",
   "metadata": {},
   "source": [
    "## Part 2: Validate — Data Quality Checks\n",
    "\n",
    "Before transforming, validate source data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Implement Quality Assertions\n",
    "\n",
    "**Your Task:** Implement a reusable `run_quality_checks` function that takes a DataFrame, a list of check tuples `(name, function, is_critical)`, and a source name. It should print a formatted report and return `True` only if all critical checks pass. Then define checks for the orders data and run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_quality_checks(df, checks, source_name):\n",
    "    \"\"\"Run a list of quality checks and report results.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to check\n",
    "        checks: list of tuples (check_name, check_fn, is_critical)\n",
    "        source_name: name for the report header\n",
    "    \n",
    "    Returns: True if all critical checks pass\n",
    "    \"\"\"\n",
    "    # TODO: Print report header with source_name\n",
    "    # TODO: Loop through checks, call each check_fn(df)\n",
    "    # TODO: Print PASS/FAIL with checkmark/cross for each\n",
    "    # TODO: Track whether any critical check failed\n",
    "    # TODO: Return whether all critical checks passed\n",
    "    pass\n",
    "\n",
    "# TODO: Define order_checks as a list of tuples:\n",
    "#   (\"Not empty\", lambda df: len(df) > 0, True),\n",
    "#   (\"No null order_ids\", ..., True),\n",
    "#   (\"Unique order_ids\", ..., True),\n",
    "#   (\"No null prices\", ..., False),\n",
    "#   (\"Positive quantities\", ..., False),\n",
    "#   (\"Valid customer_ids (1-200)\", ..., False),\n",
    "#   (\"Valid dates\", ..., True),\n",
    "order_checks = None  # Your code here\n",
    "\n",
    "# orders_valid = run_quality_checks(orders_df, order_checks, \"Orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6_md",
   "metadata": {},
   "source": [
    "**Your Task:** Define quality checks for the customers DataFrame and run them. Check for: not empty, unique customer_ids, no null emails, and valid segments (Premium/Standard/Basic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define customer_checks and run quality checks on customers_df\n",
    "#   Checks: Not empty, Unique customer_ids, No null emails, Valid segments\n",
    "customer_checks = None  # Your code here\n",
    "\n",
    "# customers_valid = run_quality_checks(customers_df, customer_checks, \"Customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7",
   "metadata": {},
   "source": [
    "## Part 3: Transform — Clean and Enrich Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Clean Orders Data\n",
    "\n",
    "**Your Task:** Write a `clean_orders` function that handles missing prices (fill with median), fixes negative quantities (set to 1), and removes rows with invalid customer IDs (not in 1-200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_orders(orders_df):\n",
    "    \"\"\"Clean the orders data.\n",
    "    \n",
    "    Steps:\n",
    "    1. Fill missing prices with median\n",
    "    2. Fix negative quantities (set to 1)\n",
    "    3. Remove rows with invalid customer_ids (not in 1-200)\n",
    "    \n",
    "    Returns: cleaned DataFrame\n",
    "    \"\"\"\n",
    "    # TODO: Copy the dataframe\n",
    "    # TODO: Fill null unit_price with median\n",
    "    # TODO: Set quantity <= 0 to 1\n",
    "    # TODO: Filter to customer_id between 1 and 200\n",
    "    pass\n",
    "\n",
    "orders_clean = clean_orders(orders_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Enrich and Derive Fields\n",
    "\n",
    "**Your Task:** Write an `enrich_orders` function that adds revenue calculations (gross_amount, discount_amount, net_amount) and date dimension fields (year, month, quarter, day_of_week, is_weekend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_orders(orders_clean):\n",
    "    \"\"\"Add derived fields to orders data.\n",
    "    \n",
    "    Fields to create:\n",
    "    - gross_amount = quantity * unit_price\n",
    "    - discount_amount = gross_amount * discount_pct / 100\n",
    "    - net_amount = gross_amount - discount_amount\n",
    "    - order_year, order_month, order_quarter, order_day_of_week\n",
    "    - is_weekend (1 if Saturday/Sunday)\n",
    "    \n",
    "    Returns: enriched DataFrame\n",
    "    \"\"\"\n",
    "    # TODO: Ensure order_date is datetime\n",
    "    # TODO: Calculate revenue fields\n",
    "    # TODO: Extract date dimensions\n",
    "    # TODO: Add is_weekend flag\n",
    "    pass\n",
    "\n",
    "orders_clean = enrich_orders(orders_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2",
   "metadata": {},
   "source": [
    "## Part 4: Model — Star Schema Design\n",
    "\n",
    "Transform our flat data into a star schema with a fact table and dimension tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1d2e3",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Create Dimension Tables\n",
    "\n",
    "**Your Task:** Create four dimension tables: `dim_customers`, `dim_products`, `dim_time`, and `dim_stores`. Each should rename primary keys to `*_key` format and add any enrichment fields specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_customers(customers_df):\n",
    "    \"\"\"Create customer dimension table.\n",
    "    \n",
    "    - Rename customer_id to customer_key\n",
    "    - Add tenure_days and tenure_years (relative to 2024-12-31)\n",
    "    \n",
    "    Returns: dim_customers DataFrame\n",
    "    \"\"\"\n",
    "    # TODO: Copy and rename key column\n",
    "    # TODO: Calculate tenure\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_products(products_df):\n",
    "    \"\"\"Create product dimension table.\n",
    "    \n",
    "    - Rename product_id to product_key\n",
    "    - Add margin_tier using pd.cut (Low/Mid/High Cost)\n",
    "    \n",
    "    Returns: dim_products DataFrame\n",
    "    \"\"\"\n",
    "    # TODO: Copy and rename key column\n",
    "    # TODO: Add margin tier based on cost_price bins [0, 30, 80, 200]\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_time():\n",
    "    \"\"\"Create time dimension table for 2024.\n",
    "    \n",
    "    Columns: date_key, year, quarter, month, month_name, week,\n",
    "             day_of_week, day_name, is_weekend\n",
    "    \n",
    "    Returns: dim_time DataFrame\n",
    "    \"\"\"\n",
    "    # TODO: Generate date range for 2024\n",
    "    # TODO: Extract all date components\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_stores - provided (no changes needed)\n",
    "dim_stores = stores_df.copy()\n",
    "dim_stores = dim_stores.rename(columns={'store_id': 'store_key'})\n",
    "print(f\"dim_stores: {dim_stores.shape}\")\n",
    "dim_stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Create Fact Table\n",
    "\n",
    "**Your Task:** Build the `fact_orders` table by selecting the relevant columns and renaming foreign keys to match the dimension key names. Then validate referential integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fact_orders(orders_clean):\n",
    "    \"\"\"Create fact_orders table from cleaned orders.\n",
    "    \n",
    "    - Select relevant columns\n",
    "    - Rename foreign keys: customer_id->customer_key, etc.\n",
    "    \n",
    "    Returns: fact_orders DataFrame\n",
    "    \"\"\"\n",
    "    # TODO: Select columns (order_id, customer_id, product_id, order_date, store_id,\n",
    "    #        quantity, unit_price, discount_pct, gross_amount, discount_amount, net_amount)\n",
    "    # TODO: Rename to dimension keys\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_star_schema(fact_orders, dim_customers, dim_products, dim_stores):\n",
    "    \"\"\"Validate referential integrity of the star schema.\n",
    "    \n",
    "    Check that all foreign keys in fact_orders exist in dimension tables.\n",
    "    \"\"\"\n",
    "    # TODO: Check for orphan keys in each dimension\n",
    "    # TODO: Print validation results\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9f0a1",
   "metadata": {},
   "source": [
    "## Part 5: Load — Persist to SQLite and Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0a1b2",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Load into SQLite\n",
    "\n",
    "**Your Task:** Write a function that loads all five star schema tables into a SQLite database and verifies the row counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_sqlite(fact_orders, dim_customers, dim_products, dim_time, dim_stores, db_path='ecommerce_warehouse.db'):\n",
    "    \"\"\"Load star schema tables into a SQLite database.\n",
    "    \n",
    "    Returns: sqlite3 connection\n",
    "    \"\"\"\n",
    "    # TODO: Remove existing db file if present\n",
    "    # TODO: Connect to SQLite\n",
    "    # TODO: Write each table using to_sql (convert datetimes to strings first)\n",
    "    # TODO: Verify row counts\n",
    "    pass\n",
    "\n",
    "conn = load_to_sqlite(fact_orders, dim_customers, dim_products, dim_time, dim_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d5",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Load into Parquet\n",
    "\n",
    "**Your Task:** Write a function that saves all five star schema tables as Parquet files and prints the file sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_parquet(fact_orders, dim_customers, dim_products, dim_time, dim_stores, parquet_dir='warehouse_parquet'):\n",
    "    \"\"\"Save star schema tables as Parquet files.\n",
    "    \n",
    "    Print file sizes after saving.\n",
    "    \"\"\"\n",
    "    # TODO: Create directory\n",
    "    # TODO: Save each table as parquet\n",
    "    # TODO: Print file sizes\n",
    "    pass\n",
    "\n",
    "load_to_parquet(fact_orders, dim_customers, dim_products, dim_time, dim_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f7",
   "metadata": {},
   "source": [
    "## Part 6: Analyze — SQL Analytics\n",
    "\n",
    "Run analytical queries against our star schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a8",
   "metadata": {},
   "source": [
    "### Exercise 6.1: Revenue Analytics\n",
    "\n",
    "**Your Task:** Write three SQL queries against the star schema: monthly revenue trend, top 10 customers by revenue, and category performance by store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write SQL query to get monthly revenue trend\n",
    "# Join fact_orders with dim_time, group by month\n",
    "# Select: month_name, month, total_orders, total_revenue, avg_order_value\n",
    "query = \"\"\"\n",
    "-- Your SQL here\n",
    "\"\"\"\n",
    "# monthly_revenue = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write SQL query to get top 10 customers by revenue\n",
    "# Join fact_orders with dim_customers\n",
    "# Select: name, segment, city, orders, total_spent, avg_order\n",
    "query = \"\"\"\n",
    "-- Your SQL here\n",
    "\"\"\"\n",
    "# top_customers = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write SQL query for category performance by store\n",
    "# Join fact_orders with dim_stores and dim_products\n",
    "# Select: store_name, category, orders, revenue, avg_discount\n",
    "query = \"\"\"\n",
    "-- Your SQL here\n",
    "\"\"\"\n",
    "# store_category = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_md",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection and clean up\n",
    "conn.close()\n",
    "\n",
    "# Clean up temporary files\n",
    "import shutil\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "if os.path.exists(parquet_dir):\n",
    "    shutil.rmtree(parquet_dir)\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. **Extract** data from multiple source formats (CSV, JSON, dict)\n",
    "2. **Validate** data quality with assertion-based checks\n",
    "3. **Transform** data — cleaning, enrichment, derived fields\n",
    "4. **Model** data into a star schema (fact + dimension tables)\n",
    "5. **Load** into both SQLite (for SQL queries) and Parquet (for analytics)\n",
    "6. **Analyze** with SQL joins across the star schema\n",
    "\n",
    "---\n",
    "\n",
    "*Introduction to Data Science & Engineering | AI Elevate*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}