{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 3: Generative AI with Ollama\n",
    "\n",
    "**Duration:** 90-120 minutes | **Difficulty:** Intermediate to Advanced\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "1. Connect to and use Ollama for local LLM inference\n",
    "2. Generate text using the Llama model\n",
    "3. Apply prompt engineering techniques for better results\n",
    "4. Control generation with temperature and other parameters\n",
    "5. Build multi-turn conversations with chat history\n",
    "6. Implement Retrieval-Augmented Generation (RAG)\n",
    "7. Understand fine-tuning concepts with LoRA and QLoRA\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Ollama must be installed and running locally\n",
    "- The Llama 3.2 model should be pulled: `ollama pull llama3.2`\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Read each markdown cell carefully before running the code cell below it\n",
    "- For **Exercise** cells, replace `None` with your code\n",
    "- Use the **Demonstration** cells as examples to guide you\n",
    "- Run cells with `Shift+Enter`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cell below to import the required libraries and verify Ollama is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# Check connection to Ollama\n",
    "try:\n",
    "    models = ollama.list()\n",
    "    print(\"Connected to Ollama!\")\n",
    "    print(\"\\nAvailable models:\")\n",
    "    for model in models.get('models', []):\n",
    "        print(f\"  - {model['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama: {e}\")\n",
    "    print(\"\\nMake sure Ollama is running: ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Basic Text Generation\n",
    "\n",
    "Ollama provides a simple API for generating text with local LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1.1 Simple Generation - Demonstration\n",
    "\n",
    "Use `ollama.generate()` to generate text from a prompt:\n",
    "- `model` - The model name (e.g., 'llama3.2')\n",
    "- `prompt` - Your input text\n",
    "\n",
    "Run the cell below to see basic generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text generation\n",
    "response = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt='What is machine learning in one sentence?'\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 1.2 Understanding the Response - Demonstration\n",
    "\n",
    "The response object contains useful metadata:\n",
    "\n",
    "Run the cell below to explore the response structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt='Name three programming languages.'\n",
    ")\n",
    "\n",
    "print(\"Full response keys:\", list(response.keys()))\n",
    "print(\"\\n--- Key Information ---\")\n",
    "print(f\"Model: {response['model']}\")\n",
    "print(f\"Response text: {response['response'][:100]}...\")\n",
    "print(f\"Done: {response['done']}\")\n",
    "print(f\"Total duration: {response.get('total_duration', 0) / 1e9:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Generate Your First Response\n",
    "\n",
    "Use `ollama.generate()` to ask the model to explain what an API is.\n",
    "\n",
    "| Variable | What to do |\n",
    "|----------|------------|\n",
    "| `my_response` | Call `ollama.generate()` with model 'llama3.2' and a prompt asking \"What is an API?\" |\n",
    "| `answer` | Extract the response text from `my_response` |\n",
    "\n",
    "**Hint:** Look at the demonstration above for the syntax. The response text is in `response['response']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response asking about APIs\n",
    "my_response = None\n",
    "\n",
    "# Extract the text\n",
    "answer = None\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Prompt Engineering\n",
    "\n",
    "The way you write prompts significantly affects the quality of responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 2.1 Role-Based Prompts - Demonstration\n",
    "\n",
    "Giving the model a role or persona can improve responses:\n",
    "\n",
    "Run the cell below to see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without role\n",
    "basic_prompt = \"Explain recursion.\"\n",
    "\n",
    "# With role\n",
    "role_prompt = \"\"\"You are an experienced computer science teacher who explains concepts \n",
    "to beginners using simple analogies.\n",
    "\n",
    "Explain recursion.\"\"\"\n",
    "\n",
    "print(\"=== Basic Prompt ===\")\n",
    "response1 = ollama.generate(model='llama3.2', prompt=basic_prompt)\n",
    "print(response1['response'][:300])\n",
    "\n",
    "print(\"\\n=== With Role ===\")\n",
    "response2 = ollama.generate(model='llama3.2', prompt=role_prompt)\n",
    "print(response2['response'][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 2.2 Structured Output - Demonstration\n",
    "\n",
    "Ask for specific formats to get structured responses:\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_prompt = \"\"\"List 3 benefits of exercise.\n",
    "\n",
    "Format your response as a numbered list with exactly 3 items.\n",
    "Each item should be one sentence.\"\"\"\n",
    "\n",
    "response = ollama.generate(model='llama3.2', prompt=structured_prompt)\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 2.3 JSON Output - Demonstration\n",
    "\n",
    "You can request JSON-formatted responses for programmatic use:\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_prompt = \"\"\"Generate information about a fictional book.\n",
    "\n",
    "Respond with ONLY valid JSON in this exact format:\n",
    "{\"title\": \"...\", \"author\": \"...\", \"year\": YYYY, \"genre\": \"...\"}\n",
    "\n",
    "Do not include any other text, just the JSON.\"\"\"\n",
    "\n",
    "response = ollama.generate(model='llama3.2', prompt=json_prompt)\n",
    "json_text = response['response'].strip()\n",
    "print(\"Raw response:\")\n",
    "print(json_text)\n",
    "\n",
    "# Try to parse as JSON\n",
    "try:\n",
    "    book_data = json.loads(json_text)\n",
    "    print(\"\\nParsed successfully!\")\n",
    "    print(f\"Title: {book_data['title']}\")\n",
    "    print(f\"Author: {book_data['author']}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\nFailed to parse JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Write a Role-Based Prompt\n",
    "\n",
    "Create a prompt that asks the model to act as a helpful chef and suggest a simple dinner recipe.\n",
    "\n",
    "| Variable | What to do |\n",
    "|----------|------------|\n",
    "| `chef_prompt` | Write a prompt that gives the model a chef persona and asks for a simple dinner recipe |\n",
    "| `recipe_response` | Generate a response using your prompt |\n",
    "\n",
    "**Hint:** Start your prompt with something like \"You are a professional chef who...\" then ask your question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt with a chef persona\n",
    "chef_prompt = None\n",
    "\n",
    "# Generate the response\n",
    "recipe_response = None\n",
    "\n",
    "if recipe_response:\n",
    "    print(recipe_response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Generate JSON Output\n",
    "\n",
    "Create a prompt that generates a JSON object representing a movie.\n",
    "\n",
    "| Variable | What to do |\n",
    "|----------|------------|\n",
    "| `movie_prompt` | Write a prompt asking for movie info in JSON format with keys: title, director, year, rating |\n",
    "| `movie_response` | Generate the response |\n",
    "| `movie_data` | Parse the response as JSON using `json.loads()` |\n",
    "\n",
    "**Hint:** Be explicit about the JSON format you want. Tell the model to respond with ONLY the JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt for movie JSON\n",
    "movie_prompt = None\n",
    "\n",
    "# Generate response\n",
    "movie_response = None\n",
    "\n",
    "# Parse as JSON\n",
    "movie_data = None\n",
    "\n",
    "if movie_data:\n",
    "    print(\"Movie data:\")\n",
    "    for key, value in movie_data.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Generation Parameters\n",
    "\n",
    "Control the creativity and randomness of generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 3.1 Temperature - Demonstration\n",
    "\n",
    "Temperature controls randomness:\n",
    "- **Low (0.0-0.3)**: More focused, deterministic responses\n",
    "- **Medium (0.5-0.7)**: Balanced creativity\n",
    "- **High (0.8-1.0+)**: More creative, varied responses\n",
    "\n",
    "Run the cell below to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a one-sentence story about a robot.\"\n",
    "\n",
    "print(\"=== Low Temperature (0.1) - Focused ===\")\n",
    "for i in range(2):\n",
    "    response = ollama.generate(\n",
    "        model='llama3.2',\n",
    "        prompt=prompt,\n",
    "        options={'temperature': 0.1}\n",
    "    )\n",
    "    print(f\"{i+1}. {response['response'].strip()}\")\n",
    "\n",
    "print(\"\\n=== High Temperature (1.0) - Creative ===\")\n",
    "for i in range(2):\n",
    "    response = ollama.generate(\n",
    "        model='llama3.2',\n",
    "        prompt=prompt,\n",
    "        options={'temperature': 1.0}\n",
    "    )\n",
    "    print(f\"{i+1}. {response['response'].strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 3.2 Other Parameters - Demonstration\n",
    "\n",
    "Additional parameters you can control:\n",
    "- `top_p` - Nucleus sampling (0.0-1.0)\n",
    "- `top_k` - Limit vocabulary choices\n",
    "- `num_predict` - Maximum tokens to generate\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short response with num_predict\n",
    "response = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt='Tell me about Python programming.',\n",
    "    options={\n",
    "        'temperature': 0.7,\n",
    "        'num_predict': 50  # Limit to ~50 tokens\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Limited response (50 tokens max):\")\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Exercise 3.1: Experiment with Temperature\n",
    "\n",
    "Generate two responses to the same creative prompt with different temperatures.\n",
    "\n",
    "| Variable | What to do |\n",
    "|----------|------------|\n",
    "| `creative_prompt` | Write a prompt asking for a creative name for a coffee shop |\n",
    "| `low_temp_response` | Generate with temperature 0.2 |\n",
    "| `high_temp_response` | Generate with temperature 0.9 |\n",
    "\n",
    "**Hint:** Use the `options` parameter with a dictionary containing `'temperature'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your creative prompt\n",
    "creative_prompt = None\n",
    "\n",
    "# Low temperature (focused)\n",
    "low_temp_response = None\n",
    "\n",
    "# High temperature (creative)\n",
    "high_temp_response = None\n",
    "\n",
    "if low_temp_response and high_temp_response:\n",
    "    print(\"Low temp (0.2):\")\n",
    "    print(low_temp_response['response'])\n",
    "    print(\"\\nHigh temp (0.9):\")\n",
    "    print(high_temp_response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Chat Conversations\n",
    "\n",
    "The chat API maintains conversation context for multi-turn dialogues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 4.1 Basic Chat - Demonstration\n",
    "\n",
    "Use `ollama.chat()` with a list of messages:\n",
    "- Each message has a `role` ('user' or 'assistant') and `content`\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single message chat\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'What is the capital of France?'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Assistant:\", response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 4.2 Multi-Turn Conversation - Demonstration\n",
    "\n",
    "Include previous messages to maintain context:\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a conversation\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'My name is Alex.'},\n",
    "]\n",
    "\n",
    "# First response\n",
    "response1 = ollama.chat(model='llama3.2', messages=messages)\n",
    "print(\"User: My name is Alex.\")\n",
    "print(\"Assistant:\", response1['message']['content'])\n",
    "\n",
    "# Add assistant's response to history\n",
    "messages.append(response1['message'])\n",
    "\n",
    "# Follow-up question\n",
    "messages.append({'role': 'user', 'content': 'What is my name?'})\n",
    "\n",
    "response2 = ollama.chat(model='llama3.2', messages=messages)\n",
    "print(\"\\nUser: What is my name?\")\n",
    "print(\"Assistant:\", response2['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## 4.3 System Messages - Demonstration\n",
    "\n",
    "Use a 'system' role to set the assistant's behavior:\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a pirate. Always respond in pirate speak with lots of \"arr\" and nautical terms.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'How do I make a cup of coffee?'\n",
    "    }\n",
    "]\n",
    "\n",
    "response = ollama.chat(model='llama3.2', messages=messages)\n",
    "print(\"Pirate Assistant:\")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Create a Multi-Turn Conversation\n",
    "\n",
    "Build a conversation where you:\n",
    "1. Tell the assistant your favorite color\n",
    "2. Ask what your favorite color is\n",
    "\n",
    "| Variable | What to do |\n",
    "|----------|------------|\n",
    "| `messages` | Start with a message telling the assistant your favorite color |\n",
    "| `response1` | Get the first response using `ollama.chat()` |\n",
    "| `response2` | Add a follow-up asking what your favorite color is |\n",
    "\n",
    "**Hint:** After getting response1, append `response1['message']` to messages, then add your follow-up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation - tell the assistant your favorite color\n",
    "messages = None\n",
    "\n",
    "# Get first response\n",
    "response1 = None\n",
    "\n",
    "if response1:\n",
    "    print(\"Assistant:\", response1['message']['content'])\n",
    "    \n",
    "    # Add response to history and ask follow-up\n",
    "    # Your code here...\n",
    "    \n",
    "    response2 = None\n",
    "    \n",
    "    if response2:\n",
    "        print(\"\\nAssistant:\", response2['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Create a Specialized Chatbot\n",
    "\n",
    "Create a chatbot with a system prompt that makes it act as a helpful Python tutor.\n",
    "\n",
    "| Variable | What to do |\n",
    "|----------|------------|\n",
    "| `system_prompt` | Write a system message describing the Python tutor persona |\n",
    "| `messages` | Create messages list with system prompt and a user question about Python |\n",
    "| `tutor_response` | Get the response from the tutor |\n",
    "\n",
    "**Hint:** The system message should describe how the tutor should behave (helpful, explains concepts clearly, gives examples, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for a Python tutor\n",
    "system_prompt = None\n",
    "\n",
    "# Create messages with system prompt and a question\n",
    "messages = None\n",
    "\n",
    "# Get response\n",
    "tutor_response = None\n",
    "\n",
    "if tutor_response:\n",
    "    print(\"Python Tutor:\")\n",
    "    print(tutor_response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Building a Simple Application\n",
    "\n",
    "Combine what you've learned to build a useful application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "## 5.1 Text Summarizer - Demonstration\n",
    "\n",
    "Create a function that summarizes text:\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, max_sentences=2):\n",
    "    \"\"\"Summarize text using the LLM.\"\"\"\n",
    "    prompt = f\"\"\"Summarize the following text in exactly {max_sentences} sentences.\n",
    "Be concise and capture the main points.\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    response = ollama.generate(\n",
    "        model='llama3.2',\n",
    "        prompt=prompt,\n",
    "        options={'temperature': 0.3}\n",
    "    )\n",
    "    return response['response'].strip()\n",
    "\n",
    "# Test it\n",
    "long_text = \"\"\"\n",
    "Artificial intelligence (AI) is transforming industries across the globe. \n",
    "From healthcare to finance, AI systems are being deployed to automate tasks, \n",
    "analyze data, and make predictions. Machine learning, a subset of AI, enables \n",
    "computers to learn from data without being explicitly programmed. Deep learning, \n",
    "which uses neural networks with many layers, has achieved remarkable results in \n",
    "image recognition, natural language processing, and game playing. However, AI \n",
    "also raises important ethical considerations around bias, privacy, and job displacement.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize(long_text)\n",
    "print(\"Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "## Exercise 5.1: Build a Sentiment Analyzer\n",
    "\n",
    "Create a function that analyzes the sentiment of text and returns a structured result.\n",
    "\n",
    "| Function | What to do |\n",
    "|----------|------------|\n",
    "| `analyze_sentiment(text)` | Take text as input and return a dictionary with 'sentiment' (positive/negative/neutral) and 'confidence' (high/medium/low) |\n",
    "\n",
    "**Hint:** Use a prompt that asks for JSON output with the exact keys needed. Parse the response with `json.loads()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyze sentiment of text and return structured result.\"\"\"\n",
    "    # Create a prompt asking for JSON output\n",
    "    prompt = None  # Your prompt here\n",
    "    \n",
    "    # Generate response\n",
    "    response = None\n",
    "    \n",
    "    # Parse and return\n",
    "    return None\n",
    "\n",
    "# Test with different texts\n",
    "test_texts = [\n",
    "    \"I absolutely love this product! Best purchase ever!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"The weather today is cloudy.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = analyze_sentiment(text)\n",
    "    if result:\n",
    "        print(f\"Text: {text[:50]}...\")\n",
    "        print(f\"  Sentiment: {result.get('sentiment')}\")\n",
    "        print(f\"  Confidence: {result.get('confidence')}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "## Exercise 5.2: Build a Q&A Bot\n",
    "\n",
    "Create a simple Q&A function that answers questions based on provided context.\n",
    "\n",
    "| Function | What to do |\n",
    "|----------|------------|\n",
    "| `answer_question(context, question)` | Take context and question as input, return the answer based only on the context |\n",
    "\n",
    "**Hint:** Your prompt should instruct the model to answer based ONLY on the provided context, and to say \"I don't know\" if the answer isn't in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(context, question):\n",
    "    \"\"\"Answer a question based on the provided context.\"\"\"\n",
    "    prompt = None  # Your prompt here\n",
    "    \n",
    "    response = None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test context\n",
    "context = \"\"\"\n",
    "Python was created by Guido van Rossum and first released in 1991. \n",
    "It emphasizes code readability and uses significant indentation. \n",
    "Python supports multiple programming paradigms including procedural, \n",
    "object-oriented, and functional programming. The language is named \n",
    "after the British comedy group Monty Python.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Who created Python?\",\n",
    "    \"When was Python first released?\",\n",
    "    \"What is Python's mascot?\"  # Not in context\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    answer = answer_question(context, q)\n",
    "    if answer:\n",
    "        print(f\"Q: {q}\")\n",
    "        print(f\"A: {answer}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-46",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "RAG enhances LLM responses by retrieving relevant information from a knowledge base before generating answers. This allows the model to access up-to-date or domain-specific information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-47",
   "metadata": {},
   "source": [
    "## 6.1 Understanding RAG - Concept Overview\n",
    "\n",
    "RAG consists of three main steps:\n",
    "\n",
    "1. **Indexing**: Convert documents into embeddings (vector representations)\n",
    "2. **Retrieval**: Find the most relevant documents for a query\n",
    "3. **Generation**: Use retrieved context to generate an informed response\n",
    "\n",
    "```\n",
    "Query → Embed → Search Vector DB → Retrieve Top-K → Augment Prompt → Generate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-48",
   "metadata": {},
   "source": [
    "## 6.2 Creating Embeddings - Demonstration\n",
    "\n",
    "Ollama can generate embeddings for text. Embeddings are dense vector representations that capture semantic meaning.\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an embedding for a piece of text\n",
    "text = \"Machine learning is a subset of artificial intelligence.\"\n",
    "\n",
    "response = ollama.embed(\n",
    "    model='llama3.2',\n",
    "    input=text\n",
    ")\n",
    "\n",
    "embedding = response['embeddings'][0]\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-50",
   "metadata": {},
   "source": [
    "## 6.3 Semantic Similarity - Demonstration\n",
    "\n",
    "We can compare embeddings using cosine similarity to find related content:\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Get embedding for a text string.\"\"\"\n",
    "    response = ollama.embed(model='llama3.2', input=text)\n",
    "    return response['embeddings'][0]\n",
    "\n",
    "# Compare different texts\n",
    "texts = [\n",
    "    \"Python is a programming language.\",\n",
    "    \"Java is used for software development.\",\n",
    "    \"I love eating pizza for dinner.\"\n",
    "]\n",
    "\n",
    "query = \"What programming languages are popular?\"\n",
    "query_embedding = get_embedding(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Similarity scores:\")\n",
    "for text in texts:\n",
    "    text_embedding = get_embedding(text)\n",
    "    similarity = cosine_similarity(query_embedding, text_embedding)\n",
    "    print(f\"  {similarity:.4f} - {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-52",
   "metadata": {},
   "source": [
    "## 6.4 Building a Simple RAG System - Demonstration\n",
    "\n",
    "Let's build a complete RAG pipeline:\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"A simple RAG implementation using Ollama.\"\"\"\n",
    "    \n",
    "    def __init__(self, model='llama3.2'):\n",
    "        self.model = model\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def add_documents(self, docs: List[str]):\n",
    "        \"\"\"Add documents to the knowledge base.\"\"\"\n",
    "        for doc in docs:\n",
    "            embedding = get_embedding(doc)\n",
    "            self.documents.append(doc)\n",
    "            self.embeddings.append(embedding)\n",
    "        print(f\"Added {len(docs)} documents. Total: {len(self.documents)}\")\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 2) -> List[str]:\n",
    "        \"\"\"Retrieve most relevant documents for a query.\"\"\"\n",
    "        query_embedding = get_embedding(query)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for i, doc_embedding in enumerate(self.embeddings):\n",
    "            sim = cosine_similarity(query_embedding, doc_embedding)\n",
    "            similarities.append((sim, i))\n",
    "        \n",
    "        # Sort by similarity (descending) and get top-k\n",
    "        similarities.sort(reverse=True)\n",
    "        top_indices = [idx for _, idx in similarities[:top_k]]\n",
    "        \n",
    "        return [self.documents[i] for i in top_indices]\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 2) -> str:\n",
    "        \"\"\"Answer a question using RAG.\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = self.retrieve(question, top_k)\n",
    "        context = \"\\n\\n\".join(relevant_docs)\n",
    "        \n",
    "        # Generate response with context\n",
    "        prompt = f\"\"\"Use the following context to answer the question. \n",
    "If the answer is not in the context, say \"I don't have information about that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        response = ollama.generate(\n",
    "            model=self.model,\n",
    "            prompt=prompt,\n",
    "            options={'temperature': 0.3}\n",
    "        )\n",
    "        return response['response'].strip()\n",
    "\n",
    "# Create RAG system and add knowledge\n",
    "rag = SimpleRAG()\n",
    "\n",
    "knowledge_base = [\n",
    "    \"The Eiffel Tower is located in Paris, France. It was built in 1889 and stands 330 meters tall.\",\n",
    "    \"The Great Wall of China is over 21,000 kilometers long and was built over many centuries.\",\n",
    "    \"Python programming language was created by Guido van Rossum and released in 1991.\",\n",
    "    \"Machine learning is a subset of AI that enables computers to learn from data.\",\n",
    "    \"The Amazon rainforest produces about 20% of the world's oxygen.\"\n",
    "]\n",
    "\n",
    "rag.add_documents(knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG system\n",
    "questions = [\n",
    "    \"How tall is the Eiffel Tower?\",\n",
    "    \"Who created Python?\",\n",
    "    \"What is the population of Tokyo?\"  # Not in knowledge base\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    answer = rag.query(q)\n",
    "    print(f\"A: {answer}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-55",
   "metadata": {},
   "source": [
    "## Exercise 6.1: Extend the RAG Knowledge Base\n",
    "\n",
    "Add your own documents to the RAG system and query it.\n",
    "\n",
    "| Task | What to do |\n",
    "|------|------------|\n",
    "| `my_documents` | Create a list of 3-5 facts about a topic you choose (e.g., sports, history, science) |\n",
    "| Add documents | Use `rag.add_documents()` to add your documents |\n",
    "| Test queries | Ask questions that can be answered from your documents |\n",
    "\n",
    "**Hint:** Make sure your facts contain specific information that can be retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own knowledge documents\n",
    "my_documents = None  # Create a list of facts\n",
    "\n",
    "# Add to the RAG system\n",
    "if my_documents:\n",
    "    rag.add_documents(my_documents)\n",
    "\n",
    "# Test with your own questions\n",
    "my_questions = None  # Create a list of questions\n",
    "\n",
    "if my_questions:\n",
    "    for q in my_questions:\n",
    "        print(f\"Q: {q}\")\n",
    "        answer = rag.query(q)\n",
    "        print(f\"A: {answer}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-57",
   "metadata": {},
   "source": [
    "## Exercise 6.2: Implement Document Chunking\n",
    "\n",
    "Real RAG systems split long documents into chunks. Implement a chunking function.\n",
    "\n",
    "| Function | What to do |\n",
    "|----------|------------|\n",
    "| `chunk_text(text, chunk_size, overlap)` | Split text into overlapping chunks of approximately `chunk_size` words |\n",
    "\n",
    "**Hint:** Split by sentences first, then group sentences into chunks. Overlap helps maintain context between chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 100, overlap: int = 20) -> List[str]:\n",
    "    \"\"\"Split text into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to chunk\n",
    "        chunk_size: Target number of words per chunk\n",
    "        overlap: Number of words to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    return None\n",
    "\n",
    "# Test with a longer document\n",
    "long_document = \"\"\"\n",
    "Artificial intelligence has transformed the technology landscape dramatically over the past decade. \n",
    "Machine learning algorithms now power everything from recommendation systems to autonomous vehicles.\n",
    "Deep learning, a subset of machine learning, uses neural networks with many layers to learn complex patterns.\n",
    "Natural language processing enables computers to understand and generate human language.\n",
    "Computer vision allows machines to interpret and analyze visual information from the world.\n",
    "Reinforcement learning teaches agents to make decisions through trial and error.\n",
    "The field continues to advance rapidly, with new breakthroughs announced regularly.\n",
    "Ethical considerations around AI bias and fairness have become increasingly important.\n",
    "Researchers are working on making AI systems more transparent and explainable.\n",
    "The future of AI holds both tremendous promise and significant challenges for society.\n",
    "\"\"\"\n",
    "\n",
    "chunks = chunk_text(long_document, chunk_size=50, overlap=10)\n",
    "if chunks:\n",
    "    print(f\"Created {len(chunks)} chunks:\\n\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk[:80]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-59",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Fine-tuning Concepts with LoRA and QLoRA\n",
    "\n",
    "Fine-tuning adapts a pre-trained model to specific tasks or domains. LoRA and QLoRA make this process efficient and accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-60",
   "metadata": {},
   "source": [
    "## 7.1 Why Fine-tune?\n",
    "\n",
    "Pre-trained models like Llama are general-purpose. Fine-tuning helps when you need:\n",
    "\n",
    "- **Domain expertise**: Medical, legal, or technical knowledge\n",
    "- **Specific format**: Always output JSON, follow templates\n",
    "- **Personality/style**: Customer service tone, brand voice\n",
    "- **Task specialization**: Classification, extraction, summarization\n",
    "\n",
    "### Fine-tuning Approaches\n",
    "\n",
    "| Approach | Description | Memory | Quality |\n",
    "|----------|-------------|--------|----------|\n",
    "| Full fine-tuning | Update all parameters | Very High | Best |\n",
    "| LoRA | Low-rank adaptation matrices | Medium | Very Good |\n",
    "| QLoRA | Quantized LoRA | Low | Good |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-61",
   "metadata": {},
   "source": [
    "## 7.2 Understanding LoRA (Low-Rank Adaptation)\n",
    "\n",
    "LoRA adds small trainable matrices to frozen model weights:\n",
    "\n",
    "```\n",
    "Original: W (frozen)\n",
    "LoRA:     W + BA (where B and A are small trainable matrices)\n",
    "\n",
    "If W is 4096 x 4096 (16M params)\n",
    "And rank r = 8:\n",
    "  B is 4096 x 8 (32K params)\n",
    "  A is 8 x 4096 (32K params)\n",
    "  Total: 64K params (0.4% of original!)\n",
    "```\n",
    "\n",
    "### Key LoRA Parameters\n",
    "\n",
    "- **rank (r)**: Size of low-rank matrices (4-64 typical)\n",
    "- **alpha**: Scaling factor (often 2x rank)\n",
    "- **target_modules**: Which layers to adapt (attention, MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-62",
   "metadata": {},
   "source": [
    "## 7.3 QLoRA: Quantized LoRA\n",
    "\n",
    "QLoRA combines LoRA with 4-bit quantization:\n",
    "\n",
    "1. **4-bit NormalFloat (NF4)**: Optimal for normally distributed weights\n",
    "2. **Double quantization**: Quantize the quantization constants\n",
    "3. **Paged optimizers**: Handle memory spikes during training\n",
    "\n",
    "This enables fine-tuning a 7B model on a single consumer GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-63",
   "metadata": {},
   "source": [
    "## 7.4 Preparing Training Data - Demonstration\n",
    "\n",
    "Fine-tuning requires properly formatted training examples:\n",
    "\n",
    "Run the cell below to see training data formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training data formats\n",
    "\n",
    "# 1. Instruction format (most common)\n",
    "instruction_format = {\n",
    "    \"instruction\": \"Summarize the following text in one sentence.\",\n",
    "    \"input\": \"The quick brown fox jumps over the lazy dog. This sentence contains every letter of the alphabet.\",\n",
    "    \"output\": \"A pangram sentence featuring a fox jumping over a dog demonstrates all 26 alphabet letters.\"\n",
    "}\n",
    "\n",
    "# 2. Chat format\n",
    "chat_format = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How do I reverse a string in Python?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"You can reverse a string using slicing: `reversed_string = original_string[::-1]`\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 3. Completion format (simpler)\n",
    "completion_format = {\n",
    "    \"prompt\": \"Translate to French: Hello, how are you?\",\n",
    "    \"completion\": \"Bonjour, comment allez-vous?\"\n",
    "}\n",
    "\n",
    "print(\"Instruction Format:\")\n",
    "print(json.dumps(instruction_format, indent=2))\n",
    "print(\"\\nChat Format:\")\n",
    "print(json.dumps(chat_format, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-65",
   "metadata": {},
   "source": [
    "## 7.5 Creating a Training Dataset - Demonstration\n",
    "\n",
    "Let's create a small example dataset for fine-tuning a customer service bot:\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer service training examples\n",
    "training_data = [\n",
    "    {\n",
    "        \"instruction\": \"Respond to a customer inquiry about order status.\",\n",
    "        \"input\": \"Where is my order? I ordered 3 days ago.\",\n",
    "        \"output\": \"I understand you're eager to receive your order! Let me look that up for you. Could you please provide your order number? In the meantime, most orders ship within 2-3 business days and arrive within 5-7 days.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Respond to a customer inquiry about returns.\",\n",
    "        \"input\": \"I want to return this item, it doesn't fit.\",\n",
    "        \"output\": \"I'm sorry to hear the item doesn't fit! We offer free returns within 30 days of purchase. You can initiate a return through your account or I can help you start one right now. Would you like an exchange for a different size instead?\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Respond to a frustrated customer.\",\n",
    "        \"input\": \"This is ridiculous! Your product broke after one week!\",\n",
    "        \"output\": \"I sincerely apologize for this frustrating experience. A product failing after just one week is absolutely not acceptable, and I understand your disappointment. Let me make this right for you immediately. I can offer a full replacement or refund - which would you prefer?\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Respond to a customer asking about a discount.\",\n",
    "        \"input\": \"Do you have any discount codes available?\",\n",
    "        \"output\": \"Great question! While I don't have specific codes at the moment, I can share a few ways to save: Sign up for our newsletter for 10% off your first order, check our sale section for up to 50% off, and follow us on social media for exclusive deals!\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created {len(training_data)} training examples\")\n",
    "print(\"\\nExample:\")\n",
    "print(json.dumps(training_data[0], indent=2))\n",
    "\n",
    "# Save to JSONL format (common for fine-tuning)\n",
    "def save_jsonl(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    print(f\"\\nSaved to {filename}\")\n",
    "\n",
    "# Uncomment to save:\n",
    "# save_jsonl(training_data, 'customer_service_training.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-67",
   "metadata": {},
   "source": [
    "## 7.6 Fine-tuning Configuration - Demonstration\n",
    "\n",
    "Here's what a typical LoRA/QLoRA configuration looks like:\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Configuration (conceptual - actual implementation requires transformers/peft libraries)\n",
    "lora_config = {\n",
    "    \"r\": 16,                    # Rank of the update matrices\n",
    "    \"lora_alpha\": 32,           # Scaling factor\n",
    "    \"target_modules\": [         # Which layers to apply LoRA\n",
    "        \"q_proj\",               # Query projection in attention\n",
    "        \"k_proj\",               # Key projection\n",
    "        \"v_proj\",               # Value projection\n",
    "        \"o_proj\",               # Output projection\n",
    "    ],\n",
    "    \"lora_dropout\": 0.05,       # Dropout for regularization\n",
    "    \"bias\": \"none\",             # Don't train bias terms\n",
    "}\n",
    "\n",
    "# Training Configuration\n",
    "training_config = {\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"warmup_steps\": 100,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"max_seq_length\": 512,\n",
    "}\n",
    "\n",
    "# QLoRA additions\n",
    "qlora_config = {\n",
    "    **lora_config,\n",
    "    \"load_in_4bit\": True,       # Use 4-bit quantization\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",  # NormalFloat 4-bit\n",
    "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
    "    \"bnb_4bit_use_double_quant\": True,  # Double quantization\n",
    "}\n",
    "\n",
    "print(\"LoRA Config:\")\n",
    "print(json.dumps(lora_config, indent=2))\n",
    "print(\"\\nTraining Config:\")\n",
    "print(json.dumps(training_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-69",
   "metadata": {},
   "source": [
    "## 7.7 Ollama Modelfile for Custom Models - Demonstration\n",
    "\n",
    "Ollama uses Modelfiles to create custom model variants:\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Ollama Modelfile content\n",
    "modelfile_content = '''# Modelfile for a customer service assistant\n",
    "\n",
    "FROM llama3.2\n",
    "\n",
    "# Set the temperature for more consistent responses\n",
    "PARAMETER temperature 0.7\n",
    "\n",
    "# Set the system prompt\n",
    "SYSTEM \"\"\"\n",
    "You are a friendly and professional customer service representative. \n",
    "Always be helpful, empathetic, and solution-oriented. \n",
    "If you don't know the answer, offer to connect the customer with a specialist.\n",
    "Keep responses concise but warm.\n",
    "\"\"\"\n",
    "\n",
    "# You can also set a custom template\n",
    "TEMPLATE \"\"\"\n",
    "{{ if .System }}<|system|>\n",
    "{{ .System }}<|end|>\n",
    "{{ end }}{{ if .Prompt }}<|user|>\n",
    "{{ .Prompt }}<|end|>\n",
    "{{ end }}<|assistant|>\n",
    "{{ .Response }}<|end|>\n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "print(\"Example Ollama Modelfile:\")\n",
    "print(modelfile_content)\n",
    "\n",
    "print(\"\\nTo create this model, save as 'Modelfile' and run:\")\n",
    "print(\"  ollama create customer-service -f Modelfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-71",
   "metadata": {},
   "source": [
    "## Exercise 7.1: Create Training Data\n",
    "\n",
    "Create a small training dataset for a specific use case.\n",
    "\n",
    "| Task | What to do |\n",
    "|------|------------|\n",
    "| Choose a use case | Pick something like: code assistant, recipe helper, fitness coach, etc. |\n",
    "| Create 5 examples | Write 5 instruction/input/output training examples |\n",
    "| Validate format | Ensure each example has all required fields |\n",
    "\n",
    "**Hint:** Good training data is diverse and covers edge cases. Include both simple and complex examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your use case: _______________\n",
    "\n",
    "my_training_data = [\n",
    "    # Example 1\n",
    "    {\n",
    "        \"instruction\": None,\n",
    "        \"input\": None,\n",
    "        \"output\": None\n",
    "    },\n",
    "    # Example 2\n",
    "    {\n",
    "        \"instruction\": None,\n",
    "        \"input\": None,\n",
    "        \"output\": None\n",
    "    },\n",
    "    # Add more examples...\n",
    "]\n",
    "\n",
    "# Validate the data\n",
    "def validate_training_data(data):\n",
    "    required_keys = ['instruction', 'input', 'output']\n",
    "    for i, example in enumerate(data):\n",
    "        for key in required_keys:\n",
    "            if key not in example or example[key] is None:\n",
    "                print(f\"Example {i+1} missing or has None for '{key}'\")\n",
    "                return False\n",
    "    print(f\"All {len(data)} examples are valid!\")\n",
    "    return True\n",
    "\n",
    "validate_training_data(my_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-73",
   "metadata": {},
   "source": [
    "## Exercise 7.2: Design a LoRA Configuration\n",
    "\n",
    "Design a LoRA configuration for your use case.\n",
    "\n",
    "| Parameter | Consideration |\n",
    "|-----------|---------------|\n",
    "| `r` (rank) | Higher = more capacity but more memory. Start with 8-16 |\n",
    "| `lora_alpha` | Usually 2x the rank |\n",
    "| `target_modules` | Which attention layers to adapt |\n",
    "| `learning_rate` | 1e-4 to 3e-4 is typical for LoRA |\n",
    "\n",
    "**Hint:** For simple tasks, lower rank works. For complex tasks requiring nuanced understanding, use higher rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design your LoRA config\n",
    "my_lora_config = {\n",
    "    \"r\": None,              # Choose: 4, 8, 16, 32, or 64\n",
    "    \"lora_alpha\": None,     # Usually 2x r\n",
    "    \"target_modules\": None, # List of module names\n",
    "    \"lora_dropout\": None,   # 0.0 to 0.1 typical\n",
    "}\n",
    "\n",
    "my_training_config = {\n",
    "    \"num_epochs\": None,         # 1-5 typical for fine-tuning\n",
    "    \"batch_size\": None,         # 2, 4, or 8 depending on GPU memory\n",
    "    \"learning_rate\": None,      # 1e-4 to 3e-4\n",
    "    \"warmup_ratio\": None,       # 0.03 to 0.1\n",
    "}\n",
    "\n",
    "# Print your configuration\n",
    "print(\"Your LoRA Config:\")\n",
    "print(json.dumps(my_lora_config, indent=2))\n",
    "print(\"\\nYour Training Config:\")\n",
    "print(json.dumps(my_training_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-75",
   "metadata": {},
   "source": [
    "---\n",
    "# Lab Complete!\n",
    "\n",
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- **Basic Generation**: Use `ollama.generate()` for text completion\n",
    "- **Prompt Engineering**: Role-based prompts, structured output, JSON responses\n",
    "- **Parameters**: Control creativity with temperature\n",
    "- **Chat API**: Multi-turn conversations with `ollama.chat()`\n",
    "- **Applications**: Build summarizers, sentiment analyzers, and Q&A bots\n",
    "- **RAG**: Implement retrieval-augmented generation with embeddings\n",
    "- **Fine-tuning**: Understand LoRA/QLoRA for efficient model adaptation\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "```python\n",
    "# Basic generation\n",
    "response = ollama.generate(model='llama3.2', prompt='...')\n",
    "text = response['response']\n",
    "\n",
    "# Embeddings for RAG\n",
    "embedding = ollama.embed(model='llama3.2', input='text')['embeddings'][0]\n",
    "\n",
    "# Chat with history\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are...'},\n",
    "    {'role': 'user', 'content': 'Hello!'}\n",
    "]\n",
    "response = ollama.chat(model='llama3.2', messages=messages)\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Implement a production RAG system with a vector database (Chroma, FAISS)\n",
    "- Try fine-tuning with Hugging Face's PEFT library\n",
    "- Explore multi-modal models for image understanding\n",
    "- Build agent systems with tool use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
